{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS5sQetee7vc"
   },
   "source": [
    "# Model Pipeline\n",
    "\n",
    "By: Aditya Mengani, Ognjen Sosa, Sanjay Elangovan, Song Park, Sophia Skowronski\n",
    "\n",
    "**Can we improve on the baseline scores using different encoding, imputing, and scaling schemes?**\n",
    "- Averaged Logistic Regression accuracy Score: 0.5\n",
    "- Averaged Linear Regression accuracy score: 0.2045\n",
    "- Averaged K-Nearest Neighbour accuracy score: 0.6198\n",
    "- Averaged Naive Bayes accuracy score: 0.649\n",
    "\n",
    "**`p1_tag` ~  `rank` + `total_funding_usd` + `employee_count` (ordinal) + `country` (nominal) + `category_groups` (nominal)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lzz5DdXWTUOh"
   },
   "source": [
    "### STEPS FOR CONNECTING TO COLAB\n",
    "\n",
    "https://www.marktechpost.com/2019/06/07/how-to-connect-google-colab-with-google-drive/\n",
    "\n",
    "*  Upload the .csv files to your google drive\n",
    "*  Go to the file in google drive, right click on file name, then click on 'Get Link' and it shows the unique id of the file. Copy it and save it in the below code:\n",
    "downloaded = drive.CreateFile({'id':\"1uWwO-geA8IRNaerjQCk92******\"}) \n",
    "*  Replace the id with id of file you want to access\n",
    "downloaded.GetContentFile('baseline.csv')\n",
    "\n",
    "\n",
    "### Enabling GPU settings in COLAB\n",
    "\n",
    "https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UKGov2n9gm4i",
    "outputId": "cd1932d4-0a3a-4aef-c44f-c26bde01d4d8"
   },
   "outputs": [],
   "source": [
    "## GCP drive to colab connectivity Code\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "downloaded = drive.CreateFile({'id':\"1uWwO-geA8IR***\"})   # replace the id with id of file you want to access\n",
    "downloaded.GetContentFile('baseline.csv')\n",
    "\n",
    "downloaded = drive.CreateFile({'id':\"13zLq9t_***\"})   # replace the id with id of file you want to access\n",
    "downloaded.GetContentFile('pagerank_df_deg3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6OY3uRhuhBs6",
    "outputId": "9738bc24-1302-4fa5-b342-ecbae4256890"
   },
   "outputs": [],
   "source": [
    "#pip install prince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zal_raJUhSkB"
   },
   "outputs": [],
   "source": [
    "#pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1PxJFhiRa5c"
   },
   "outputs": [],
   "source": [
    "#pip install from libsvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Model Set up Options\n",
    "\n",
    "    Option 1 : Graph + Baseline\n",
    "    Option 2 : Baseline only\n",
    "    Option 3 : Graph + Baseline reduced\n",
    "    Option 4 : Baseline reduced only\n",
    "    Option 5 : Graph only\n",
    "*********************************************************** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE ONLY METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Baseline_Only(df,n_degrees, setup, iteration):\n",
    "    df = df.copy()\n",
    "    print(\"Original DF shape\",df.shape)\n",
    "    \n",
    "    # Have industry mapper for 'ind_1'...'ind_46' columns\n",
    "    industries = ['Software', 'Information Technology', 'Internet Services', 'Data and Analytics',\n",
    "                  'Sales and Marketing', 'Media and Entertainment', 'Commerce and Shopping', \n",
    "                  'Financial Services', 'Apps', 'Mobile', 'Science and Engineering', 'Hardware',\n",
    "                  'Health Care', 'Education', 'Artificial Intelligence', 'Professional Services', \n",
    "                  'Design', 'Community and Lifestyle', 'Real Estate', 'Advertising',\n",
    "                  'Transportation', 'Consumer Electronics', 'Lending and Investments',\n",
    "                  'Sports', 'Travel and Tourism', 'Food and Beverage',\n",
    "                  'Content and Publishing', 'Consumer Goods', 'Privacy and Security',\n",
    "                  'Video', 'Payments', 'Sustainability', 'Events', 'Manufacturing',\n",
    "                  'Clothing and Apparel', 'Administrative Services', 'Music and Audio',\n",
    "                  'Messaging and Telecommunications', 'Energy', 'Platforms', 'Gaming',\n",
    "                  'Government and Military', 'Biotechnology', 'Navigation and Mapping',\n",
    "                  'Agriculture and Farming', 'Natural Resources']\n",
    "    industry_map = {industry:'ind_'+str(idx+1) for idx,industry in enumerate(industries)}\n",
    "    \n",
    "    df_simple = reduce_mem_usage(df)\n",
    "    #print('\\nEnding Dataframe Columns:\\n\\n{}'.format(df_simple.columns.to_list()))\n",
    "    print('\\nDataframe shape:', df_simple.shape)\n",
    "    del industries, industry_map\n",
    "    \n",
    "    #print(\"df_simple columns\",list(df_simple.columns))\n",
    "    \n",
    "    # Extract baseline UUIDS part of Graph Network\n",
    "    list_Set_Up = ['BL_Only','G_Only','G+BL','G+BL_Red','BL_Red_Only']\n",
    "    folders = ['B', 'G', 'GB', 'GBR', 'BR']\n",
    "    save_map = dict(zip(list_Set_Up,folders))\n",
    "    if n_degrees == 4:\n",
    "        df_bl = pd.read_csv('files/output/Model_DF_D4/{}/{}.csv'.format(save_map[setup], iteration),sep=',')\n",
    "        print(\"Original Model_DF_D2 shape\",df.shape)\n",
    "        #print(df_gr.columns)\n",
    "    elif n_degrees == 5:\n",
    "        df_bl = pd.read_csv('files/output/Model_DF_D5/{}/{}.csv'.format(save_map[setup], iteration),sep=',')\n",
    "        print(\"Original Model_DF_D4 shape\",df.shape)\n",
    "        #print(df_gr.columns)\n",
    "    df_simple = pd.merge(df_bl.copy(),df_simple.copy(),how='inner',on='uuid') \n",
    "    \n",
    "    #print(list(df_simple.columns))\n",
    "    return df_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE REDUCED METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ELIMINIATING FEATURES: RANK and total_funding_usd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Baseline_Reduced(df,n_degrees, setup, iteration):\n",
    "    df = df.copy()\n",
    "    print(\"Original DF shape\",df.shape)\n",
    "    \n",
    "    #print('\\nStarting Dataframe Columns:\\n\\n{}\\n'.format(df.columns.to_list()))\n",
    "    # Have industry mapper for 'ind_1'...'ind_46' columns\n",
    "    industries = ['Software', 'Information Technology', 'Internet Services', 'Data and Analytics',\n",
    "                  'Sales and Marketing', 'Media and Entertainment', 'Commerce and Shopping', \n",
    "                  'Financial Services', 'Apps', 'Mobile', 'Science and Engineering', 'Hardware',\n",
    "                  'Health Care', 'Education', 'Artificial Intelligence', 'Professional Services', \n",
    "                  'Design', 'Community and Lifestyle', 'Real Estate', 'Advertising',\n",
    "                  'Transportation', 'Consumer Electronics', 'Lending and Investments',\n",
    "                  'Sports', 'Travel and Tourism', 'Food and Beverage',\n",
    "                  'Content and Publishing', 'Consumer Goods', 'Privacy and Security',\n",
    "                  'Video', 'Payments', 'Sustainability', 'Events', 'Manufacturing',\n",
    "                  'Clothing and Apparel', 'Administrative Services', 'Music and Audio',\n",
    "                  'Messaging and Telecommunications', 'Energy', 'Platforms', 'Gaming',\n",
    "                  'Government and Military', 'Biotechnology', 'Navigation and Mapping',\n",
    "                  'Agriculture and Farming', 'Natural Resources']\n",
    "    industry_map = {industry:'ind_'+str(idx+1) for idx,industry in enumerate(industries)}\n",
    "    \n",
    "\n",
    "    # Reduced baseline doesnt have these two columns\n",
    "    df_simple = df.drop(['rank','total_funding_usd'], axis=1)\n",
    "    df_simple = reduce_mem_usage(df_simple)\n",
    "    #print('\\nEnding Dataframe Columns:\\n\\n{}'.format(df_simple.columns.to_list()))\n",
    "    print('\\nDataframe shape:', df_simple.shape)\n",
    "    \n",
    "    # Extract baseline UUIDS part of Graph Network\n",
    "    list_Set_Up = ['BL_Only','G_Only','G+BL','G+BL_Red','BL_Red_Only']\n",
    "    folders = ['B', 'G', 'GB', 'GBR', 'BR']\n",
    "    save_map = dict(zip(list_Set_Up,folders))\n",
    "    if n_degrees == 4:\n",
    "        df_bl = pd.read_csv('files/output/Model_DF_D4/{}/{}.csv'.format(save_map[setup], iteration),sep=',')\n",
    "        print(\"Original Model_DF_D2 shape\",df.shape)\n",
    "        #print(df_gr.columns)\n",
    "    elif n_degrees == 5:\n",
    "        df_bl = pd.read_csv('files/output/Model_DF_D5/{}/{}.csv'.format(save_map[setup], iteration),sep=',')\n",
    "        print(\"Original Model_DF_D4 shape\",df.shape)\n",
    "        #print(df_gr.columns)\n",
    "    df_simple = pd.merge(df_bl.copy(),df_simple.copy(),how='inner',on='uuid')\n",
    "    \n",
    "    #print(\"Original DF_GR shape after merge\",df_gr.shape)\n",
    "    #print(df_gr.columns)\n",
    "    \n",
    "    del industries, industry_map\n",
    "    return df_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAPH ONLY METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_Only(df, n_degrees):\n",
    "    df = df.copy()\n",
    "    df = df[['uuid','p1_tag']]\n",
    "    df['__id'] = df['uuid']\n",
    "    print(\"Original DF shape\",df.shape)\n",
    "    if n_degrees == 2:\n",
    "        df_gr = pd.read_csv('files/output/Model_DF_D2.csv',sep=',')\n",
    "        print(\"Original Model_DF_D2 shape\",df.shape)\n",
    "        #print(df_gr.columns)\n",
    "    elif n_degrees == 4:\n",
    "        df_gr = pd.read_csv('files/output/Model_DF_D4.csv',sep=',')\n",
    "        print(\"Original Model_DF_D4 shape\",df.shape)\n",
    "        #print(df_gr.columns)\n",
    "    elif n_degrees == 0:\n",
    "        df_gr = pd.read_csv('files/output/Model_DF_ALLLLL.csv',sep=',')\n",
    "        print(\"Original Model_DF_ALLLLL.csv shape\",df.shape)\n",
    "    print(\"Original DF_GR shape after merge\",df_gr.shape)\n",
    "    #print(df_gr.columns)\n",
    "    df_gr = reduce_mem_usage(df_gr) \n",
    "    df_gr = df_gr.fillna(0)\n",
    "    del df\n",
    "    #impute\n",
    "    return df_gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAPH ONLY METHOD for 100 ITERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR 100 ITERATIONS\n",
    "def Graph_Only_SS(df,n_degrees, setup, iteration):\n",
    "    df = df.copy()\n",
    "    df = df[['uuid','p1_tag']]\n",
    "    print(\"Original DF shape\",df.shape)\n",
    "    list_Set_Up = ['BL_Only','G_Only','G+BL','G+BL_Red','BL_Red_Only']\n",
    "    folders = ['B', 'G', 'GB', 'GBR', 'BR']\n",
    "    save_map = dict(zip(list_Set_Up,folders))\n",
    "    if n_degrees == 4:\n",
    "        df_gr = pd.read_csv('files/output/Model_DF_D4/{}/{}.csv'.format(save_map[setup], iteration),sep=',')\n",
    "        print(\"Original Model_DF_D2 shape\",df.shape)\n",
    "        #print(df_gr.columns)\n",
    "    elif n_degrees == 5:\n",
    "        df_gr = pd.read_csv('files/output/Model_DF_D5/{}/{}.csv'.format(save_map[setup], iteration),sep=',')\n",
    "        print(\"Original Model_DF_D4 shape\",df.shape)\n",
    "        #print(df_gr.columns)\n",
    "    df_gr = pd.merge(df_gr.copy(),df.copy(),how='inner',on='uuid')\n",
    "    print(\"Original DF_GR shape after merge\",df_gr.shape)\n",
    "    #print(df_gr.columns)\n",
    "    df_gr = reduce_mem_usage(df_gr) \n",
    "    df_gr['w_spath_top_3_0'][df_gr['w_spath_top_3_0']==1e30] = 1000\n",
    "    df_gr['w_spath_top_3_1'][df_gr['w_spath_top_3_1']==1e30] = 1000\n",
    "    df_gr['w_spath_top_3_3'][df_gr['w_spath_top_3_3']==1e30] = 1000\n",
    "    df_gr['w_spath_top_3_4'][df_gr['w_spath_top_3_4']==1e30] = 1000\n",
    "    df_gr['w_spath_top_min_3'][df_gr['w_spath_top_min_3']==1e30] = 1000\n",
    "    df_gr = df_gr.fillna(0)\n",
    "       \n",
    "    del df\n",
    "    #impute\n",
    "    return df_gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UaDSZ2aje7ve",
    "outputId": "8df246b7-5589-41ff-bb88-2b6e309e0887"
   },
   "outputs": [],
   "source": [
    "## Select equal sample of non-Pledge 1% organizations\n",
    "def gen_Train_Test_Split(df_simple):\n",
    "    df_p1 = df_simple[df_simple['p1_tag']==1]\n",
    "    print(df_p1.shape)\n",
    "    df_notp1 = df_simple[df_simple['p1_tag']==0].sample(n=df_p1.shape[0], replace=True)\n",
    "    df_model = pd.concat([df_p1, df_notp1]).reset_index(drop=True)\n",
    "    df_model = reduce_mem_usage(df_model)\n",
    "\n",
    "    # Create variable for each feature type: categorical and numerical\n",
    "    numeric_features = df_model.select_dtypes(include=['uint8','int8', 'int16', 'int32', 'int64', 'float16', 'float32','float64']).drop(['p1_tag'], axis=1).columns\n",
    "    categorical_features = df_model.select_dtypes(include=['object']).columns\n",
    "    #print('Numeric features:', numeric_features.to_list())\n",
    "    #print('Categorical features:', categorical_features.to_list())\n",
    "\n",
    "    X = df_model.drop('p1_tag', axis=1)\n",
    "    y = df_model['p1_tag']\n",
    "    y = preprocessing.LabelEncoder().fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=99)\n",
    "    print('Training data shape:', X_train.shape)\n",
    "    print('Train label shape:', y_train.shape)\n",
    "    print('Test data shape:',  X_test.shape)\n",
    "    print('Test label shape:', y_test.shape)\n",
    "\n",
    "    # reset indexes for train and test\n",
    "    X_train= X_train.reset_index(drop=True)\n",
    "    X_test= X_test.reset_index(drop=True)\n",
    "    return X_train,X_test,X,y,y_train,y_test,numeric_features,categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERFORM PCA COUNTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "Hfb6xuWChK3Z",
    "outputId": "96f95c5d-8cc0-4803-86d3-2ee6a4fc88ae"
   },
   "outputs": [],
   "source": [
    "# Perform PCA of country dataset\n",
    "def PCA_Country(X_train,X_test):\n",
    "    # Perform PCA of country dataset\n",
    "    country_train = X_train.filter(regex='^country',axis=1).fillna(0)\n",
    "    country_test = X_test.filter(regex='^country',axis=1).fillna(0)\n",
    "#     # For each value of k, use PCA to project the data feature sets to k principle components\n",
    "#     matrix = [['k', 'total variance']] # For display\n",
    "#     k_values = list(range(1,113)) # To loop through, there are 112 country codes\n",
    "#     # For each value of k, use PCA to project the data feature sets to k principle components\n",
    "#     for k in k_values:\n",
    "#         pca = PCA(n_components=k, whiten=True,random_state=random.seed(1234))\n",
    "#         pca.fit(country_train)\n",
    "#         matrix.append([k, round(pca.explained_variance_ratio_.sum(),4)])\n",
    "#     # Print results\n",
    "#     print('Fraction of the total variance in the training data explained by the first k principal components:\\n')\n",
    "#     s = [[str(e) for e in row] for row in matrix]\n",
    "#     lens = [max(map(len, col)) for col in zip(*s)]\n",
    "#     fmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\n",
    "#     table = [fmt.format(*row) for row in s]\n",
    "#     print('\\n'.join(table))\n",
    "#     print()\n",
    "#     # Plots\n",
    "#     _, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,5))\n",
    "#     # Plotting lineplot of fraction of total variance vs. number of principal components\n",
    "#     # For all possible numbers of principal components\n",
    "#     ax.plot(np.cumsum(PCA().fit(country_train).explained_variance_ratio_))\n",
    "#     # Labels\n",
    "#     ax.set_title('Fraction of total variance vs. number of principal components')\n",
    "#     ax.set_xlabel('k = number of components')\n",
    "#     ax.set_ylabel('Cumulative explained variance')\n",
    "#     # Display\n",
    "#     plt.show()\n",
    "    \n",
    "    # create PCA features for train and test set\n",
    "    #print(\"country train\",list(country_train.columns))\n",
    "    n_components = 15\n",
    "    pca = PCA(n_components=n_components,whiten=True,random_state=random.seed(1234))  \n",
    "    pca_train = pca.fit_transform(country_train)\n",
    "    pca_test = pca.transform(country_test)\n",
    "    # create dataframes from numpy\n",
    "    df_cty_train = pd.DataFrame(pca_train,columns=['cntry_pca_'+ str(x) for x in range(n_components)])\n",
    "    df_cty_test = pd.DataFrame(pca_test,columns=['cntry_pca_'+ str(x) for x in range(n_components)])\n",
    "    # drop country prefix columns\n",
    "    X_train = X_train.drop(list(X_train.filter(regex='^country_',axis=1).columns), axis=1)\n",
    "    X_test = X_test.drop(list(X_test.filter(regex='^country_',axis=1).columns), axis=1)\n",
    "    # concat with train dataset\n",
    "    X_train = pd.concat([X_train, df_cty_train],axis = 1)\n",
    "    X_test = pd.concat([X_test, df_cty_test],axis = 1)\n",
    "    del df_cty_train,df_cty_test,country_train,country_test\n",
    "    return X_train,X_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERFORM PCA INDUSTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "hNCYcDu9gj_z",
    "outputId": "38dcead6-df46-444e-d6a4-f4a112b87ef7"
   },
   "outputs": [],
   "source": [
    "# Perform PCA of country dataset\n",
    "def PCA_Industry(X_train,X_test):\n",
    "    # Perform PCA of industry dataset\n",
    "    industry_train = X_train.filter(regex='^ind_',axis=1).fillna(0)\n",
    "    industry_test = X_test.filter(regex='^ind_',axis=1).fillna(0)\n",
    "#     # For each value of k, use PCA to project the data feature sets to k principle components\n",
    "#     matrix = [['k', 'total variance']] # For display\n",
    "#     k_values = list(range(1,47)) # To loop through, there are 46 industries\n",
    "#     # For each value of k, use PCA to project the data feature sets to k principle components\n",
    "#     for k in k_values:\n",
    "#         pca = PCA(n_components=k, whiten=True,random_state=random.seed(1234))\n",
    "#         pca.fit(industry_train)\n",
    "#         matrix.append([k, round(pca.explained_variance_ratio_.sum(),4)])\n",
    "#     # Print results\n",
    "#     print('Fraction of the total variance in the training data explained by the first k principal components:\\n')\n",
    "#     s = [[str(e) for e in row] for row in matrix]\n",
    "#     lens = [max(map(len, col)) for col in zip(*s)]\n",
    "#     fmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\n",
    "#     table = [fmt.format(*row) for row in s]\n",
    "#     print('\\n'.join(table))\n",
    "#     print()\n",
    "#     # Plots\n",
    "#     _, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,5))\n",
    "#     # Plotting lineplot of fraction of total variance vs. number of principal components\n",
    "#     # For all possible numbers of principal components\n",
    "#     ax.plot(np.cumsum(PCA().fit(industry_train).explained_variance_ratio_))\n",
    "#     # Labels\n",
    "#     ax.set_title('Fraction of total variance vs. number of principal components')\n",
    "#     ax.set_xlabel('k = number of components')\n",
    "#     ax.set_ylabel('Cumulative explained variance')\n",
    "#     # Display\n",
    "#     plt.show()\n",
    "    \n",
    "    # create PCA features for train and test set\n",
    "    n_components=10\n",
    "    pca = PCA(n_components=n_components, whiten=True, random_state=random.seed(1234)) \n",
    "    pca_train = pca.fit_transform(industry_train)\n",
    "    pca_test = pca.transform(industry_test)\n",
    "    # create dataframes from numpy\n",
    "    df_ind_train = pd.DataFrame(pca_train,columns=['ind_pca'+ str(x) for x in range(n_components)])\n",
    "    df_ind_test = pd.DataFrame(pca_test,columns=['ind_pca'+ str(x) for x in range(n_components)])\n",
    "    # drop country prefix columns\n",
    "    X_train = X_train.drop(list(X_train.filter(regex='^ind_',axis=1).columns), axis=1)\n",
    "    X_test = X_test.drop(list(X_test.filter(regex='^ind_',axis=1).columns), axis=1)\n",
    "    # concat with train dataset\n",
    "    X_train = pd.concat([X_train, df_ind_train],axis = 1)\n",
    "    X_test = pd.concat([X_test, df_ind_test],axis = 1)\n",
    "    del df_ind_train,df_ind_test,industry_train,industry_test\n",
    "\n",
    "    return X_train,X_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIZUALIZE COUNTRY INDUSTRY PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 985
    },
    "id": "UYBtgwajQZ8L",
    "outputId": "6174a13c-5e59-4e88-e92e-72c577ebe7fa"
   },
   "outputs": [],
   "source": [
    "# create graphs for PCA analysis for country and industry features\n",
    "def Visualize_Country_Ind_PCA(X,y):\n",
    "    print(\"None\")\n",
    "#     Country_df = X.filter(regex='^country',axis=1).fillna(0)\n",
    "#     pca_new_Country = PCA(n_components=10,random_state=random.seed(1234))  \n",
    "#     Country_df_PCA = pca_new_Country.fit_transform(Country_df)\n",
    "\n",
    "#     Industry_df = X.filter(regex='^ind_',axis=1).fillna(0)\n",
    "#     pca_new_Industry_df = PCA(n_components=30,random_state=random.seed(1234))  \n",
    "#     Industry_df_PCA = pca_new_Industry_df.fit_transform(Industry_df)\n",
    "\n",
    "#     # The PCA model\n",
    "#     fig, axes = plt.subplots(1,2,figsize=(15,15))\n",
    "#     colors = ['r','g']\n",
    "#     fig.suptitle('PCA Analysis for Country and Industry', fontsize=30)\n",
    "#     targets = [1,0]\n",
    "#     for target, color in zip(targets,colors):\n",
    "#       indexes = np.where(y == target)\n",
    "#       axes[0].scatter(Country_df_PCA[indexes][:,0], Country_df_PCA[indexes][:,1],color=color)\n",
    "#       axes[0].set_xlabel('PC1')\n",
    "#       axes[0].set_ylabel('PC2')\n",
    "#       axes[0].set_title('PCA-Country')\n",
    "#       axes[1].scatter(Industry_df_PCA[indexes][:,0], Industry_df_PCA[indexes][:,1], color=color)\n",
    "#       axes[1].set_xlabel('PC1')\n",
    "#       axes[1].set_ylabel('PC2')\n",
    "#       axes[1].set_title('PCA-Industry')\n",
    "#     plt.axis('tight')\n",
    "\n",
    "#     out_labels = ['p1','non-p1']\n",
    "#     plt.legend(out_labels,prop={'size':10},loc='upper right',title='Legend of plot')\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN CLASSIFIER\n",
    "\n",
    " - Uncomment the classifier that you need to run and comment the ones that you are not running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BmnuF44ge7ve",
    "outputId": "ac7feb4d-171c-455d-faf1-df090485dad0"
   },
   "outputs": [],
   "source": [
    "def Run_Classifier(X_train,X_test,y_train,y_test,numeric_features,categorical_features,n_deg,Type):\n",
    "    \n",
    "    results = OrderedDict()\n",
    "    results['n_deg'] = n_deg\n",
    "    results['Model_Type'] = Type\n",
    "    #results['Column_Name'] = col_graph\n",
    "    classifier_list = []\n",
    "    LRR = LogisticRegression(max_iter=10000, tol=0.1)\n",
    "#     KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "#     BNB = BernoulliNB()\n",
    "#     GNB = GaussianNB()\n",
    "#     SVM = svm.SVC()\n",
    "#     DCT = DecisionTreeClassifier()\n",
    "#     XGB = xgb.XGBRegressor() #tree_method='gpu_hist', gpu_id=0\n",
    "#     RMF = RandomForestClassifier()\n",
    "\n",
    "    #classifier\n",
    "    classifier_list.append(('LRR', LRR, {'classifier__C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000],\\\n",
    "                                        'classifier__random_state': [random.seed(1234)]}))\n",
    "#     classifier_list.append(('KNN', KNN, {}))\n",
    "#     classifier_list.append(('BNB', BNB, {'classifier__alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]}))\n",
    "#     classifier_list.append(('GNB', GNB, {'classifier__var_smoothing': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]}))\n",
    "#     classifier_list.append(('DCT', DCT, {'classifier__max_depth':np.arange(1, 21),\n",
    "#                                         'classifier__min_samples_leaf':[1, 5, 10, 20, 50, 100],\n",
    "#                                         'classifier__random_state' : [random.seed(1234)]}))\n",
    "#     classifier_list.append(('XGB', XGB, {'classifier__random_state' : [random.seed(1234)]}))\n",
    "#     classifier_list.append(('RMF', RMF, {'classifier__random_state' : [random.seed(1234)]}))\n",
    "#     classifier_list.append(('SVM', SVM, {'classifier__random_state' : [random.seed(1234)]}))\n",
    "\n",
    "    encoder_list = [ce.one_hot.OneHotEncoder]\n",
    "    scaler_list = [StandardScaler()]\n",
    "\n",
    "    for label, classifier, params in classifier_list:\n",
    "        results[label] = {}\n",
    "        for encoder in encoder_list:\n",
    "            for feature_scaler in scaler_list:\n",
    "                results[label][f'{encoder.__name__} with {feature_scaler}'] = {}\n",
    "                print('{} with {} and {}'.format(label,encoder.__name__,feature_scaler))\n",
    "\n",
    "                numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),('scaler', StandardScaler())])\n",
    "\n",
    "                categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                                                          ('woe', encoder())])\n",
    "\n",
    "                preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),\n",
    "                                                               ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "                pipe = Pipeline(steps=[#('preprocessor', preprocessor),\n",
    "                                       ('scaler', feature_scaler),\n",
    "                                       ('classifier', classifier)])\n",
    "\n",
    "                if params != {}:\n",
    "                    search = RandomizedSearchCV(pipe, params, n_jobs=-1)\n",
    "                    search.fit(X_train, y_train)\n",
    "                    print('Best parameter (CV score={:.3f}): {}'.format(search.best_score_, search.best_params_))\n",
    "                    model = search.fit(X_train, y_train)\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    if label == 'XGB':\n",
    "                        y_pred = [round(value) for value in y_pred]\n",
    "                    score = f1_score(y_test, y_pred,average='weighted')\n",
    "                    print('Best score: {:.4f}\\n'.format(score))\n",
    "                    results[label][f'{encoder.__name__} with {feature_scaler}']['score'] = score\n",
    "                    try:\n",
    "                        results[label][f'{encoder.__name__} with {feature_scaler}']['best_params'] = search.best_params_\n",
    "                    except:\n",
    "                        print('Something went wrong w/ GridSearch or pipeline fitting.')\n",
    "                else:\n",
    "                    try:\n",
    "                        model = pipe.fit(X_train, y_train)\n",
    "                        y_pred = model.predict(X_test)\n",
    "                        if label == 'XGB':\n",
    "                            y_pred = [round(value) for value in y_pred]\n",
    "                        score = f1_score(y_test, y_pred,average='weighted')\n",
    "                        print('Score: {:.4f}\\n'.format(score))\n",
    "                        results[label][f'{encoder.__name__} with {feature_scaler}']['score'] = score\n",
    "                    except:\n",
    "                        print('Something went wrong with pipeline fitting')\n",
    "    #print(results)\n",
    "    return results    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRITE OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Write_Output(out_list,iteration):\n",
    "    # encode to encode int/float and array types and write the output json\n",
    "    class NpEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            else:\n",
    "                return super(NpEncoder, self).default(obj)\n",
    "\n",
    "    # File is saved under Files directory. /content would be the baseline folder\n",
    "    # You can click on folder icon on left side of the directory structure to\n",
    "    # see the created file\n",
    "    \n",
    "    with open(f'files/output/results_baseline_ITER_{iteration}.json', 'w') as fp:\n",
    "        json.dump(out_list, fp, sort_keys=False, indent=4, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REDUCE MEMORY USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100*(start_mem-end_mem)/start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate average\n",
    "- Calculate the average of all the generated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the generated results file\n",
    "def calculate_avg(iterations):\n",
    "    # Enter the model names #'KNN':0,'BNB':0,'GNB':0,'DCT':0,'XGB':0,'RMF':0,'SVM':0\n",
    "    test_accuracies = {'LRR':0}\n",
    "    data_cnt = {'LRR':0}\n",
    "    \n",
    "    for i in range(iterations):\n",
    "         with open(f'files/output/results_baseline_ITER_{i}.json') as g:\n",
    "                #json.dump(out_list, fp, sort_keys=False, indent=4, cls=NpEncoder)\n",
    "                data = json.load(g)\n",
    "                for i in list(test_accuracies.keys()):\n",
    "                    for j in data:\n",
    "                        test_accuracies[i] = test_accuracies[i] + (j['result'][0][i]['OneHotEncoder with StandardScaler()']['score'])\n",
    "                        data_cnt[i] = data_cnt[i] + len(data)\n",
    "    \n",
    "    for i in test_accuracies:\n",
    "        test_accuracies[i] = round(test_accuracies[i]/data_cnt[i],2)\n",
    "    \n",
    "    \n",
    "    print(\"\\nAveraged accuracies: \",test_accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN MODULE\n",
    "- Run 100 iterations,for each set up and each degree type\n",
    "- Capture the results in json\n",
    "- Calculate the average across all scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''Data analysis'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "#import itertoolss\n",
    "import statistics\n",
    "from collections import OrderedDict \n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "'''Plotting'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "'''Stat'''\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import chi2_contingency\n",
    "'''ML'''\n",
    "import prince\n",
    "import category_encoders as ce\n",
    "from sklearn import metrics, svm, preprocessing, utils\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,\\\n",
    "MaxAbsScaler,RobustScaler,QuantileTransformer,PowerTransformer\n",
    "from libsvm.svmutil import *\n",
    "\n",
    "list_Set_Up = ['BL_Only','G_Only','G+BL','G+BL_Red','BL_Red_Only']\n",
    "#list_Set_Up = ['G_Only']\n",
    "degrees = [4,5]\n",
    "\n",
    "# Defining main function \n",
    "def main():\n",
    "    final_out = []\n",
    "    df = pd.read_csv('files/output/baseline.csv',sep=';')\n",
    "    \n",
    "    # set the total iterations needed to 100\n",
    "    total_iterations = 100\n",
    "    \n",
    "    for iteration in range(total_iterations):\n",
    "        out_dict = {}\n",
    "        out_dict['iteration'] = iteration\n",
    "        out_list = []\n",
    "        \n",
    "        for n_deg in degrees:\n",
    "            for setup_Type in list_Set_Up:\n",
    "                print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:SETUP:{setup_Type} BEGIN...\")\n",
    "                    \n",
    "                #********* BASE LINE ONLY *******\n",
    "                if setup_Type == 'BL_Only':\n",
    "                    print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:BASELINE_ONLY SET UP:START...\")\n",
    "                    df_bo = Baseline_Only(df,n_deg, setup_Type, iteration)\n",
    "                    df_bo = df_bo.drop(['uuid'],axis=1)\n",
    "                    df_simple = df_bo\n",
    "                    X_train,X_test,X,y,y_train,y_test,numeric_features,\\\n",
    "                    categorical_features = gen_Train_Test_Split(df_simple)\n",
    "                    X_train,X_test = PCA_Industry(X_train,X_test)\n",
    "                    X_train,X_test = PCA_Country(X_train,X_test)\n",
    "                    #Visualize_Country_Ind_PCA(X,y)\n",
    "                    \n",
    "                    print(\"Final train dataset shape\",X_train.shape)\n",
    "                    print(\"\\nFinal test dataset shape\",X_test.shape)                             \n",
    "                    \n",
    "                    results = Run_Classifier(X_train,X_test,y_train,y_test,numeric_features,categorical_features,n_deg,setup_Type)\n",
    "                    out_list.append(results)\n",
    "                    print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:BASELINE_ONLY SET UP:END\")\n",
    "                \n",
    "                #********* GRAPH ONLY *******\n",
    "                elif setup_Type == 'G_Only':\n",
    "                    print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:GRAPH_ONLY SET UP:START...\")\n",
    "                \n",
    "                    df_gr = Graph_Only_SS(df,n_deg, setup_Type, iteration)\n",
    "                    df_gr = df_gr.drop(['uuid'],axis=1)\n",
    "                    df_simple = df_gr\n",
    "                    \n",
    "                    X_train,X_test,X,y,y_train,y_test,numeric_features,\\\n",
    "                    categorical_features = gen_Train_Test_Split(df_simple)\n",
    "                    \n",
    "                    print(\"Final train dataset shape\",X_train.shape)\n",
    "                    print(\"\\nFinal test dataset shape\",X_test.shape)\n",
    "                    \n",
    "                    #print('\\nTest Dataframe Columns:\\n\\n{}'.format(X_test.columns.to_list()))\n",
    "                    results = Run_Classifier(X_train,X_test,y_train,y_test,numeric_features,categorical_features,n_deg,setup_Type)\n",
    "                    out_list.append(results)\n",
    "                    print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:GRAPH_ONLY SET UP:END\")\n",
    "                \n",
    "                #********* GRAPH + BASELINE ONLY *******\n",
    "                elif setup_Type == 'G+BL':   \n",
    "                    print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:GRAPH+BASELINE:START...\") \n",
    "                    df_gr = Graph_Only_SS(df,n_deg, setup_Type, iteration)\n",
    "\n",
    "                    print(\"Graph shape after merge\",df_gr.shape)\n",
    "                    df_bo = Baseline_Only(df,n_deg,'BL_Only', iteration)\n",
    "                    df_simple = pd.merge(df_gr.copy(),df_bo.copy(), how = 'inner',on='uuid')\n",
    "                    df_simple = df_simple.drop(['uuid','p1_tag_y'],axis=1)\n",
    "                    print(\"Merged shape after baseline and graph\",df_simple.shape)\n",
    "                    df_simple = df_simple.rename(columns={\"p1_tag_x\": \"p1_tag\"})\n",
    "                    X_train,X_test,X,y,y_train,y_test,numeric_features,\\\n",
    "                    categorical_features = gen_Train_Test_Split(df_simple)\n",
    "                    print(\"Before pca dataset shape\",X_train.shape)\n",
    "                    print(\"\\nBefore pca dataset shape\",X_test.shape)\n",
    "                    X_train,X_test = PCA_Industry(X_train,X_test)\n",
    "                    X_train,X_test = PCA_Country(X_train,X_test)\n",
    "                    #Visualize_Country_Ind_PCA(X)\n",
    "                    #print(\"Train set columns list\",X_train.columns)\n",
    "                    print(\"Final train dataset shape\",X_train.shape)\n",
    "                    print(\"\\nFinal test dataset shape\",X_test.shape)\n",
    "\n",
    "                    print('\\nTrain Dataframe Columns:\\n\\n{}'.format(X_train.columns.to_list()))\n",
    "                    #print('\\nTest Dataframe Columns:\\n\\n{}'.format(X_test.columns.to_list()))\n",
    "                    \n",
    "                    #check for nan and infinite columns\n",
    "                    nan_values = X_train.isna()\n",
    "                    nan_columns = nan_values.any()\n",
    "                    columns_with_nan = X_train.columns[nan_columns].tolist()\n",
    "                    if columns_with_nan != []:\n",
    "                        print(\"columns_with_nan \",columns_with_nan)\n",
    "\n",
    "                    print(\"Infinite columns train\",(X_train.columns.to_series()[np.isinf(X_train).any()]))\n",
    "                    print(\"Infinite columns test\",(X_test.columns.to_series()[np.isinf(X_test).any()]))\n",
    "                    \n",
    "                    results = Run_Classifier(X_train,X_test,y_train,y_test,numeric_features,categorical_features,n_deg,setup_Type)\n",
    "                    out_list.append(results)\n",
    "                    print(f\"\\nITERATION:{iteration}DEGREE:{n_deg}:GRAPH+BASELINE SET UP:END\")\n",
    "                \n",
    "                #********* GRAPH + BASELINE REDUCED ONLY *******\n",
    "                elif setup_Type == 'G+BL_Red':\n",
    "                    print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:GRAPH+BASELINE_REDUCED:START...\") \n",
    "                    df_gr = Graph_Only_SS(df,n_deg, setup_Type, iteration)\n",
    "                    print(\"Graph shape after merge\",df_gr.shape)\n",
    "                    df_bo = Baseline_Reduced(df,n_deg, 'BL_Red_Only', iteration)\n",
    "                    df_simple = pd.merge(df_gr.copy(),df_bo.copy(), how = 'inner',on='uuid')\n",
    "                    df_simple = df_simple.drop(['uuid','p1_tag_y'],axis=1)\n",
    "                    print(\"Merged shape after baseline and graph\",df_simple.shape)\n",
    "                    #print(list(df_simple.columns))\n",
    "                    df_simple = df_simple.rename(columns={\"p1_tag_x\": \"p1_tag\"})\n",
    "                    X_train,X_test,X,y,y_train,y_test,numeric_features,\\\n",
    "                    categorical_features = gen_Train_Test_Split(df_simple)\n",
    "                    print(\"Before pca dataset shape\",X_train.shape)\n",
    "                    print(\"\\nBefore pca dataset shape\",X_test.shape)\n",
    "                    X_train,X_test = PCA_Industry(X_train,X_test)\n",
    "                    X_train,X_test = PCA_Country(X_train,X_test)\n",
    "                    #Visualize_Country_Ind_PCA(X)\n",
    "                    print(\"Final train dataset shape\",X_train.shape)\n",
    "                    print(\"\\nFinal test dataset shape\",X_test.shape)\n",
    "                    print('\\nTrain Dataframe Columns:\\n\\n{}'.format(X_train.columns.to_list()))\n",
    "                    #print('\\nTest Dataframe Columns:\\n\\n{}'.format(X_test.columns.to_list()))\n",
    "                    \n",
    "                    #check for nan and infinite columns\n",
    "                    nan_values = X_train.isna()\n",
    "                    nan_columns = nan_values.any()\n",
    "                    columns_with_nan = X_train.columns[nan_columns].tolist()\n",
    "                    if columns_with_nan != []:\n",
    "                        print(\"columns_with_nan \",columns_with_nan)\n",
    "                    print(\"Infinite columns train\",(X_train.columns.to_series()[np.isinf(X_train).any()]))\n",
    "                    print(\"Infinite columns test\",(X_test.columns.to_series()[np.isinf(X_test).any()]))\n",
    "                    \n",
    "                    results = Run_Classifier(X_train,X_test,y_train,y_test,numeric_features,categorical_features,n_deg,setup_Type)\n",
    "                    out_list.append(results)\n",
    "                    print(f\"\\nDEGREE:{n_deg}:GRAPH+BASELINE_REDUCED SET UP:END\")               \n",
    "                \n",
    "                #********* BASELINE REDUCED ONLY *******\n",
    "                elif setup_Type == 'BL_Red_Only':\n",
    "                    \n",
    "                    print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:BASELINE_REDUCED_ONLY SET UP:START...\")\n",
    "                    df_bo = Baseline_Reduced(df,n_deg, setup_Type, iteration)\n",
    "                    df_bo = df_bo.drop(['uuid'],axis=1)\n",
    "                    df_simple = df_bo\n",
    "                    X_train,X_test,X,y,y_train,y_test,numeric_features,\\\n",
    "                    categorical_features = gen_Train_Test_Split(df_simple)\n",
    "                    X_train,X_test = PCA_Industry(X_train,X_test)\n",
    "                    X_train,X_test = PCA_Country(X_train,X_test)\n",
    "                    #Visualize_Country_Ind_PCA(X,y)\n",
    "                    \n",
    "                    print(\"Final train dataset shape\",X_train.shape)\n",
    "                    print(\"\\nFinal test dataset shape\",X_test.shape)                             \n",
    "                    \n",
    "                    results = Run_Classifier(X_train,X_test,y_train,y_test,numeric_features,categorical_features,n_deg,setup_Type)\n",
    "                    out_list.append(results)\n",
    "                    print(f\"\\nDEGREE:{n_deg}:BASELINE_REDUCED_ONLY SET UP:END\")\n",
    "        out_dict['result'] = out_list\n",
    "        final_out.append(out_dict)\n",
    "        Write_Output(final_out,iteration)\n",
    "        print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:END\")\n",
    "    #Write_Output(final_out)\n",
    "    calculate_avg(total_iterations)\n",
    "    print(\"Completed all runs!....\")\n",
    "if __name__ == \"__main__\":\n",
    "    # execute only if run as a script\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "5_AM_Model_Pipeline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "common-cpu.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
