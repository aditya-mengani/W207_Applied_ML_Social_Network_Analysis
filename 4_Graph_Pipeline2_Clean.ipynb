{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **This notebook's process**\n",
    "\n",
    "1. Load in Crunchbase dataframes(4 merged CSVs created in `1_SS_EDA.ipynb`)\n",
    "    - Organizations: `files/output/organizations_merged.csv`\n",
    "    - Jobs: `files/output/p1_jobs.csv`\n",
    "    - Investments: `files/output/p1_investments.csv`\n",
    "    - Partner investments: `files/output/p1_investments_partner.csv`\n",
    "2. Select date and filter the dataframes by date\n",
    "3. Save filtered dataframes as separate CSVs, and then load in as SFrames.\n",
    "    - Crunchbase network: `files/output/graph_temp/cb/{}_df.csv`\n",
    "    - Pledge 1% network: `files/output/graph_temp/p1/{}_df.csv`\n",
    "    - Model network: `files/output/graph_temp/model/{}_df.csv`\n",
    "    - Not Pledge 1% network: `files/output/graph_temp/np1/{}_df.csv`\n",
    "4. Load SFrames into graph and remove duplicate edges. Produce 8 graphs based on # of edges allowed & direction.\n",
    "5. Reduce size of dataset by limiting degrees of freedom from Pledge 1% companies, and save the vertices list for a few different network sizes\n",
    "6. Produce 100 samples of the Crunchbase graphs and save to CSV.\n",
    "    - 5 Degrees from Pledge 1% Companies: `Model_DF_D5`\n",
    "        - Baseline: `files/output/Model_DF_D5/B/{}.csv`\n",
    "        - Baseline Reduced: `files/output/Model_DF_D5/BR/{}.csv`\n",
    "        - Graph & Baseline: `files/output/Model_DF_D5/GB/{}.csv`\n",
    "        - Graph & Baseline Reduced: `files/output/Model_DF_D5/GBR/{}.csv`\n",
    "        - Graph: `files/output/Model_DF_D5/G/{}.csv`\n",
    "    - 4 Degrees from Pledge 1% Companies: `Model_DF_D4`\n",
    "        - Baseline: `files/output/Model_DF_D4/B/{}.csv`\n",
    "        - Baseline Reduced: `files/output/Model_DF_D4/BR/{}.csv`\n",
    "        - Graph & Baseline: `files/output/Model_DF_D4/GB/{}.csv`\n",
    "        - Graph & Baseline Reduced: `files/output/Model_DF_D4/GBR/{}.csv`\n",
    "        - Graph: `files/output/Model_DF_D4/G/{}.csv`\n",
    "\n",
    "## **Model**\n",
    "`p1_tag` ~ `rank` + `total_funding_usd` + `age_yr` + `employee_count` (ordinal) + `country` (nominal, 112 indicator columns) + `category_groups` (nominal, 46 indicator columns) + ((GRAPH FEATURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''Importing basic data analysis packages'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "'''Graph'''\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import turicreate\n",
    "from turicreate import pagerank, kcore, degree_counting, shortest_path, connected_components, triangle_counting\n",
    "from turicreate import SFrame, SGraph, SArray, load_sgraph, aggregate \n",
    "\n",
    "'''Plotting packages'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='white', font_scale=1.3)\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):   \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def network_by_date(date, df_input, jobs_input, invest_input, invest_prtnr_input, model_uuids=[], skip_not_p1=True):\n",
    "    '''\n",
    "    This function filters down Crunchbase dataframes by date \n",
    "    to ensure that the companies/people/investments being used in modeling exist at a given time.\n",
    "\n",
    "    INPUT:\n",
    "        - `date`: string w/ format 'YEAR-MO-DY' (e.g. '2020-09-08')\n",
    "        - `df`: pandas dataframe of Crunchbase organizationss with necessary column fields:\n",
    "            * `p1_date`, `founded_on`, `closed_on`\n",
    "        - `jobs`: pandas dataframe of Crunchbase jobss with necessary column fields:\n",
    "            * `p1_date`, `started_on`, `ended_on`\n",
    "        - `invest`: pandas dataframe of Crunchbase investmentss with necessary column fields:\n",
    "            * `p1_date`, `announced_on`\n",
    "        - `invest_prtnr`: pandas dataframe of Crunchbase investments with necessary column fields:\n",
    "            * `p1_date`, `announced_on`\n",
    "        - `model_uuids`: list that contains the uuids of organizations that are used to construct the model graph\n",
    "        - `skip_no_p1`: Boolean that defaults to excluding the opposite of the Pledge 1% neighborhood. Likely will delete option altogether later.\n",
    "    \n",
    "    OUTPUT:\n",
    "        - List of dataframe lists, 2 lists of length 12: \n",
    "            * [Crunchbase neighborhood dataframes], [Pledge 1% neighborhood dataframes]\n",
    "                                        OR\n",
    "              [Crunchbase neighborhood dataframes], [Model neighborhood dataframes]\n",
    "        - Each dataframe list contains 12 dataframes that will be saved & loaded as SFrames in the next processing step.\n",
    "            0. Companies\n",
    "            1. Investors\n",
    "            2. Investments\n",
    "            3. Partner investments\n",
    "            4. Current Jobs\n",
    "            5. Former jobs\n",
    "            6. Former affiliated's new jobs\n",
    "            7. Partner investor's affiliation (if not in jobs dataframes)\n",
    "            8. Partner investor's coworkers at the investing firm\n",
    "            9. Partner investor's coworkers' partner investments\n",
    "            10. Current affiliated's old jobs\n",
    "            11. Organization nodes from edges in 2,3,6,7,9,10 if not already in 0 or 1\n",
    "    '''\n",
    "    # Soft copy of dataframes\n",
    "    df = df_input.copy()\n",
    "    jobs = jobs_input.copy()\n",
    "    invest = invest_input.copy()\n",
    "    invest_prtnr = invest_prtnr_input.copy()\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # DATE PROCESSING\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    df['p1_date'] = pd.to_datetime(df['p1_date'], errors='coerce')\n",
    "    df['founded_on'] = pd.to_datetime(df['founded_on'], errors='coerce')\n",
    "    df['closed_on'] = pd.to_datetime(df['closed_on'], errors='coerce')\n",
    "    jobs['p1_date'] = pd.to_datetime(jobs['p1_date'], errors='coerce')\n",
    "    jobs['started_on'] = pd.to_datetime(jobs['started_on'], errors='coerce')\n",
    "    jobs['ended_on'] = pd.to_datetime(jobs['ended_on'], errors='coerce')\n",
    "    invest['p1_date'] = pd.to_datetime(invest['p1_date'], errors='coerce')\n",
    "    invest['announced_on'] = pd.to_datetime(invest['announced_on'], errors='coerce')\n",
    "    invest_prtnr['p1_date'] = pd.to_datetime(invest_prtnr['p1_date'], errors='coerce')\n",
    "    invest_prtnr['announced_on'] = pd.to_datetime(invest_prtnr['announced_on'], errors='coerce')\n",
    "    \n",
    "    # Convert input date to datetime object\n",
    "    date = pd.Timestamp(date)\n",
    "    print('\\nAS OF {}:\\n'.format(date.strftime('%B %d, %Y').upper()))\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # Create new row for tagging model companies\n",
    "    df['add_to_model'] = 0\n",
    "    df['add_to_model'][df['uuid'].isin(model_uuids)] = 1\n",
    "    jobs['add_to_model'] = 0\n",
    "    jobs['add_to_model'][jobs['org_uuid'].isin(model_uuids)] = 1\n",
    "    invest['add_to_model'] = 0\n",
    "    invest['add_to_model'][invest['org_uuid'].isin(model_uuids)] = 1\n",
    "    invest_prtnr['add_to_model'] = 0\n",
    "    invest_prtnr['add_to_model'][invest_prtnr['org_uuid'].isin(model_uuids)] = 1\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # COMPANY FILTER\n",
    "    # Crunchbase company must be founded after DATE and closed before DATE (or DATE == NaT)\n",
    "    CB_companies = df[(df['founded_on']<=date) & \n",
    "                      ((df['closed_on']>date) | (pd.isnull(df['closed_on']))) & \n",
    "                      (df['primary_role']=='company')].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # INVESTOR FILTER:\n",
    "    # Crunchbase investor must be founded AFTER date and closed BEFORE date (or date == NaT)\n",
    "    CB_investors = df[(df['founded_on']<=date) & \n",
    "                      ((df['closed_on']>date) | (pd.isnull(df['closed_on']))) & \n",
    "                      (df['primary_role']=='investor')].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # INVESTMENT FILTER\n",
    "    # Crunchbase investment must have taken place BEFORE date\n",
    "    CB_investments = invest[(invest['announced_on']<=date) & \n",
    "                            (invest['investor_type']=='organization')].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # PARTNER INVESTMENT FILTER\n",
    "    # Crunchbase partner investment must have taken place BEFORE date\n",
    "    CB_investment_partners = invest_prtnr[invest_prtnr['announced_on']<=date].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # CURRENT JOB FILTER\n",
    "    # Crunchbase job must have started BEFORE date and ended AFTER date (or date == NaT)\n",
    "    CB_jobs = jobs[(jobs['job_type'].isin(['executive','board_member','advisor','board_observer'])) & \n",
    "                      (jobs['started_on']<=date) & \n",
    "                      ((jobs['ended_on']>date) | (pd.isnull(jobs['ended_on'])))].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # FORMER JOB FILTER\n",
    "    # Crunchbase job must have ended BEFORE date or started AFTER date\n",
    "    CB_jobs_former = jobs[(jobs['job_type'].isin(['executive','board_member','advisor','board_observer'])) & \n",
    "                          ((jobs['ended_on']<=date) | (jobs['started_on']>date))].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # COMBINE THESE 6 (or 7) INTO LIST OF FRAMES\n",
    "    lst_of_frames = []\n",
    "    # Crunchbase frames\n",
    "    CB_frames = [CB_companies,CB_investors,CB_investments,CB_investment_partners,CB_jobs,CB_jobs_former]\n",
    "    # Add to list of frames\n",
    "    lst_of_frames.append(CB_frames)\n",
    "    # If model_uuids are not supplied, calculate Pledge 1% neighborhood\n",
    "    if model_uuids == []:\n",
    "        P1_frames = []\n",
    "        for frame in CB_frames:\n",
    "            # Pledge 1% frames must have Crunchbase assumptions in addition to an earlier pledge date\n",
    "            new_frame = frame[frame['p1_date']<=date].reset_index(drop=True).drop('add_to_model',axis=1)\n",
    "            P1_frames.append(new_frame)\n",
    "        # Add to list of frames\n",
    "        lst_of_frames.append(P1_frames)\n",
    "    # If model_uuids are supplied, calculate model neighborhood\n",
    "    if model_uuids != []:\n",
    "        model_frames = []\n",
    "        for frame in CB_frames:\n",
    "            # Include model dataframe if condition satisfied: either are a Pledge 1% company or tagged by model_uuids\n",
    "            new_frame=frame[(frame['p1_date']<=date) | (frame['add_to_model']==1)].reset_index(drop=True).drop('add_to_model',axis=1)\n",
    "            model_frames.append(new_frame)\n",
    "        # Add to list of frames\n",
    "        lst_of_frames.append(model_frames)\n",
    "    # If this boolean value is False, calculate ~Pledge 1% neighborhood\n",
    "    if skip_not_p1 is False:\n",
    "        not_P1_frames = []\n",
    "        for frame in CB_frames:\n",
    "            # Non-Pledge 1% frames must have Crunchbase assumptions in addition to NaT pledge date or later pledge date\n",
    "            new_frame = frame[(pd.isnull(frame['p1_date']) | (frame['p1_date']>date))].reset_index(drop=True).drop('add_to_model',axis=1)\n",
    "            not_P1_frames.append(new_frame)\n",
    "        # Add to list of frames\n",
    "        lst_of_frames.append(not_P1_frames) \n",
    "    # Remove extra column 'add_to_model'\n",
    "    for idx,frame in enumerate(CB_frames):\n",
    "        CB_frames[idx] = frame.drop('add_to_model',axis=1)\n",
    "\n",
    "    #*******************************************************************************************************\n",
    "    # FORMER NEW JOB FILTER\n",
    "    print('CaLcUlAtInG... FORMER NEW JOB FILTER')\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        # Where do the former affiliated work now?\n",
    "        # Pull their uuids\n",
    "        former_people = frame[5].person_uuid.unique()\n",
    "        # Pull their current jobs from Crunchbase\n",
    "        jobs_former_new = CB_frames[4][CB_frames[4].person_uuid.isin(former_people)] \n",
    "        # Check they're not already in the current jobs dataframe\n",
    "        # Combine into one temp data frame\n",
    "        combined_jobs = pd.concat([frame[4], jobs_former_new]).reset_index(drop=True) \n",
    "        df_gpby = combined_jobs.groupby(list(combined_jobs.columns))\n",
    "        # Only count non-duplicated columns\n",
    "        idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "        # Reindex dataframe\n",
    "        jobs_former_new = combined_jobs.reindex(idx)\n",
    "        # Add to list of frames\n",
    "        frame.append(jobs_former_new)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # PARTNER INVESTMENT JOB FILTER\n",
    "    print('CaLcUlAtInG... PARTNER INVESTMENT JOB FILTER')\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        # Are the partner investment jobs already in one of the jobs dataframes? If not, we should add them.\n",
    "        # Create temporary dataframe and column to make checking the intersection between dataframes easier \n",
    "        # frame[4]: current jobs | frame[5]: former jobs | frame[6]: former new jobs\n",
    "        jobs_combined = pd.concat([frame[4],frame[5],frame[6]])\n",
    "        jobs_combined['person,company'] = jobs_combined['person_uuid'] + ',' + jobs_combined['org_uuid']\n",
    "        # frame[3]: partner investments\n",
    "        frame[3]['person,company'] = frame[3]['partner_uuid']+ ',' + frame[3]['investor_uuid']\n",
    "        # Number of unique partner investments\n",
    "        unique_PI = frame[3]['person,company'].unique()\n",
    "        # Overlap between PI and combined J frames, create temporary jobs view\n",
    "        # These PI are already found in J frames, so we do not need to include them\n",
    "        jobs_already_in_J = jobs_combined[jobs_combined['person,company'].isin(unique_PI)] \n",
    "        # This will return non intersecting value of PI with temp J\n",
    "        # These PI are not found in J, so we would like to include them\n",
    "        PI_not_in_J = np.setdiff1d(unique_PI,jobs_already_in_J['person,company'].unique())\n",
    "        # Need to create separate jobs dataframe for non intersecting PI/J person/company pairs\n",
    "        grouped = frame[3][frame[3]['person,company'].isin(PI_not_in_J)].groupby(['partner_uuid','partner_name','investor_uuid','investor_name']).count()\n",
    "        grouped_df = grouped.reset_index()[['partner_uuid','partner_name','investor_uuid','investor_name']]\n",
    "        grouped_df['job_type'] = 'executive'\n",
    "        # Add to list of frames\n",
    "        frame.append(grouped_df)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # OTHER FIRM PARNTERS\n",
    "    print('CaLcUlAtInG... OTHER FIRM PARTNER JOBS & INVESTMENTS FILTER')\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        # OTHER FIRM PARNTERS - JOBS\n",
    "        # Who are the other partners that work at the investment firms present in the neighborhood?\n",
    "        # Get the unique investor uuids associated with the dataframes\n",
    "        # frame[2]: from investments dataframe\n",
    "        unique_investor_firm_A = list(frame[2]['investor_uuid'].unique())\n",
    "        # frame[3]: from partner investments dataframe\n",
    "        unique_investor_firm_B = list(frame[3]['investor_uuid'].unique())\n",
    "        partners = list(frame[3]['partner_uuid'].unique())\n",
    "        # Combine to get list of unique uuids of VC firms\n",
    "        unique_firms = list(set(unique_investor_firm_A+unique_investor_firm_B))\n",
    "        # Grab current jobs from Crunchbase for these investing firms\n",
    "        # Exclude duplicate partner job (already represented by partners list calculated above)\n",
    "        partner_jobs = CB_frames[4][(CB_frames[4]['org_uuid'].isin(unique_firms)) &  \n",
    "                                    ~(CB_frames[4]['person_uuid'].isin(partners))].reset_index(drop=True)\n",
    "        # Check they're not already in the current/former jobs dataframe\n",
    "        # Combine into one temp data frame\n",
    "        combined_jobs = pd.concat([frame[4], partner_jobs]).reset_index(drop=True) \n",
    "        df_gpby = combined_jobs.groupby(list(combined_jobs.columns))\n",
    "        # Only count non-duplicated rows\n",
    "        idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "        # Reindex dataframe\n",
    "        partner_jobs = combined_jobs.reindex(idx)\n",
    "        # Add to list of frames\n",
    "        frame.append(partner_jobs)\n",
    "        # OTHER FIRM PARNTERS - PARTNER INVESTMENTS\n",
    "        # For these new partners, what companies are they invested in?\n",
    "        # Get the unique parnter uuids associated with the dataframes\n",
    "        other_partners = partner_jobs['person_uuid'].unique()\n",
    "        other_partner_investments = CB_frames[3][CB_frames[3]['partner_uuid'].isin(other_partners)]\n",
    "        # Check they're not already in the partner investments dataframe\n",
    "        # Combine into one temp data frame\n",
    "        combined_jobs = pd.concat([frame[3], other_partner_investments]).reset_index(drop=True) \n",
    "        df_gpby = combined_jobs.groupby(list(combined_jobs.columns))\n",
    "        # Only count non-duplicated rows\n",
    "        idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "        # Reindex dataframe\n",
    "        other_partner_investments = combined_jobs.reindex(idx)\n",
    "        # Add to list of frames\n",
    "        frame.append(other_partner_investments)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # CURRENT OLD JOB FILTER\n",
    "    print('CaLcUlAtInG... CURRENT OLD JOB FILTER')\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        # Where did the current affiliated work previously?\n",
    "        current_people = frame[4].person_uuid.unique() # Pull their IDs\n",
    "        jobs_current_old = CB_frames[5][CB_frames[5].person_uuid.isin(current_people)] # Pull their current jobs from Crunchbase\n",
    "        # Check they're not already in the current jobs dataframe\n",
    "        # Combine into one temp data frame\n",
    "        combined_jobs = pd.concat([frame[5], jobs_current_old]).reset_index(drop=True) \n",
    "        df_gpby = combined_jobs.groupby(list(combined_jobs.columns))\n",
    "        # Only count non-duplicated columns\n",
    "        idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "        # Reindex dataframe\n",
    "        jobs_current_old = combined_jobs.reindex(idx)\n",
    "        # Add to list of frames\n",
    "        frame.append(jobs_current_old)\n",
    "        \n",
    "    #*******************************************************************************************************\n",
    "    # GET EXTRA ORG UUID ATTRIBUTES FROM INVESTMENTS & JOBS\n",
    "    print('CaLcUlAtInG... EXTRA ORGANIZATION NODES')\n",
    "    \n",
    "    CB_orgs = pd.concat([CB_companies, CB_investors])\n",
    "    for frame in lst_of_frames:\n",
    "        unique_orgs = []\n",
    "        # Investments\n",
    "        unique_orgs.extend(list(frame[2]['investor_uuid'].unique()))\n",
    "        # Partner investments\n",
    "        unique_orgs.extend(list(frame[3]['investor_uuid'].unique()))\n",
    "        # Former new jobs organizations\n",
    "        unique_orgs.extend(list(frame[6]['org_uuid'].unique()))\n",
    "        # Parter jobs organizations\n",
    "        unique_orgs.extend(list(frame[7]['investor_uuid'].unique()))\n",
    "        # Other parter investments organizations\n",
    "        unique_orgs.extend(list(frame[9]['org_uuid'].unique()))\n",
    "        # Current old jobs organizations\n",
    "        unique_orgs.extend(list(frame[10]['org_uuid'].unique()))\n",
    "        # Pull their organization information from Crunchbase\n",
    "        new_org_nodes = CB_orgs[CB_orgs['uuid'].isin(list(set(unique_orgs)))]\n",
    "        # Add to list of frames\n",
    "        frame.append(new_org_nodes)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    \n",
    "    # Output print statements\n",
    "    print('\\nCrunchbase Neighborhood')\n",
    "    print('NODES | OUTPUT FRAME 0/CB_companies {}'.format(CB_frames[0].shape))\n",
    "    print('NODES | OUTPUT FRAME 1/CB_investors {}'.format(CB_frames[1].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 2/CB_investments {}'.format(CB_frames[2].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 3/CB_investment_partners {}'.format(CB_frames[3].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 4/CB_jobs {}'.format(CB_frames[4].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 5/CB_jobs_former {}'.format(CB_frames[5].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 6/CB_jobs_former_new {}'.format(CB_frames[6].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 7/CB_jobs_partner {}'.format(CB_frames[7].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 8/CB_jobs_other_partners {}'.format(CB_frames[8].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 9/CB_invest_other_partners {}'.format(CB_frames[9].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 10/CB_jobs_current_old {}'.format(CB_frames[10].shape))\n",
    "    print('NODES | OUTPUT FRAME 11/CB_extra_org_nodes {}'.format(CB_frames[11].shape))\n",
    "    if model_uuids != []:\n",
    "        print('\\nModel Neighborhood')\n",
    "        print('NODES | OUTPUT FRAME 0/model_companies {}'.format(model_frames[0].shape))\n",
    "        print('NODES | OUTPUT FRAME 1/model_investors {}'.format(model_frames[1].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 2/model_investments {}'.format(model_frames[2].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 3/model_investment_partners {}'.format(model_frames[3].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 4/model_jobs {}'.format(model_frames[4].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 5/model_jobs_former {}'.format(model_frames[5].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 6/model_jobs_former_new {}'.format(model_frames[6].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 7/model_jobs_partner {}'.format(model_frames[7].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 8/model_jobs_other_partners {}'.format(model_frames[8].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 9/model_invest_other_partners {}'.format(model_frames[9].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 10/model_jobs_current_old {}'.format(model_frames[10].shape))\n",
    "        print('NODES | OUTPUT FRAME 11/model_extra_org_nodes {}'.format(model_frames[11].shape))\n",
    "        return lst_of_frames\n",
    "    print('\\nPledge 1% Neighborhood')\n",
    "    print('NODES | OUTPUT FRAME 0/P1_companies {}'.format(P1_frames[0].shape))\n",
    "    print('NODES | OUTPUT FRAME 1/P1_investors {}'.format(P1_frames[1].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 2/P1_investments {}'.format(P1_frames[2].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 3/P1_investment_partners {}'.format(P1_frames[3].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 4/P1_jobs {}'.format(P1_frames[4].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 5/P1_jobs_former {}'.format(P1_frames[5].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 6/P1_jobs_former_new {}'.format(P1_frames[6].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 7/P1_jobs_partner {}'.format(P1_frames[7].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 8/P1_jobs_other_partners {}'.format(P1_frames[8].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 9/P1_invest_other_partners {}'.format(P1_frames[9].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 10/P1_jobs_current_old {}'.format(P1_frames[10].shape))\n",
    "    print('NODES | OUTPUT FRAME 11/P1_extra_org_nodes {}'.format(P1_frames[11].shape))\n",
    "    # Skip Not P1 Calculations\n",
    "    if skip_not_p1 is False:\n",
    "        print('\\n~Pledge 1% Neighborhood')\n",
    "        print('NODES | OUTPUT FRAME 0/not_P1_companies {}'.format(not_P1_frames[0].shape))\n",
    "        print('NODES | OUTPUT FRAME 1/not_P1_investors {}'.format(not_P1_frames[1].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 2/not_P1_investments {}'.format(not_P1_frames[2].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 3/not_P1_investment_partners {}'.format(not_P1_frames[3].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 4/not_P1_jobs {}'.format(not_P1_frames[4].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 5/not_P1_jobs_former {}'.format(not_P1_frames[5].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 6/not_P1_jobs_former_new {}'.format(not_P1_frames[6].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 7/not_P1_jobs_partner {}'.format(not_P1_frames[7].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 8/not_P1_jobs_other_partners {}'.format(not_P1_frames[8].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 9/not_P1_invest_other_partners {}'.format(not_P1_frames[9].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 10/not_P1_jobs_current_old {}'.format(not_P1_frames[10].shape))\n",
    "        print('NODES | OUTPUT FRAME 11/not_P1_extra_org_nodes {}'.format(not_P1_frames[11].shape))\n",
    "    return lst_of_frames\n",
    "\n",
    "def load_vertices(sframes, g):\n",
    "    # For jobs dataframes\n",
    "    for idx in [4,5,6,8,10]:\n",
    "        # Keep relevant node attributes\n",
    "        frame_temp = sframes[idx][['person_uuid', 'person_name']].rename({'person_uuid':'__id', 'person_name':'name'})\n",
    "        frame_temp['__node_type'] = 'person'\n",
    "        # Add p1_tag to the vertex\n",
    "        frame_temp['p1_tag'] = 0\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    # For jobs and partner investments dataframes\n",
    "    for idx in [2,3,4,5,6,8,9,10]:\n",
    "        # Keep relevant node attributes\n",
    "        frame_temp = sframes[idx][['org_uuid', 'org_name', 'p1_tag']].rename({'org_uuid':'__id', 'org_name':'name'})\n",
    "        frame_temp['__node_type'] = 'company'\n",
    "        # Add p1_tag to the vertex\n",
    "        frame_temp['p1_tag'] = frame_temp['p1_tag'].apply(lambda x: 0 if (x==\"\" or x==0) else 1)\n",
    "        frame_temp['p1_tag'] = frame_temp['p1_tag'].astype(int)\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    # For investments dataframes\n",
    "    for idx in [2,3,7,9]:\n",
    "        # Keep relevant node attributes\n",
    "        frame_temp = sframes[idx][['investor_uuid', 'investor_name']].rename({'investor_uuid':'__id', 'investor_name':'name'})\n",
    "        frame_temp['__node_type'] = 'investor'\n",
    "        # Add p1_tag to the vertex\n",
    "        frame_temp['p1_tag'] = 0\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    # For partner investments dataframes\n",
    "    for idx in [3,7,9]:\n",
    "        # Keep relevant node attributes\n",
    "        frame_temp = sframes[idx][['partner_uuid', 'partner_name']].rename({'partner_uuid':'__id', 'partner_name':'name'})\n",
    "        frame_temp['__node_type'] = 'person'\n",
    "        # Add p1_tag to the vertex\n",
    "        frame_temp['p1_tag'] = 0\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    # Organizations\n",
    "    for idx in [0,1,11]:\n",
    "        # Keep relevant node attributes\n",
    "        frame_temp = sframes[idx][['uuid', 'name', 'primary_role', 'p1_tag']].rename({'uuid':'__id', 'primary_role':'__node_type'})\n",
    "        # Add p1_tag to the vertex\n",
    "        frame_temp['p1_tag'] = frame_temp['p1_tag'].apply(lambda x: 0 if (x==\"\" or x==0) else 1)\n",
    "        frame_temp['p1_tag'] = frame_temp['p1_tag'].astype(int)\n",
    "        # Load into graph\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    # Return SGraph\n",
    "    return g\n",
    "\n",
    "def find_p1_affiliations(p1_sframes):\n",
    "    frames = p1_sframes.copy()\n",
    "    # Combine company and investor Pledge 1% dataframes, keeping only the uuid column\n",
    "    p1_affiliations = frames[0][['uuid']].append(frames[1][['uuid']])\n",
    "    # Add edge connecting to Pledge 1% uuid\n",
    "    p1_affiliations['p1_uuid'] = 'fd9e2d10-a882-c6f4-737e-fd388d4ffd7c'\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    p1_affiliations = p1_affiliations.rename({'uuid':'src','p1_uuid':'dst'})\n",
    "    p1_affiliations['p1_tag'] = 1\n",
    "    # Return SFrame\n",
    "    return p1_affiliations\n",
    "\n",
    "def load_edges(sframes, g, p1_affiliations=[], include_edges=[2,3], reverse=False, add_weights=False):\n",
    "    w = {'status':{'primary':3,'secondary':2,'tertiary':1}, '__edge_type':{'job':1, 'investment':2}}\n",
    "    # Since it is a directed graph, need to include option for reverse direction\n",
    "    # Forward\n",
    "    source = 'src'\n",
    "    destination = 'dst'\n",
    "    # Reverse\n",
    "    if reverse:\n",
    "        source = 'dst'\n",
    "        destination = 'src'\n",
    "    if type(p1_affiliations) == SFrame:\n",
    "        # P1 Companies: Company/Investor --> Pledge 1%\n",
    "        g = g.add_edges(edges=p1_affiliations, src_field=source, dst_field=destination)\n",
    "        if add_weights:\n",
    "            frame_temp['weight'] = 6\n",
    "    # Investments: Investor --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[2][['investment_uuid','investor_uuid','org_uuid','investment_type','raised_amount_usd','investor_count','is_lead_investor','lead_investor_count']].rename({'investment_uuid':'__id','investor_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 'primary'\n",
    "    if add_weights:\n",
    "        frame_temp['weight'] = w['__edge_type']['investment'] * w['status']['primary']\n",
    "    g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "    # Partner Investments, Investments: Person --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[3][['investment_uuid','partner_uuid','org_uuid','investment_type','raised_amount_usd','investor_count']].rename({'investment_uuid':'__id','partner_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 'primary'\n",
    "    if add_weights:\n",
    "        frame_temp['weight'] = w['__edge_type']['investment'] * w['status']['primary']\n",
    "    g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "    # Partner Investments, Investments: Investor --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[3][['investor_uuid','org_uuid','investment_type','investor_count']].rename({'investor_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 'secondary'\n",
    "    if add_weights:\n",
    "        frame_temp['weight'] = w['__edge_type']['investment'] * w['status']['secondary']\n",
    "    # Secondary relationships, skip if not specified at input\n",
    "    if 2 in include_edges:\n",
    "        g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "    # Partner Investments, Jobs: Person --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[7][['partner_uuid','investor_uuid']].rename({'partner_uuid':'src','investor_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'job'\n",
    "    frame_temp['status'] = 'secondary'\n",
    "    if add_weights:\n",
    "        frame_temp['weight'] = w['__edge_type']['job'] * w['status']['secondary']\n",
    "    # Secondary relationships, skip if not specified at input\n",
    "    if 2 in include_edges:\n",
    "        g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)    \n",
    "    # Other Partner Investments, Investments: Person --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[9][['investment_uuid','partner_uuid','org_uuid','investment_type','raised_amount_usd','investor_count']].rename({'investment_uuid':'__id','partner_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 'tertiary'\n",
    "    if add_weights:\n",
    "        frame_temp['weight'] = w['status']['tertiary'] * w['__edge_type']['investment']\n",
    "    # Tertiary relationships, skip if not specified at input\n",
    "    if 3 in include_edges:\n",
    "        g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "    # Jobs: Person --> Company\n",
    "    for idx in [4,5,6,8,10]:\n",
    "        # Create id, source, destination fields in SFrame\n",
    "        frame_temp = sframes[idx][['job_uuid','person_uuid','org_uuid','job_type','title']].rename({'job_uuid':'__id','person_uuid':'src','org_uuid':'dst'})\n",
    "        frame_temp['__edge_type'] = 'job'\n",
    "        # Current jobs\n",
    "        if idx == 4:\n",
    "            frame_temp['status'] = 'primary'\n",
    "            if add_weights:\n",
    "                frame_temp['weight'] = w['status']['primary'] * w['__edge_type']['job']\n",
    "            g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "            continue\n",
    "        # Secondary relationships, skip if not specified at input\n",
    "        if 2 in include_edges:\n",
    "            # Former jobs | Former new jobs | Current old jobs \n",
    "            if idx in [5,6,10]:\n",
    "                frame_temp['status'] = 'secondary'\n",
    "                if add_weights:\n",
    "                    frame_temp['weight'] = w['status']['secondary'] * w['__edge_type']['job']\n",
    "                g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "                continue  \n",
    "        # Tertiary relationships, skip if not specified at input\n",
    "        if 3 in include_edges:\n",
    "            # Other partners at firm\n",
    "            if idx == 8:\n",
    "                frame_temp['status'] = 'tertiary'\n",
    "                if add_weights:\n",
    "                    frame_temp['weight'] = w['status']['tertiary'] * w['__edge_type']['job']\n",
    "                g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "                continue\n",
    "    # Return SGraph\n",
    "    return g\n",
    "\n",
    "def update_cb_weights(src, edge, dst):\n",
    "    if src['__id'] != dst['__id']: # ignore self-links\n",
    "        edge['weight'] = 0\n",
    "        edge['weight_status'] = 0\n",
    "        edge['weight_type'] = 0\n",
    "        if edge['status'] == 'primary':\n",
    "            edge['weight_status'] = 3\n",
    "        if edge['status'] == 'secondary':\n",
    "            edge['weight_status'] = 2\n",
    "        if edge['status'] == 'tertiary':\n",
    "            edge['weight_status'] = 1\n",
    "        if edge['__edge_type'] == 'job':\n",
    "            edge['weight_type'] = 1\n",
    "        if edge['__edge_type'] == 'investment':\n",
    "            edge['weight_type'] = 2\n",
    "        edge['weight'] = edge['weight_status'] * edge['weight_type']\n",
    "    return (src, edge, dst)\n",
    "\n",
    "def update_pagerank_weight(src, edge, dst):\n",
    "    if src['__id'] != dst['__id']: # ignore self-links\n",
    "        dst['pagerank'] += src['prev_pagerank'] * edge['weight']\n",
    "    return (src, edge, dst)\n",
    "\n",
    "def update_pagerank_reset_prob(src, edge, dst):\n",
    "    global reset\n",
    "    if src['__id'] != dst['__id']: # ignore self-links\n",
    "        dst['pagerank'] *= (1 - reset)\n",
    "        dst['pagerank'] += reset\n",
    "    return (src, edge, dst)\n",
    "\n",
    "def update_pagerank_prev_to_current(src, edge, dst):\n",
    "    if src['__id'] != dst['__id']: # ignore self-links\n",
    "        src['prev_pagerank'] = src['pagerank']\n",
    "    return (src, edge, dst)\n",
    "\n",
    "def sum_weight(src, edge, dst):\n",
    "    if src['__id'] != dst['__id']: # ignore self-links\n",
    "        src['total_weight'] += edge['weight']\n",
    "    return src, edge, dst\n",
    "\n",
    "def make_pagerank_zero(src, edge, dst):\n",
    "    if src['__id'] != dst['__id']: # ignore self-links\n",
    "        dst['pagerank'] = 0\n",
    "    return src, edge, dst\n",
    "\n",
    "def update_l1_delta(src, edge, dst):\n",
    "    if src['__id'] != dst['__id']: # ignore self-links\n",
    "        dst['l1_delta'] = abs(dst['pagerank'] - dst['prev_pagerank'])\n",
    "        src['l1_delta'] = abs(src['pagerank'] - src['prev_pagerank'])\n",
    "    return src, edge, dst\n",
    "\n",
    "def normalize_weight(src, edge, dst):\n",
    "    if src['__id'] != dst['__id']: # ignore self-links\n",
    "        edge['weight'] /= src['total_weight']\n",
    "    return src, edge, dst\n",
    "\n",
    "def pagerank_weighted(input_graph, reset_prob=0.15, threshold=0.01, max_iterations=3):\n",
    "    g = SGraph(input_graph.vertices, input_graph.edges)\n",
    "    global reset\n",
    "    reset = reset_prob\n",
    "    # compute normalized edge weight\n",
    "    g.vertices['total_weight'] = 0.0\n",
    "    g = g.triple_apply(sum_weight, ['total_weight'])\n",
    "    g = g.triple_apply(normalize_weight, ['weight'])\n",
    "    del g.vertices['total_weight']\n",
    "    # initialize vertex field\n",
    "    g.vertices['prev_pagerank'] = 1.0\n",
    "    it = 0\n",
    "    total_l1_delta = len(g.vertices)\n",
    "    start = time.time()\n",
    "    while(total_l1_delta > threshold and it < max_iterations):\n",
    "        if 'pagerank' not in g.get_vertex_fields():\n",
    "            g.vertices['pagerank'] = 0.0\n",
    "        else:\n",
    "            g = g.triple_apply(make_pagerank_zero, ['pagerank'])\n",
    "        g = g.triple_apply(update_pagerank_weight, ['pagerank'])\n",
    "        g = g.triple_apply(update_pagerank_reset_prob, ['pagerank'])\n",
    "        if 'l1_delta' not in g.get_vertex_fields():\n",
    "            g.vertices['l1_delta'] = (g.vertices['pagerank'] - g.vertices['prev_pagerank']).apply(lambda x: abs(x))\n",
    "        else:\n",
    "            g = g.triple_apply(update_l1_delta, ['l1_delta'])\n",
    "        total_l1_delta = g.vertices['l1_delta'].sum()\n",
    "        g = g.triple_apply(update_pagerank_prev_to_current, ['prev_pagerank'])\n",
    "        print (\"Iteration %d: total pagerank changed in L1 = %f\" % (it, total_l1_delta))\n",
    "        it = it + 1\n",
    "    print (\"Weighted pagerank finished in: %f secs\" % (time.time() - start))\n",
    "    del g.vertices['prev_pagerank']\n",
    "    return g.vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `w_spath`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wspath = pd.read_csv('wspath.csv')\n",
    "# wspath['__id'] = wspath['uuid']\n",
    "# wspath = wspath.drop('uuid', axis=1)\n",
    "# wspath.to_csv('wspath.csv', index=False)\n",
    "# wspath = pd.read_csv('wspath.csv')\n",
    "# wspath.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spath = pd.read_csv('wspath.csv')\n",
    "\n",
    "# for neighborhood in ['Model_DF_D4', 'Model_DF_D5']: # 2 times\n",
    "#     for scenario in ['G', 'GB', 'GBR']: # 3 times\n",
    "#         for idx in range(1,10): # 10 times\n",
    "#             print('{} | {} | {}'.format(neighborhood,scenario,idx))\n",
    "#             path = 'files/output/{}/{}/{}.csv'.format(neighborhood,scenario,idx)\n",
    "#             DF = pd.read_csv(path)\n",
    "#             DF_MERGE = pd.merge(DF, wspath, on='__id', how='inner')\n",
    "#             DF_MERGE.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `spath`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_DF_D4 | G | 1\n",
      "Model_DF_D4 | G | 2\n",
      "Model_DF_D4 | G | 3\n",
      "Model_DF_D4 | G | 4\n",
      "Model_DF_D4 | G | 5\n",
      "Model_DF_D4 | G | 6\n",
      "Model_DF_D4 | G | 7\n",
      "Model_DF_D4 | G | 8\n",
      "Model_DF_D4 | G | 9\n",
      "Model_DF_D4 | GB | 1\n",
      "Model_DF_D4 | GB | 2\n",
      "Model_DF_D4 | GB | 3\n",
      "Model_DF_D4 | GB | 4\n",
      "Model_DF_D4 | GB | 5\n",
      "Model_DF_D4 | GB | 6\n",
      "Model_DF_D4 | GB | 7\n",
      "Model_DF_D4 | GB | 8\n",
      "Model_DF_D4 | GB | 9\n",
      "Model_DF_D4 | GBR | 1\n",
      "Model_DF_D4 | GBR | 2\n",
      "Model_DF_D4 | GBR | 3\n",
      "Model_DF_D4 | GBR | 4\n",
      "Model_DF_D4 | GBR | 5\n",
      "Model_DF_D4 | GBR | 6\n",
      "Model_DF_D4 | GBR | 7\n",
      "Model_DF_D4 | GBR | 8\n",
      "Model_DF_D4 | GBR | 9\n",
      "Model_DF_D5 | G | 1\n",
      "Model_DF_D5 | G | 2\n",
      "Model_DF_D5 | G | 3\n",
      "Model_DF_D5 | G | 4\n",
      "Model_DF_D5 | G | 5\n",
      "Model_DF_D5 | G | 6\n",
      "Model_DF_D5 | G | 7\n",
      "Model_DF_D5 | G | 8\n",
      "Model_DF_D5 | G | 9\n",
      "Model_DF_D5 | GB | 1\n",
      "Model_DF_D5 | GB | 2\n",
      "Model_DF_D5 | GB | 3\n",
      "Model_DF_D5 | GB | 4\n",
      "Model_DF_D5 | GB | 5\n",
      "Model_DF_D5 | GB | 6\n",
      "Model_DF_D5 | GB | 7\n",
      "Model_DF_D5 | GB | 8\n",
      "Model_DF_D5 | GB | 9\n",
      "Model_DF_D5 | GBR | 1\n",
      "Model_DF_D5 | GBR | 2\n",
      "Model_DF_D5 | GBR | 3\n",
      "Model_DF_D5 | GBR | 4\n",
      "Model_DF_D5 | GBR | 5\n",
      "Model_DF_D5 | GBR | 6\n",
      "Model_DF_D5 | GBR | 7\n",
      "Model_DF_D5 | GBR | 8\n",
      "Model_DF_D5 | GBR | 9\n"
     ]
    }
   ],
   "source": [
    "# spath = pd.read_csv('spath.csv')\n",
    "\n",
    "# for neighborhood in ['Model_DF_D4', 'Model_DF_D5']: # 2 times\n",
    "#     for scenario in ['G', 'GB', 'GBR']: # 3 times\n",
    "#         for idx in range(1,10): # 10 times\n",
    "#             print('{} | {} | {}'.format(neighborhood,scenario,idx))\n",
    "#             path = 'files/output/{}/{}/{}.csv'.format(neighborhood,scenario,idx)\n",
    "#             DF = pd.read_csv(path)\n",
    "#             DF_MERGE = pd.merge(DF, spath, on='__id', how='inner')\n",
    "#             DF_MERGE.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update `__id` to `uuid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neighborhood in ['Model_DF_D4', 'Model_DF_D5']: # 2 times\n",
    "    for scenario in ['G', 'GB', 'GBR']: # 3 times\n",
    "        for idx in range(1): # 10 times\n",
    "            print('{} | {} | {}'.format(neighborhood,scenario,idx))\n",
    "            path = 'files/output/{}/{}/{}.csv'.format(neighborhood,scenario,idx)\n",
    "            DF = pd.read_csv(path)\n",
    "            DF['uuid'] = DF['__id']\n",
    "            DF = DF.drop('__id', axis=1)\n",
    "            DF_MERGE.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__id',\n",
       " 'pr_0',\n",
       " 'w_pr_0',\n",
       " 'in_deg_0',\n",
       " 'out_deg_0',\n",
       " 'tri_0',\n",
       " 'pr_1',\n",
       " 'w_pr_1',\n",
       " 'in_deg_1',\n",
       " 'out_deg_1',\n",
       " 'pr_2',\n",
       " 'w_pr_2',\n",
       " 'kc_2',\n",
       " 'in_deg_2',\n",
       " 'out_deg_2',\n",
       " 'tri_2',\n",
       " 'spath_top_3_0',\n",
       " 'spath_top_3_1',\n",
       " 'spath_top_3_2',\n",
       " 'spath_top_3_3',\n",
       " 'spath_top_3_4',\n",
       " 'spath_top_min_3',\n",
       " 'w_spath_top_3_0',\n",
       " 'w_spath_top_3_1',\n",
       " 'w_spath_top_3_2',\n",
       " 'w_spath_top_3_3',\n",
       " 'w_spath_top_3_4',\n",
       " 'w_spath_top_min_3',\n",
       " 'kc_3',\n",
       " 'in_deg_3',\n",
       " 'out_deg_3',\n",
       " 'spath']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_MERGE.columns.to_list()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
