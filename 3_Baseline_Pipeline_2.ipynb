{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Pipeline\n",
    "\n",
    "By: Aditya Mengani, Ognjen Sosa, Sanjay Elangovan, Song Park, Sophia Skowronski\n",
    "\n",
    "**Can we improve on the baseline scores using different encoding, imputing, and scaling schemes?**\n",
    "- Averaged Logistic Regression accuracy Score: 0.5\n",
    "- Averaged Linear Regression accuracy score: 0.2045\n",
    "- Averaged K-Nearest Neighbour accuracy score: 0.6198\n",
    "- Averaged Naive Bayes accuracy score: 0.649\n",
    "\n",
    "**`p1_tag` ~  `rank` + `total_funding_usd` + `employee_count` (ordinal) + `country` (nominal) + `category_groups` (nominal)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data analysis'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "import statistics\n",
    "\n",
    "'''Plotting'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "'''Stat'''\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "'''ML'''\n",
    "import prince\n",
    "import category_encoders as ce\n",
    "from sklearn import metrics, svm, preprocessing, utils\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100*(start_mem-end_mem)/start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Dataframe Columns:\n",
      "\n",
      "['ind_enc_1', 'ind_enc_2', 'ind_enc_3', 'ind_enc_4', 'ind_enc_5', 'ind_enc_6', 'ind_enc_7', 'ind_enc_8', 'ind_enc_9', 'ind_enc_10', 'ind_enc_11', 'ind_enc_12', 'ind_enc_13', 'ind_enc_14', 'ind_enc_15', 'ind_enc_16', 'ind_enc_17', 'ind_enc_18', 'ind_enc_19', 'ind_enc_20', 'ind_enc_21', 'ind_enc_22', 'ind_enc_23', 'ind_enc_24', 'ind_enc_25', 'ind_enc_26', 'ind_enc_27', 'ind_enc_28', 'ind_enc_29', 'ind_enc_30', 'ind_enc_31', 'ind_enc_32', 'ind_enc_33', 'ind_enc_34', 'ind_enc_35', 'ind_enc_36', 'ind_enc_37', 'ind_enc_38', 'ind_enc_39', 'ind_enc_40', 'ind_enc_41', 'ind_enc_42', 'ind_enc_43', 'ind_enc_44', 'ind_enc_45', 'ind_enc_46', 'country_enc_1', 'country_enc_2', 'country_enc_3', 'country_enc_4', 'country_enc_5', 'country_enc_6', 'country_enc_7', 'country_enc_8', 'country_enc_9', 'country_enc_10', 'country_enc_11', 'country_enc_12', 'country_enc_13', 'country_enc_14', 'country_enc_15', 'employee_size', 'category_groups', 'country', 'p1_tag', 'rank', 'employee_count', 'total_funding_usd', 'ind_1', 'ind_2', 'ind_3', 'ind_4', 'ind_5', 'ind_6', 'ind_7', 'ind_8', 'ind_9', 'ind_10', 'ind_11', 'ind_12', 'ind_13', 'ind_14', 'ind_15', 'ind_16', 'ind_17', 'ind_18', 'ind_19', 'ind_20', 'ind_21', 'ind_22', 'ind_23', 'ind_24', 'ind_25', 'ind_26', 'ind_27', 'ind_28', 'ind_29', 'ind_30', 'ind_31', 'ind_32', 'ind_33', 'ind_34', 'ind_35', 'ind_36', 'ind_37', 'ind_38', 'ind_39', 'ind_40', 'ind_41', 'ind_42', 'ind_43', 'ind_44', 'ind_45', 'ind_46']\n",
      "\n",
      "Mem. usage decreased to 35.60 Mb (31.2% reduction)\n",
      "\n",
      "Ending Dataframe Columns:\n",
      "\n",
      "['p1_tag', 'rank', 'country', 'employee_size', 'category_groups', 'total_funding_usd']\n",
      "\n",
      "Dataframe shape: (1131326, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-1fae7b657ce2>:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(np.int8)\n",
      "<ipython-input-1-1fae7b657ce2>:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(np.float32)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('files/output/baseline.csv')\n",
    "print('Starting Dataframe Columns:\\n\\n{}\\n'.format(df.columns.to_list()))\n",
    "\n",
    "# Have industry mapper for 'ind_1'...'ind_46' columns\n",
    "industries = ['Software', 'Information Technology', 'Internet Services', 'Data and Analytics',\n",
    "              'Sales and Marketing', 'Media and Entertainment', 'Commerce and Shopping', \n",
    "              'Financial Services', 'Apps', 'Mobile', 'Science and Engineering', 'Hardware',\n",
    "              'Health Care', 'Education', 'Artificial Intelligence', 'Professional Services', \n",
    "              'Design', 'Community and Lifestyle', 'Real Estate', 'Advertising',\n",
    "              'Transportation', 'Consumer Electronics', 'Lending and Investments',\n",
    "              'Sports', 'Travel and Tourism', 'Food and Beverage',\n",
    "              'Content and Publishing', 'Consumer Goods', 'Privacy and Security',\n",
    "              'Video', 'Payments', 'Sustainability', 'Events', 'Manufacturing',\n",
    "              'Clothing and Apparel', 'Administrative Services', 'Music and Audio',\n",
    "              'Messaging and Telecommunications', 'Energy', 'Platforms', 'Gaming',\n",
    "              'Government and Military', 'Biotechnology', 'Navigation and Mapping',\n",
    "              'Agriculture and Farming', 'Natural Resources']\n",
    "industry_map = {industry:'ind_'+str(idx+1) for idx,industry in enumerate(industries)}\n",
    "\n",
    "# Create \n",
    "df_simple = df[['p1_tag', 'rank', 'country', 'employee_size', 'category_groups', 'total_funding_usd']]\n",
    "df_simple = reduce_mem_usage(df_simple)\n",
    "\n",
    "print('\\nEnding Dataframe Columns:\\n\\n{}'.format(df_simple.columns.to_list()))\n",
    "\n",
    "print('\\nDataframe shape:', df_simple.shape)\n",
    "\n",
    "del industries, industry_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "PLEDGE 1% cols: ['uuid', 'p1_tag', 'p1_date']\n",
      "SHAPE: (7822, 3)\n",
      "\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjayelangovan/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORGANIZATION cols: ['uuid', 'name', 'type', 'permalink', 'cb_url', 'rank', 'created_at', 'updated_at', 'legal_name', 'roles', 'domain', 'homepage_url', 'country_code', 'state_code', 'region', 'city', 'address', 'postal_code', 'status', 'short_description', 'category_list', 'category_groups_list', 'num_funding_rounds', 'total_funding_usd', 'total_funding', 'total_funding_currency_code', 'founded_on', 'last_funding_on', 'closed_on', 'employee_count', 'email', 'phone', 'facebook_url', 'linkedin_url', 'twitter_url', 'logo_url', 'alias1', 'alias2', 'alias3', 'primary_role', 'num_exits']\n",
      "SHAPE: (1131310, 41)\n",
      "\n",
      "Mem. usage decreased to 132.71 Mb (14.6% reduction)\n",
      "There are 252682 entries with no founded_on date. Let's remove these from the dataset.\n",
      "Now there are None with the value of 0.\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "# Pledge 1% Company UUIDs #\n",
    "###########################\n",
    "print('*'*100)\n",
    "\n",
    "p1 = pd.read_csv('files/p1.csv')\n",
    "print('PLEDGE 1% cols: {}\\nSHAPE: {}\\n'.format(p1.columns.to_list(), p1.shape))\n",
    "\n",
    "#################\n",
    "# Organizations #\n",
    "#################\n",
    "print('*'*100)\n",
    "\n",
    "org = pd.read_csv('files/csv/organizations.csv')\n",
    "print('ORGANIZATION cols: {}\\nSHAPE: {}\\n'.format(org.columns.to_list(), org.shape))\n",
    "\n",
    "# Merge p1 and org dataframes on the organization uuid\n",
    "df = pd.merge(org.copy(),p1.copy(),how='outer',on='uuid')\n",
    "\n",
    "# Convert Boolean to binary\n",
    "df['p1_tag'] = df['p1_tag'].apply(lambda x: 1 if x == True else 0)\n",
    "p1['p1_tag'] = 1\n",
    "\n",
    "# Convert employee_count 'unknown' to NaN to get accurate missing value count\n",
    "df['employee_count'] = df['employee_count'].apply(lambda x: np.NaN if x == 'unknown' else x)\n",
    "\n",
    "# Review Pandas Profiling Report of dataframe & update columns\n",
    "df = df[['uuid','name','rank','status','employee_count','total_funding_usd','num_funding_rounds','primary_role','region','country_code','category_list','category_groups_list','founded_on','created_at','updated_at','p1_date','p1_tag']]\n",
    "\n",
    "##############\n",
    "# Timestamps #\n",
    "##############\n",
    "\n",
    "# Convert to datetime objects\n",
    "df['p1_date'] = pd.to_datetime(df['p1_date'])\n",
    "p1['p1_date'] = pd.to_datetime(p1['p1_date'])\n",
    "\n",
    "# Get OutOfBoundsDatetime error if do not coerce for CB native timestamp columns\n",
    "df['created_at'] = pd.to_datetime(df['created_at'],errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "df['updated_at'] = pd.to_datetime(df['updated_at'],errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "df['founded_on'] = pd.to_datetime(df['founded_on'],errors='coerce')\n",
    "\n",
    "# Reduce storage for numerical features\n",
    "df = reduce_mem_usage(df)\n",
    "\n",
    "# Create new pledge1 dataframe that sorts by chronological order that the company took the pledge\n",
    "pledge1 = df[df['p1_tag'] == 1].sort_values('p1_date')\n",
    "\n",
    "#Get age of each company\n",
    "now = datetime.now().date()\n",
    "df['founded_on2'] = pd.to_datetime(df['founded_on']).dt.date\n",
    "df['founded_on2'].fillna(now, inplace = True)\n",
    "\n",
    "age = []\n",
    "for i in range (len(df['founded_on'])):\n",
    "    age.append(round(((now - df['founded_on2'][i]).days)/365,3))\n",
    "    \n",
    "age_series = pd.Series(age)\n",
    "df['age'] = age_series\n",
    "\n",
    "print(f\"There are {df['age'].value_counts().get(0)} entries with no founded_on date. Let's remove these from the dataset.\")\n",
    "df['age'].replace(0, None, inplace=True)\n",
    "print(f\"Now there are {df['age'].value_counts().get(0)} with the value of 0.\")\n",
    "\n",
    "df_simple['age'] = df['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  0.52 Mb (14.6% reduction)\n",
      "Numeric features: ['rank', 'total_funding_usd', 'age']\n",
      "Categorical features: ['country', 'employee_size', 'category_groups']\n",
      "Training data shape: (12531, 6)\n",
      "Train label shape: (12531,)\n",
      "Test data shape: (3133, 6)\n",
      "Test label shape: (3133,)\n"
     ]
    }
   ],
   "source": [
    "# Select equal sample of non-Pledge 1% organizations\n",
    "df_p1 = df_simple[df_simple['p1_tag']==1]\n",
    "df_notp1 = df_simple[df_simple['p1_tag']==0].sample(n=df_p1.shape[0], replace=False)\n",
    "df_model = pd.concat([df_p1, df_notp1]).reset_index(drop=True)\n",
    "df_model = reduce_mem_usage(df_model)\n",
    "\n",
    "# Create variable for each feature type: categorical and numerical\n",
    "numeric_features = df_model.select_dtypes(include=['int8', 'int16', 'int32', 'int64', 'float16', 'float32','float64']).drop(['p1_tag'], axis=1).columns\n",
    "categorical_features = df_model.select_dtypes(include=['object']).columns\n",
    "print('Numeric features:', numeric_features.to_list())\n",
    "print('Categorical features:', categorical_features.to_list())\n",
    "\n",
    "X = df_model.drop('p1_tag', axis=1)\n",
    "y = df_model['p1_tag']\n",
    "y = preprocessing.LabelEncoder().fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print('Training data shape:', X_train.shape)\n",
    "print('Train label shape:', y_train.shape)\n",
    "print('Test data shape:',  X_test.shape)\n",
    "print('Test label shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run through pipeline to determine best categorical feature encoder\n",
    "\n",
    "From: <a href='https://towardsdatascience.com/an-easier-way-to-encode-categorical-features-d840ff6b3900'>An Easier Way to Encode Categorical Features</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRR with BackwardDifferenceEncoder\n",
      "Best parameter (CV score=0.712): {'classifier__C': 1.0}\n",
      "Best score: 0.7234\n",
      "\n",
      "LRR with BaseNEncoder\n",
      "Best parameter (CV score=0.676): {'classifier__C': 0.1}\n",
      "Best score: 0.6802\n",
      "\n",
      "LRR with BinaryEncoder\n",
      "Best parameter (CV score=0.676): {'classifier__C': 0.1}\n",
      "Best score: 0.6802\n",
      "\n",
      "LRR with CatBoostEncoder\n",
      "Best parameter (CV score=0.705): {'classifier__C': 100.0}\n",
      "Best score: 0.7078\n",
      "\n",
      "LRR with HashingEncoder\n",
      "Best parameter (CV score=nan): {'classifier__C': 0.0001}\n",
      "Best score: 0.6505\n",
      "\n",
      "LRR with HelmertEncoder\n",
      "Best parameter (CV score=0.700): {'classifier__C': 100.0}\n",
      "Best score: 0.7103\n",
      "\n",
      "LRR with JamesSteinEncoder\n",
      "Best parameter (CV score=0.695): {'classifier__C': 1.0}\n",
      "Best score: 0.6991\n",
      "\n",
      "LRR with OneHotEncoder\n",
      "Best parameter (CV score=0.717): {'classifier__C': 1.0}\n",
      "Best score: 0.7232\n",
      "\n",
      "LRR with LeaveOneOutEncoder\n",
      "Best parameter (CV score=0.704): {'classifier__C': 1000}\n",
      "Best score: 0.7082\n",
      "\n",
      "LRR with MEstimateEncoder\n",
      "Best parameter (CV score=0.688): {'classifier__C': 0.01}\n",
      "Best score: 0.6884\n",
      "\n",
      "LRR with OrdinalEncoder\n",
      "Best parameter (CV score=0.635): {'classifier__C': 0.001}\n",
      "Best score: 0.6333\n",
      "\n",
      "LRR with PolynomialEncoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/patsy/contrasts.py:266: RuntimeWarning: overflow encountered in power\n",
      "  raw_poly = scores.reshape((-1, 1)) ** np.arange(n).reshape((1, -1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something went wrong w/ GridSearch or pipeline fitting.\n",
      "LRR with SumEncoder\n",
      "Best parameter (CV score=0.717): {'classifier__C': 1.0}\n",
      "Best score: 0.7234\n",
      "\n",
      "LRR with TargetEncoder\n",
      "Best parameter (CV score=0.698): {'classifier__C': 1.0}\n",
      "Best score: 0.7062\n",
      "\n",
      "LRR with WOEEncoder\n",
      "Best parameter (CV score=0.702): {'classifier__C': 0.001}\n",
      "Best score: 0.7095\n",
      "\n",
      "KNN with BackwardDifferenceEncoder\n",
      "Score: 0.6593\n",
      "\n",
      "KNN with BaseNEncoder\n",
      "Score: 0.6908\n",
      "\n",
      "KNN with BinaryEncoder\n",
      "Score: 0.6908\n",
      "\n",
      "KNN with CatBoostEncoder\n",
      "Score: 0.7091\n",
      "\n",
      "KNN with HashingEncoder\n",
      "Score: 0.6434\n",
      "\n",
      "KNN with HelmertEncoder\n",
      "Score: 0.6691\n",
      "\n",
      "KNN with JamesSteinEncoder\n",
      "Score: 0.6922\n",
      "\n",
      "KNN with OneHotEncoder\n",
      "Score: 0.7093\n",
      "\n",
      "KNN with LeaveOneOutEncoder\n",
      "Score: 0.7061\n",
      "\n",
      "KNN with MEstimateEncoder\n",
      "Score: 0.6879\n",
      "\n",
      "KNN with OrdinalEncoder\n",
      "Score: 0.6533\n",
      "\n",
      "KNN with PolynomialEncoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/patsy/contrasts.py:266: RuntimeWarning: overflow encountered in power\n",
      "  raw_poly = scores.reshape((-1, 1)) ** np.arange(n).reshape((1, -1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something went wrong with pipeline fitting\n",
      "KNN with SumEncoder\n",
      "Score: 0.7096\n",
      "\n",
      "KNN with TargetEncoder\n",
      "Score: 0.7100\n",
      "\n",
      "KNN with WOEEncoder\n",
      "Score: 0.7138\n",
      "\n",
      "BNB with BackwardDifferenceEncoder\n",
      "Best parameter (CV score=0.508): {'classifier__alpha': 0.0001}\n",
      "Best score: 0.2665\n",
      "\n",
      "BNB with BaseNEncoder\n",
      "Best parameter (CV score=0.657): {'classifier__alpha': 10.0}\n",
      "Best score: 0.6556\n",
      "\n",
      "BNB with BinaryEncoder\n",
      "Best parameter (CV score=0.657): {'classifier__alpha': 10.0}\n",
      "Best score: 0.6556\n",
      "\n",
      "BNB with CatBoostEncoder\n",
      "Best parameter (CV score=0.621): {'classifier__alpha': 0.0001}\n",
      "Best score: 0.6344\n",
      "\n",
      "BNB with HashingEncoder\n",
      "Best parameter (CV score=nan): {'classifier__alpha': 0.0001}\n",
      "Best score: 0.5818\n",
      "\n",
      "BNB with HelmertEncoder\n",
      "Best parameter (CV score=0.699): {'classifier__alpha': 1.0}\n",
      "Best score: 0.6991\n",
      "\n",
      "BNB with JamesSteinEncoder\n",
      "Best parameter (CV score=0.619): {'classifier__alpha': 0.0001}\n",
      "Best score: 0.6237\n",
      "\n",
      "BNB with OneHotEncoder\n",
      "Best parameter (CV score=0.698): {'classifier__alpha': 1.0}\n",
      "Best score: 0.7005\n",
      "\n",
      "BNB with LeaveOneOutEncoder\n",
      "Best parameter (CV score=0.621): {'classifier__alpha': 10.0}\n",
      "Best score: 0.6344\n",
      "\n",
      "BNB with MEstimateEncoder\n",
      "Best parameter (CV score=0.621): {'classifier__alpha': 0.0001}\n",
      "Best score: 0.6344\n",
      "\n",
      "BNB with OrdinalEncoder\n",
      "Best parameter (CV score=0.621): {'classifier__alpha': 0.0001}\n",
      "Best score: 0.6344\n",
      "\n",
      "BNB with PolynomialEncoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/patsy/contrasts.py:266: RuntimeWarning: overflow encountered in power\n",
      "  raw_poly = scores.reshape((-1, 1)) ** np.arange(n).reshape((1, -1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something went wrong w/ GridSearch or pipeline fitting.\n",
      "BNB with SumEncoder\n",
      "Best parameter (CV score=0.698): {'classifier__alpha': 1.0}\n",
      "Best score: 0.7000\n",
      "\n",
      "BNB with TargetEncoder\n",
      "Best parameter (CV score=0.621): {'classifier__alpha': 0.0001}\n",
      "Best score: 0.6344\n",
      "\n",
      "BNB with WOEEncoder\n",
      "Best parameter (CV score=0.671): {'classifier__alpha': 10.0}\n",
      "Best score: 0.6849\n",
      "\n",
      "GNB with BackwardDifferenceEncoder\n",
      "Best parameter (CV score=0.550): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.4207\n",
      "\n",
      "GNB with BaseNEncoder\n",
      "Best parameter (CV score=0.663): {'classifier__var_smoothing': 0.1}\n",
      "Best score: 0.6194\n",
      "\n",
      "GNB with BinaryEncoder\n",
      "Best parameter (CV score=0.663): {'classifier__var_smoothing': 0.1}\n",
      "Best score: 0.6194\n",
      "\n",
      "GNB with CatBoostEncoder\n",
      "Best parameter (CV score=0.691): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.6510\n",
      "\n",
      "GNB with HashingEncoder\n",
      "Best parameter (CV score=nan): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.5445\n",
      "\n",
      "GNB with HelmertEncoder\n",
      "Best parameter (CV score=0.528): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.1878\n",
      "\n",
      "GNB with JamesSteinEncoder\n",
      "Best parameter (CV score=0.676): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.5999\n",
      "\n",
      "GNB with OneHotEncoder\n",
      "Best parameter (CV score=0.689): {'classifier__var_smoothing': 0.1}\n",
      "Best score: 0.7030\n",
      "\n",
      "GNB with LeaveOneOutEncoder\n",
      "Best parameter (CV score=0.693): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.6505\n",
      "\n",
      "GNB with MEstimateEncoder\n",
      "Best parameter (CV score=0.682): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.6454\n",
      "\n",
      "GNB with OrdinalEncoder\n",
      "Best parameter (CV score=0.515): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.2315\n",
      "\n",
      "GNB with PolynomialEncoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/patsy/contrasts.py:266: RuntimeWarning: overflow encountered in power\n",
      "  raw_poly = scores.reshape((-1, 1)) ** np.arange(n).reshape((1, -1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something went wrong w/ GridSearch or pipeline fitting.\n",
      "GNB with SumEncoder\n",
      "Best parameter (CV score=0.616): {'classifier__var_smoothing': 10.0}\n",
      "Best score: 0.4620\n",
      "\n",
      "GNB with TargetEncoder\n",
      "Best parameter (CV score=0.692): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.6568\n",
      "\n",
      "GNB with WOEEncoder\n",
      "Best parameter (CV score=0.696): {'classifier__var_smoothing': 0.01}\n",
      "Best score: 0.6524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "classifier_list = []\n",
    "LRR = LogisticRegression(max_iter=10000, tol=0.1)\n",
    "KNN = KNeighborsClassifier(n_neighbors=30, p=1, leaf_size=25)\n",
    "BNB = BernoulliNB()\n",
    "GNB = GaussianNB()\n",
    "classifier_list.append(('LRR', LRR, {'classifier__C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000]}))\n",
    "classifier_list.append(('KNN', KNN, {}))\n",
    "classifier_list.append(('BNB', BNB, {'classifier__alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]}))\n",
    "classifier_list.append(('GNB', GNB, {'classifier__var_smoothing': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]}))\n",
    "\n",
    "#classifier_list.append(('SVM', svm.SVC()))\n",
    "#classifier_list.append(('CART', DecisionTreeClassifier()))\n",
    "#classifier_list.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "\n",
    "encoder_list = [ce.backward_difference.BackwardDifferenceEncoder, \n",
    "                ce.basen.BaseNEncoder,\n",
    "                ce.binary.BinaryEncoder,\n",
    "                ce.cat_boost.CatBoostEncoder,\n",
    "                ce.hashing.HashingEncoder,\n",
    "                ce.helmert.HelmertEncoder,\n",
    "                ce.james_stein.JamesSteinEncoder,\n",
    "                ce.one_hot.OneHotEncoder,\n",
    "                ce.leave_one_out.LeaveOneOutEncoder,\n",
    "                ce.m_estimate.MEstimateEncoder,\n",
    "                ce.ordinal.OrdinalEncoder,\n",
    "                ce.polynomial.PolynomialEncoder,\n",
    "                ce.sum_coding.SumEncoder,\n",
    "                ce.target_encoder.TargetEncoder,\n",
    "                ce.woe.WOEEncoder]\n",
    "\n",
    "for label, classifier, params in classifier_list:\n",
    "    results[label] = {}\n",
    "    for encoder in encoder_list:\n",
    "        results[label][encoder.__name__] = {}\n",
    "        print('{} with {}'.format(label, encoder.__name__))\n",
    "        \n",
    "        #numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),('scaler', MinMaxScaler())])\n",
    "        numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),('scaler', StandardScaler())])\n",
    "\n",
    "        categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                                                  ('woe', encoder())])\n",
    "\n",
    "        preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),\n",
    "                                                       ('cat', categorical_transformer, categorical_features)])\n",
    "        pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', classifier)])\n",
    "        if params != {}:\n",
    "            try:\n",
    "                search = GridSearchCV(pipe, params, n_jobs=-1)\n",
    "                search.fit(X_train, y_train)\n",
    "                print('Best parameter (CV score={:.3f}): {}'.format(search.best_score_, search.best_params_))\n",
    "                model = search.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                score = f1_score(y_test, y_pred)\n",
    "                print('Best score: {:.4f}\\n'.format(score))\n",
    "                results[label][encoder.__name__]['score'] = score\n",
    "                results[label][encoder.__name__]['best_params'] = search.best_params_\n",
    "            except:\n",
    "                print('Something went wrong w/ GridSearch or pipeline fitting.')\n",
    "        else:\n",
    "            try:\n",
    "                model = pipe.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                score = f1_score(y_test, y_pred)\n",
    "                print('Score: {:.4f}\\n'.format(score))\n",
    "                results[label][encoder.__name__]['score'] = score\n",
    "            except:\n",
    "                print('Something went wrong with pipeline fitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with manual encoding from previous notebook + `total_funding_usd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  1.08 Mb (86.2% reduction)\n",
      "\n",
      "MISSING VALUES BY PERCENTAGE\n",
      "rank                0.00%\n",
      "employee_count      0.00%\n",
      "total_funding_usd   0.00%\n",
      "\n",
      "Training data shape: (12531, 64)\n",
      "Train label shape: (12531,)\n",
      "Test data shape: (3133, 64)\n",
      "Test label shape: (3133,)\n",
      "\n",
      "KNN Accuracy score: 0.6336\n",
      "LRR Accuracy score: 0.5918\n"
     ]
    }
   ],
   "source": [
    "# Comparison\n",
    "df_b4 = df.drop(['category_groups','country','employee_size'], axis=1)\n",
    "df_b4 = df_b4.drop(df_b4.columns.to_list()[-46:], axis=1)\n",
    "\n",
    "# Sample\n",
    "df_p1 = df_b4[df_b4['p1_tag']==1]\n",
    "df_notp1 = df_b4[df_b4['p1_tag']==0].sample(n=df_p1.shape[0], replace=False)\n",
    "df_b4 = pd.concat([df_p1, df_notp1]).reset_index(drop=True)\n",
    "df_b4 = reduce_mem_usage(df_b4)\n",
    "\n",
    "# Impute missing data in employee_count and rank columns\n",
    "imputer = SimpleImputer(missing_values=-1, strategy='median')\n",
    "df_b4['employee_count'] = imputer.fit_transform(df_b4['employee_count'].values.reshape(-1, 1))\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_b4['rank'] = imputer.fit_transform(df_b4['rank'].values.reshape(-1, 1))\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_b4['total_funding_usd'] = imputer.fit_transform(df_b4['total_funding_usd'].values.reshape(-1, 1))\n",
    "df_num_missing = df_b4[['rank', 'employee_count', 'total_funding_usd']].isna().sum()/len(df_b4)\n",
    "output_string = df_num_missing.to_string(float_format=lambda x: '{:.2f}%'.format(x*100))\n",
    "print('\\nMISSING VALUES BY PERCENTAGE')\n",
    "print(output_string)\n",
    "\n",
    "# Scale numeric values\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "X = df_b4.drop('p1_tag', axis=1)\n",
    "y = df_b4['p1_tag']\n",
    "y = preprocessing.LabelEncoder().fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print('\\nTraining data shape:', X_train.shape)\n",
    "print('Train label shape:', y_train.shape)\n",
    "print('Test data shape:',  X_test.shape)\n",
    "print('Test label shape:', y_test.shape)\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=30, p=1, leaf_size=25)\n",
    "KNN.fit(X_train, y_train)\n",
    "y_pred = KNN.predict(X_test)\n",
    "print('\\nKNN Accuracy score: {:.4f}'.format(KNN.score(X_test, y_test)))\n",
    "\n",
    "LR = LogisticRegression(C=10)\n",
    "LR.fit(X_train, y_train)\n",
    "print('LRR Accuracy score: {:.4f}'.format(LR.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('results_baseline.json', 'w') as fp:\n",
    "    json.dump(results, fp, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BNB': {'BackwardDifferenceEncoder': {'best_params': {'classifier__alpha': 0.0001}, 'score': 0.26653974297953353}, 'BaseNEncoder': {'best_params': {'classifier__alpha': 10.0}, 'score': 0.6556420233463035}, 'BinaryEncoder': {'best_params': {'classifier__alpha': 10.0}, 'score': 0.6556420233463035}, 'CatBoostEncoder': {'best_params': {'classifier__alpha': 0.0001}, 'score': 0.6343825665859564}, 'HashingEncoder': {'best_params': {'classifier__alpha': 0.0001}, 'score': 0.5817531305903398}, 'HelmertEncoder': {'best_params': {'classifier__alpha': 1.0}, 'score': 0.6990725935401342}, 'JamesSteinEncoder': {'best_params': {'classifier__alpha': 0.0001}, 'score': 0.6237225147104367}, 'LeaveOneOutEncoder': {'best_params': {'classifier__alpha': 10.0}, 'score': 0.6343825665859564}, 'MEstimateEncoder': {'best_params': {'classifier__alpha': 0.0001}, 'score': 0.6343825665859564}, 'OneHotEncoder': {'best_params': {'classifier__alpha': 1.0}, 'score': 0.7004784688995215}, 'OrdinalEncoder': {'best_params': {'classifier__alpha': 0.0001}, 'score': 0.6343825665859564}, 'PolynomialEncoder': {}, 'SumEncoder': {'best_params': {'classifier__alpha': 1.0}, 'score': 0.7000318775900541}, 'TargetEncoder': {'best_params': {'classifier__alpha': 0.0001}, 'score': 0.6343825665859564}, 'WOEEncoder': {'best_params': {'classifier__alpha': 10.0}, 'score': 0.6848804719031357}}, 'GNB': {'BackwardDifferenceEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.42070665571076415}, 'BaseNEncoder': {'best_params': {'classifier__var_smoothing': 0.1}, 'score': 0.6193595342066956}, 'BinaryEncoder': {'best_params': {'classifier__var_smoothing': 0.1}, 'score': 0.6193595342066956}, 'CatBoostEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.6509746230231703}, 'HashingEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.5444575009792401}, 'HelmertEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.18783930510314875}, 'JamesSteinEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.5999232834675873}, 'LeaveOneOutEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.6504604051565377}, 'MEstimateEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.6453697056712132}, 'OneHotEncoder': {'best_params': {'classifier__var_smoothing': 0.1}, 'score': 0.7029855340104647}, 'OrdinalEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.23153692614770457}, 'PolynomialEncoder': {}, 'SumEncoder': {'best_params': {'classifier__var_smoothing': 10.0}, 'score': 0.4619516562220232}, 'TargetEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.6568132660418169}, 'WOEEncoder': {'best_params': {'classifier__var_smoothing': 0.01}, 'score': 0.6523636363636365}}, 'KNN': {'BackwardDifferenceEncoder': {'score': 0.6593406593406593}, 'BaseNEncoder': {'score': 0.6907832651777288}, 'BinaryEncoder': {'score': 0.6907832651777288}, 'CatBoostEncoder': {'score': 0.7090732967387795}, 'HashingEncoder': {'score': 0.643370346178968}, 'HelmertEncoder': {'score': 0.669100564221706}, 'JamesSteinEncoder': {'score': 0.692156862745098}, 'LeaveOneOutEncoder': {'score': 0.706111833550065}, 'MEstimateEncoder': {'score': 0.6878688524590163}, 'OneHotEncoder': {'score': 0.7093212081844755}, 'OrdinalEncoder': {'score': 0.6533416614615866}, 'PolynomialEncoder': {}, 'SumEncoder': {'score': 0.7095934959349592}, 'TargetEncoder': {'score': 0.7099678456591639}, 'WOEEncoder': {'score': 0.7137831787655902}}, 'LRR': {'BackwardDifferenceEncoder': {'best_params': {'classifier__C': 1.0}, 'score': 0.723404255319149}, 'BaseNEncoder': {'best_params': {'classifier__C': 0.1}, 'score': 0.680190174326466}, 'BinaryEncoder': {'best_params': {'classifier__C': 0.1}, 'score': 0.680190174326466}, 'CatBoostEncoder': {'best_params': {'classifier__C': 100.0}, 'score': 0.7078085642317381}, 'HashingEncoder': {'best_params': {'classifier__C': 0.0001}, 'score': 0.6505426356589147}, 'HelmertEncoder': {'best_params': {'classifier__C': 100.0}, 'score': 0.710344827586207}, 'JamesSteinEncoder': {'best_params': {'classifier__C': 1.0}, 'score': 0.6990784874483635}, 'LeaveOneOutEncoder': {'best_params': {'classifier__C': 1000}, 'score': 0.7081761006289309}, 'MEstimateEncoder': {'best_params': {'classifier__C': 0.01}, 'score': 0.6883691529709228}, 'OneHotEncoder': {'best_params': {'classifier__C': 1.0}, 'score': 0.7232170907948478}, 'OrdinalEncoder': {'best_params': {'classifier__C': 0.001}, 'score': 0.6332794830371568}, 'PolynomialEncoder': {}, 'SumEncoder': {'best_params': {'classifier__C': 1.0}, 'score': 0.7233908948194662}, 'TargetEncoder': {'best_params': {'classifier__C': 1.0}, 'score': 0.7061790668348046}, 'WOEEncoder': {'best_params': {'classifier__C': 0.001}, 'score': 0.7094743468681145}}}\n"
     ]
    }
   ],
   "source": [
    "with open('results_baseline.json', 'r') as fp:\n",
    "    results = json.load(fp)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
