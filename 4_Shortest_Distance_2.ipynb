{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing basic data analysis packages'''\n",
    "'''test'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "#datetime.today().strftime('%Y%m%d')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "'''Graph'''\n",
    "#import networkx as nx\n",
    "#from pyvis.network import Network\n",
    "from turicreate import SFrame, SGraph, pagerank, load_sgraph, degree_counting, aggregate, visualization, shortest_path, connected_components, kcore\n",
    "\n",
    "'''Plotting packages'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#%matplotlib inline\n",
    "sns.set(style='white', font_scale=1.3)\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):   \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def network_by_date(date, df_input, jobs_input, invest_input, invest_prtnr_input, model_uuids=[], skip_not_p1=True):\n",
    "    '''\n",
    "    This function filters down Crunchbase dataframes by date \n",
    "    to ensure that the companies/people/investments being used in modeling exist at a given time.\n",
    "\n",
    "    INPUT:\n",
    "        - `date`: string w/ format 'YEAR-MO-DY' (e.g. '2020-09-08')\n",
    "        - `df`: pandas dataframe of Crunchbase organizationss with necessary column fields:\n",
    "            * `p1_date`, `founded_on`, `closed_on`\n",
    "        - `jobs`: pandas dataframe of Crunchbase jobss with necessary column fields:\n",
    "            * `p1_date`, `started_on`, `ended_on`\n",
    "        - `invest`: pandas dataframe of Crunchbase investmentss with necessary column fields:\n",
    "            * `p1_date`, `announced_on`\n",
    "        - `invest_prtnr`: pandas dataframe of Crunchbase investments with necessary column fields:\n",
    "            * `p1_date`, `announced_on`\n",
    "        - `model_uuids`: list that contains the uuids of organizations that are used to construct the model graph\n",
    "        - `skip_no_p1`: Boolean that defaults to excluding the opposite of the Pledge 1% neighborhood. Likely will delete option altogether later.\n",
    "    \n",
    "    OUTPUT:\n",
    "        - List of dataframe lists, 2 lists of length 12: \n",
    "            * [Crunchbase neighborhood dataframes], [Pledge 1% neighborhood dataframes]\n",
    "                                        OR\n",
    "              [Crunchbase neighborhood dataframes], [Model neighborhood dataframes]\n",
    "        - Each dataframe list contains 12 dataframes that will be saved & loaded as SFrames in the next processing step.\n",
    "            0. Companies\n",
    "            1. Investors\n",
    "            2. Investments\n",
    "            3. Partner investments\n",
    "            4. Current Jobs\n",
    "            5. Former jobs\n",
    "            6. Former affiliated's new jobs\n",
    "            7. Partner investor's affiliation (if not in jobs dataframes)\n",
    "            8. Partner investor's coworkers at the investing firm\n",
    "            9. Partner investor's coworkers' partner investments\n",
    "            10. Current affiliated's old jobs\n",
    "            11. Organization nodes from edges in 2,3,6,7,9,10 if not already in 0 or 1\n",
    "    '''\n",
    "    # Soft copy of dataframes\n",
    "    df = df_input.copy()\n",
    "    jobs = jobs_input.copy()\n",
    "    invest = invest_input.copy()\n",
    "    invest_prtnr = invest_prtnr_input.copy()\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # DATE PROCESSING\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    df['p1_date'] = pd.to_datetime(df['p1_date'], errors='coerce')\n",
    "    df['founded_on'] = pd.to_datetime(df['founded_on'], errors='coerce')\n",
    "    df['closed_on'] = pd.to_datetime(df['closed_on'], errors='coerce')\n",
    "    jobs['p1_date'] = pd.to_datetime(jobs['p1_date'], errors='coerce')\n",
    "    jobs['started_on'] = pd.to_datetime(jobs['started_on'], errors='coerce')\n",
    "    jobs['ended_on'] = pd.to_datetime(jobs['ended_on'], errors='coerce')\n",
    "    invest['p1_date'] = pd.to_datetime(invest['p1_date'], errors='coerce')\n",
    "    invest['announced_on'] = pd.to_datetime(invest['announced_on'], errors='coerce')\n",
    "    invest_prtnr['p1_date'] = pd.to_datetime(invest_prtnr['p1_date'], errors='coerce')\n",
    "    invest_prtnr['announced_on'] = pd.to_datetime(invest_prtnr['announced_on'], errors='coerce')\n",
    "    \n",
    "    # Convert input date to datetime object\n",
    "    date = pd.Timestamp(date)\n",
    "    print('\\nAS OF {}:\\n'.format(date.strftime('%B %d, %Y').upper()))\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # Create new row for tagging model companies\n",
    "    df['add_to_model'] = 0\n",
    "    df['add_to_model'][df['uuid'].isin(model_uuids)] = 1\n",
    "    jobs['add_to_model'] = 0\n",
    "    jobs['add_to_model'][jobs['org_uuid'].isin(model_uuids)] = 1\n",
    "    invest['add_to_model'] = 0\n",
    "    invest['add_to_model'][invest['org_uuid'].isin(model_uuids)] = 1\n",
    "    invest_prtnr['add_to_model'] = 0\n",
    "    invest_prtnr['add_to_model'][invest_prtnr['org_uuid'].isin(model_uuids)] = 1\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # COMPANY FILTER\n",
    "    # Crunchbase company must be founded after DATE and closed before DATE (or DATE == NaT)\n",
    "    CB_companies = df[(df['founded_on']<=date) & \n",
    "                      ((df['closed_on']>date) | (pd.isnull(df['closed_on']))) & \n",
    "                      (df['primary_role']=='company')].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # INVESTOR FILTER:\n",
    "    # Crunchbase investor must be founded AFTER date and closed BEFORE date (or date == NaT)\n",
    "    CB_investors = df[(df['founded_on']<=date) & \n",
    "                      ((df['closed_on']>date) | (pd.isnull(df['closed_on']))) & \n",
    "                      (df['primary_role']=='investor')].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # INVESTMENT FILTER\n",
    "    # Crunchbase investment must have taken place BEFORE date\n",
    "    CB_investments = invest[(invest['announced_on']<=date) & \n",
    "                            (invest['investor_type']=='organization')].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # PARTNER INVESTMENT FILTER\n",
    "    # Crunchbase partner investment must have taken place BEFORE date\n",
    "    CB_investment_partners = invest_prtnr[invest_prtnr['announced_on']<=date].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # CURRENT JOB FILTER\n",
    "    # Crunchbase job must have started BEFORE date and ended AFTER date (or date == NaT)\n",
    "    CB_jobs = jobs[(jobs['job_type'].isin(['executive','board_member','advisor','board_observer'])) & \n",
    "                      (jobs['started_on']<=date) & \n",
    "                      ((jobs['ended_on']>date) | (pd.isnull(jobs['ended_on'])))].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # FORMER JOB FILTER\n",
    "    # Crunchbase job must have ended BEFORE date or started AFTER date\n",
    "    CB_jobs_former = jobs[(jobs['job_type'].isin(['executive','board_member','advisor','board_observer'])) & \n",
    "                          ((jobs['ended_on']<=date) | (jobs['started_on']>date))].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # COMBINE THESE 6 (or 7) INTO LIST OF FRAMES\n",
    "    lst_of_frames = []\n",
    "    \n",
    "    # Crunchbase frames\n",
    "    CB_frames = [CB_companies,CB_investors,CB_investments,CB_investment_partners,CB_jobs,CB_jobs_former]\n",
    "    # Add to list of frames\n",
    "    lst_of_frames.append(CB_frames)\n",
    "    \n",
    "    # If model_uuids are not supplied, calculate Pledge 1% neighborhood\n",
    "    if model_uuids == []:\n",
    "        P1_frames = []\n",
    "        for frame in CB_frames:\n",
    "            # Pledge 1% frames must have Crunchbase assumptions in addition to an earlier pledge date\n",
    "            new_frame = frame[frame['p1_date']<=date].reset_index(drop=True).drop('add_to_model',axis=1)\n",
    "            P1_frames.append(new_frame)\n",
    "        # Add to list of frames\n",
    "        lst_of_frames.append(P1_frames)\n",
    "    \n",
    "    # If model_uuids are supplied, calculate model neighborhood\n",
    "    if model_uuids != []:\n",
    "        model_frames = []\n",
    "        for frame in CB_frames:\n",
    "            # Include model dataframe if condition satisfied: either are a Pledge 1% company or tagged by model_uuids\n",
    "            new_frame=frame[(frame['p1_date']<=date) | (frame['add_to_model']==1)].reset_index(drop=True).drop('add_to_model',axis=1)\n",
    "            model_frames.append(new_frame)\n",
    "        # Add to list of frames\n",
    "        lst_of_frames.append(model_frames)\n",
    "    \n",
    "    # If this boolean value is False, calculate ~Pledge 1% neighborhood\n",
    "    if skip_not_p1 is False:\n",
    "        not_P1_frames = []\n",
    "        for frame in CB_frames:\n",
    "            # Non-Pledge 1% frames must have Crunchbase assumptions in addition to NaT pledge date or later pledge date\n",
    "            new_frame = frame[(pd.isnull(frame['p1_date']) | (frame['p1_date']>date))].reset_index(drop=True).drop('add_to_model',axis=1)\n",
    "            not_P1_frames.append(new_frame)\n",
    "        # Add to list of frames\n",
    "        lst_of_frames.append(not_P1_frames)\n",
    "        \n",
    "    # Remove extra column 'add_to_model'\n",
    "    for idx,frame in enumerate(CB_frames):\n",
    "        CB_frames[idx] = frame.drop('add_to_model',axis=1)\n",
    "\n",
    "    #*******************************************************************************************************\n",
    "    # FORMER NEW JOB FILTER\n",
    "    print('CaLcUlAtInG... FORMER NEW JOB FILTER')\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        \n",
    "        # Where do the former affiliated work now?\n",
    "        \n",
    "        # Pull their uuids\n",
    "        former_people = frame[5].person_uuid.unique()\n",
    "        # Pull their current jobs from Crunchbase\n",
    "        jobs_former_new = CB_frames[4][CB_frames[4].person_uuid.isin(former_people)] \n",
    "\n",
    "        # Check they're not already in the current jobs dataframe\n",
    "        # Combine into one temp data frame\n",
    "        combined_jobs = pd.concat([frame[4], jobs_former_new]).reset_index(drop=True) \n",
    "        df_gpby = combined_jobs.groupby(list(combined_jobs.columns))\n",
    "        \n",
    "        # Only count non-duplicated columns\n",
    "        idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "        \n",
    "        # Reindex dataframe\n",
    "        jobs_former_new = combined_jobs.reindex(idx)\n",
    "        \n",
    "        # Add to list of frames\n",
    "        frame.append(jobs_former_new)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # PARTNER INVESTMENT JOB FILTER\n",
    "    print('CaLcUlAtInG... PARTNER INVESTMENT JOB FILTER')\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        \n",
    "        # Are the partner investment jobs already in one of the jobs dataframes? If not, we should add them.\n",
    "        \n",
    "        # Create temporary dataframe and column to make checking the intersection between dataframes easier \n",
    "        # frame[4]: current jobs | frame[5]: former jobs | frame[6]: former new jobs\n",
    "        jobs_combined = pd.concat([frame[4],frame[5],frame[6]])\n",
    "        jobs_combined['person,company'] = jobs_combined['person_uuid'] + ',' + jobs_combined['org_uuid']\n",
    "        \n",
    "        # frame[3]: partner investments\n",
    "        frame[3]['person,company'] = frame[3]['partner_uuid']+ ',' + frame[3]['investor_uuid']\n",
    "\n",
    "        # Number of unique partner investments\n",
    "        unique_PI = frame[3]['person,company'].unique()\n",
    "\n",
    "        # Overlap between PI and combined J frames, create temporary jobs view\n",
    "        # These PI are already found in J frames, so we do not need to include them\n",
    "        jobs_already_in_J = jobs_combined[jobs_combined['person,company'].isin(unique_PI)] \n",
    "\n",
    "        # This will return non intersecting value of PI with temp J\n",
    "        # These PI are not found in J, so we would like to include them\n",
    "        PI_not_in_J = np.setdiff1d(unique_PI,jobs_already_in_J['person,company'].unique())\n",
    "\n",
    "        # Need to create separate jobs dataframe for non intersecting PI/J person/company pairs\n",
    "        grouped = frame[3][frame[3]['person,company'].isin(PI_not_in_J)].groupby(['partner_uuid','partner_name','investor_uuid','investor_name']).count()\n",
    "        grouped_df = grouped.reset_index()[['partner_uuid','partner_name','investor_uuid','investor_name']]\n",
    "        grouped_df['job_type'] = 'executive'\n",
    "        \n",
    "        # Add to list of frames\n",
    "        frame.append(grouped_df)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # OTHER FIRM PARNTERS\n",
    "    print('CaLcUlAtInG... OTHER FIRM PARTNER JOBS & INVESTMENTS FILTER')\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        \n",
    "        # OTHER FIRM PARNTERS - JOBS\n",
    "        # Who are the other partners that work at the investment firms present in the neighborhood?\n",
    "        \n",
    "        # Get the unique investor uuids associated with the dataframes\n",
    "        # frame[2]: from investments dataframe\n",
    "        unique_investor_firm_A = list(frame[2]['investor_uuid'].unique())\n",
    "        \n",
    "        # frame[3]: from partner investments dataframe\n",
    "        unique_investor_firm_B = list(frame[3]['investor_uuid'].unique())\n",
    "        partners = list(frame[3]['partner_uuid'].unique())\n",
    "        \n",
    "        # Combine to get list of unique uuids of VC firms\n",
    "        unique_firms = list(set(unique_investor_firm_A+unique_investor_firm_B))\n",
    "        \n",
    "        # Grab current jobs from Crunchbase for these investing firms\n",
    "        # Exclude duplicate partner job (already represented by partners list calculated above)\n",
    "        partner_jobs = CB_frames[4][(CB_frames[4]['org_uuid'].isin(unique_firms)) &  \n",
    "                                    ~(CB_frames[4]['person_uuid'].isin(partners))].reset_index(drop=True)\n",
    "        \n",
    "        # Check they're not already in the current/former jobs dataframe\n",
    "        # Combine into one temp data frame\n",
    "        combined_jobs = pd.concat([frame[4], partner_jobs]).reset_index(drop=True) \n",
    "        df_gpby = combined_jobs.groupby(list(combined_jobs.columns))\n",
    "        \n",
    "        # Only count non-duplicated rows\n",
    "        idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "        \n",
    "        # Reindex dataframe\n",
    "        partner_jobs = combined_jobs.reindex(idx)\n",
    "        \n",
    "        # Add to list of frames\n",
    "        frame.append(partner_jobs)\n",
    "        \n",
    "        # OTHER FIRM PARNTERS - PARTNER INVESTMENTS\n",
    "        # For these new partners, what companies are they invested in?\n",
    "        # Get the unique parnter uuids associated with the dataframes\n",
    "        other_partners = partner_jobs['person_uuid'].unique()\n",
    "        other_partner_investments = CB_frames[3][CB_frames[3]['partner_uuid'].isin(other_partners)]\n",
    "        \n",
    "        # Check they're not already in the partner investments dataframe\n",
    "        # Combine into one temp data frame\n",
    "        combined_jobs = pd.concat([frame[3], other_partner_investments]).reset_index(drop=True) \n",
    "        df_gpby = combined_jobs.groupby(list(combined_jobs.columns))\n",
    "        \n",
    "        # Only count non-duplicated rows\n",
    "        idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "        \n",
    "        # Reindex dataframe\n",
    "        other_partner_investments = combined_jobs.reindex(idx)\n",
    "        \n",
    "        # Add to list of frames\n",
    "        frame.append(other_partner_investments)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # CURRENT OLD JOB FILTER\n",
    "    print('CaLcUlAtInG... CURRENT OLD JOB FILTER')\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        \n",
    "        # Where did the current affiliated work previously?\n",
    "        current_people = frame[4].person_uuid.unique() # Pull their IDs\n",
    "        jobs_current_old = CB_frames[5][CB_frames[5].person_uuid.isin(current_people)] # Pull their current jobs from Crunchbase\n",
    "\n",
    "        # Check they're not already in the current jobs dataframe\n",
    "        # Combine into one temp data frame\n",
    "        combined_jobs = pd.concat([frame[5], jobs_current_old]).reset_index(drop=True) \n",
    "        df_gpby = combined_jobs.groupby(list(combined_jobs.columns))\n",
    "        \n",
    "        # Only count non-duplicated columns\n",
    "        idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "        \n",
    "        # Reindex dataframe\n",
    "        jobs_current_old = combined_jobs.reindex(idx)\n",
    "        \n",
    "        # Add to list of frames\n",
    "        frame.append(jobs_current_old)\n",
    "        \n",
    "    #*******************************************************************************************************\n",
    "    # GET EXTRA ORG UUID ATTRIBUTES FROM INVESTMENTS & JOBS\n",
    "    print('CaLcUlAtInG... EXTRA ORGANIZATION NODES')\n",
    "    \n",
    "    CB_orgs = pd.concat([CB_companies, CB_investors])\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        \n",
    "        unique_orgs = []\n",
    "        # Investments\n",
    "        unique_orgs.extend(list(frame[2]['investor_uuid'].unique()))\n",
    "        # Partner investments\n",
    "        unique_orgs.extend(list(frame[3]['investor_uuid'].unique()))\n",
    "        # Former new jobs organizations\n",
    "        unique_orgs.extend(list(frame[6]['org_uuid'].unique()))\n",
    "        # Parter jobs organizations\n",
    "        unique_orgs.extend(list(frame[7]['investor_uuid'].unique()))\n",
    "        # Other parter investments organizations\n",
    "        unique_orgs.extend(list(frame[9]['org_uuid'].unique()))\n",
    "        # Current old jobs organizations\n",
    "        unique_orgs.extend(list(frame[10]['org_uuid'].unique()))\n",
    "\n",
    "        # Pull their organization information from Crunchbase\n",
    "        new_org_nodes = CB_orgs[CB_orgs['uuid'].isin(list(set(unique_orgs)))]\n",
    "        # Add to list of frames\n",
    "        frame.append(new_org_nodes)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    del df['add_to_model'], invest['add_to_model'], invest_prtnr['add_to_model'], jobs['add_to_model']\n",
    "    \n",
    "    # Output print statements\n",
    "    print('\\nCrunchbase Neighborhood')\n",
    "    print('NODES | OUTPUT FRAME 0/CB_companies {}'.format(CB_frames[0].shape))\n",
    "    print('NODES | OUTPUT FRAME 1/CB_investors {}'.format(CB_frames[1].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 2/CB_investments {}'.format(CB_frames[2].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 3/CB_investment_partners {}'.format(CB_frames[3].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 4/CB_jobs {}'.format(CB_frames[4].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 5/CB_jobs_former {}'.format(CB_frames[5].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 6/CB_jobs_former_new {}'.format(CB_frames[6].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 7/CB_jobs_partner {}'.format(CB_frames[7].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 8/CB_jobs_other_partners {}'.format(CB_frames[8].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 9/CB_invest_other_partners {}'.format(CB_frames[9].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 10/CB_jobs_current_old {}'.format(CB_frames[10].shape))\n",
    "    print('NODES | OUTPUT FRAME 11/CB_extra_org_nodes {}'.format(CB_frames[11].shape))\n",
    "    \n",
    "    if model_uuids != []:\n",
    "\n",
    "        print('\\nModel Neighborhood')\n",
    "        print('NODES | OUTPUT FRAME 0/model_companies {}'.format(model_frames[0].shape))\n",
    "        print('NODES | OUTPUT FRAME 1/model_investors {}'.format(model_frames[1].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 2/model_investments {}'.format(model_frames[2].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 3/model_investment_partners {}'.format(model_frames[3].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 4/model_jobs {}'.format(model_frames[4].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 5/model_jobs_former {}'.format(model_frames[5].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 6/model_jobs_former_new {}'.format(model_frames[6].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 7/model_jobs_partner {}'.format(model_frames[7].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 8/model_jobs_other_partners {}'.format(model_frames[8].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 9/model_invest_other_partners {}'.format(model_frames[9].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 10/model_jobs_current_old {}'.format(model_frames[10].shape))\n",
    "        print('NODES | OUTPUT FRAME 11/model_extra_org_nodes {}'.format(model_frames[11].shape))\n",
    "        \n",
    "        return lst_of_frames\n",
    "\n",
    "    print('\\nPledge 1% Neighborhood')\n",
    "    print('NODES | OUTPUT FRAME 0/P1_companies {}'.format(P1_frames[0].shape))\n",
    "    print('NODES | OUTPUT FRAME 1/P1_investors {}'.format(P1_frames[1].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 2/P1_investments {}'.format(P1_frames[2].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 3/P1_investment_partners {}'.format(P1_frames[3].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 4/P1_jobs {}'.format(P1_frames[4].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 5/P1_jobs_former {}'.format(P1_frames[5].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 6/P1_jobs_former_new {}'.format(P1_frames[6].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 7/P1_jobs_partner {}'.format(P1_frames[7].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 8/P1_jobs_other_partners {}'.format(P1_frames[8].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 9/P1_invest_other_partners {}'.format(P1_frames[9].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 10/P1_jobs_current_old {}'.format(P1_frames[10].shape))\n",
    "    print('NODES | OUTPUT FRAME 11/P1_extra_org_nodes {}'.format(P1_frames[11].shape))\n",
    "    \n",
    "    # Skip Not P1 Calculations\n",
    "    if skip_not_p1 is False:\n",
    "        \n",
    "        print('\\n~Pledge 1% Neighborhood')\n",
    "        print('NODES | OUTPUT FRAME 0/not_P1_companies {}'.format(not_P1_frames[0].shape))\n",
    "        print('NODES | OUTPUT FRAME 1/not_P1_investors {}'.format(not_P1_frames[1].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 2/not_P1_investments {}'.format(not_P1_frames[2].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 3/not_P1_investment_partners {}'.format(not_P1_frames[3].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 4/not_P1_jobs {}'.format(not_P1_frames[4].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 5/not_P1_jobs_former {}'.format(not_P1_frames[5].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 6/not_P1_jobs_former_new {}'.format(not_P1_frames[6].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 7/not_P1_jobs_partner {}'.format(not_P1_frames[7].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 8/not_P1_jobs_other_partners {}'.format(not_P1_frames[8].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 9/not_P1_invest_other_partners {}'.format(not_P1_frames[9].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 10/not_P1_jobs_current_old {}'.format(not_P1_frames[10].shape))\n",
    "        print('NODES | OUTPUT FRAME 11/not_P1_extra_org_nodes {}'.format(not_P1_frames[11].shape))\n",
    "    \n",
    "    return lst_of_frames\n",
    "\n",
    "def load_vertices(sframes, g):\n",
    "    \n",
    "    # For jobs dataframes\n",
    "    for idx in [4,5,6,8,10]:\n",
    "        # Keep relevant node attributes\n",
    "        frame_temp = sframes[idx][['person_uuid', 'person_name']].rename({'person_uuid':'__id', 'person_name':'name'})\n",
    "        frame_temp['__node_type'] = 'person'\n",
    "        # Add p1_tag to the vertex\n",
    "        frame_temp['p1_tag'] = 0\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    \n",
    "    # For jobs and partner investments dataframes\n",
    "    for idx in [2,3,4,5,6,8,9,10]:\n",
    "        # Keep relevant node attributes\n",
    "        frame_temp = sframes[idx][['org_uuid', 'org_name', 'p1_tag']].rename({'org_uuid':'__id', 'org_name':'name'})\n",
    "        frame_temp['__node_type'] = 'company'\n",
    "        # Add p1_tag to the vertex\n",
    "        frame_temp['p1_tag'] = frame_temp['p1_tag'].apply(lambda x: 0 if (x==\"\" or x==0) else 1)\n",
    "        frame_temp['p1_tag'] = frame_temp['p1_tag'].astype(int)\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    \n",
    "    # For investments dataframes\n",
    "    for idx in [2,3,7,9]:\n",
    "        # Keep relevant node attributes\n",
    "        frame_temp = sframes[idx][['investor_uuid', 'investor_name']].rename({'investor_uuid':'__id', 'investor_name':'name'})\n",
    "        frame_temp['__node_type'] = 'investor'\n",
    "        # Add p1_tag to the vertex\n",
    "        frame_temp['p1_tag'] = 0\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    \n",
    "    # For partner investments dataframes\n",
    "    for idx in [3,7,9]:\n",
    "        # Keep relevant node attributes\n",
    "        frame_temp = sframes[idx][['partner_uuid', 'partner_name']].rename({'partner_uuid':'__id', 'partner_name':'name'})\n",
    "        frame_temp['__node_type'] = 'person'\n",
    "        # Add p1_tag to the vertex\n",
    "        frame_temp['p1_tag'] = 0\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    \n",
    "    # Organizations\n",
    "    for idx in [0,1,11]:\n",
    "        # Keep relevant node attributes\n",
    "        frame_temp = sframes[idx][['uuid', 'name', 'primary_role', 'p1_tag']].rename({'uuid':'__id', 'primary_role':'__node_type'})\n",
    "        # Add p1_tag to the vertex\n",
    "        frame_temp['p1_tag'] = frame_temp['p1_tag'].apply(lambda x: 0 if (x==\"\" or x==0) else 1)\n",
    "        frame_temp['p1_tag'] = frame_temp['p1_tag'].astype(int)\n",
    "        # Load into graph\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    \n",
    "    # Return SGraph\n",
    "    return g\n",
    "\n",
    "def find_p1_affiliations(p1_sframes):\n",
    "    frames = p1_sframes.copy()\n",
    "    \n",
    "    # Combine company and investor Pledge 1% dataframes, keeping only the uuid column\n",
    "    p1_affiliations = frames[0][['uuid']].append(frames[1][['uuid']])\n",
    "    \n",
    "    # Add edge connecting to Pledge 1% uuid\n",
    "    p1_affiliations['p1_uuid'] = 'fd9e2d10-a882-c6f4-737e-fd388d4ffd7c'\n",
    "    \n",
    "    # Create id, source, destination fields in SFrame\n",
    "    p1_affiliations = p1_affiliations.rename({'uuid':'src','p1_uuid':'dst'})\n",
    "    p1_affiliations['p1_tag'] = 1\n",
    "    \n",
    "    # Return SFrame\n",
    "    return p1_affiliations\n",
    "\n",
    "def load_edges(sframes, g, p1_affiliations=[], include_edges=[2,3], reverse=False, add_weights=False):\n",
    "    w = {'status':{'primary':3,'secondary':2,'tertiary':1}, '__edge_type':{'job':1, 'investment':2}}\n",
    "    \n",
    "    # Since it is a directed graph, need to include option for reverse direction\n",
    "    # Forward\n",
    "    source = 'src'\n",
    "    destination = 'dst'\n",
    "    # Reverse\n",
    "    if reverse:\n",
    "        source = 'dst'\n",
    "        destination = 'src'\n",
    "\n",
    "    if type(p1_affiliations) == SFrame:\n",
    "        # P1 Companies: Company/Investor --> Pledge 1%\n",
    "        g = g.add_edges(edges=p1_affiliations, src_field=source, dst_field=destination)\n",
    "        if add_weights:\n",
    "            frame_temp['weight'] = 6\n",
    "    \n",
    "    # Investments: Investor --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[2][['investment_uuid','investor_uuid','org_uuid','investment_type','raised_amount_usd','investor_count','is_lead_investor','lead_investor_count']].rename({'investment_uuid':'__id','investor_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 'primary'\n",
    "    if add_weights:\n",
    "        frame_temp['weight'] = w['__edge_type']['investment'] * w['status']['primary']\n",
    "    g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "    \n",
    "    # Partner Investments, Investments: Person --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[3][['investment_uuid','partner_uuid','org_uuid','investment_type','raised_amount_usd','investor_count']].rename({'investment_uuid':'__id','partner_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 'primary'\n",
    "    if add_weights:\n",
    "        frame_temp['weight'] = w['__edge_type']['investment'] * w['status']['primary']\n",
    "    g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "    \n",
    "    # Partner Investments, Investments: Investor --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[3][['investor_uuid','org_uuid','investment_type','investor_count']].rename({'investor_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 'secondary'\n",
    "    if add_weights:\n",
    "        frame_temp['weight'] = w['__edge_type']['investment'] * w['status']['secondary']\n",
    "    # Secondary relationships, skip if not specified at input\n",
    "    if 2 in include_edges:\n",
    "        g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "    \n",
    "    # Partner Investments, Jobs: Person --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[7][['partner_uuid','investor_uuid']].rename({'partner_uuid':'src','investor_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'job'\n",
    "    frame_temp['status'] = 'secondary'\n",
    "    if add_weights:\n",
    "        frame_temp['weight'] = w['__edge_type']['job'] * w['status']['secondary']\n",
    "    # Secondary relationships, skip if not specified at input\n",
    "    if 2 in include_edges:\n",
    "        g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)    \n",
    "    \n",
    "    # Other Partner Investments, Investments: Person --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[9][['investment_uuid','partner_uuid','org_uuid','investment_type','raised_amount_usd','investor_count']].rename({'investment_uuid':'__id','partner_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 'tertiary'\n",
    "    if add_weights:\n",
    "        frame_temp['weight'] = w['status']['tertiary'] * w['__edge_type']['investment']\n",
    "    # Tertiary relationships, skip if not specified at input\n",
    "    if 3 in include_edges:\n",
    "        g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "    \n",
    "    # Jobs: Person --> Company\n",
    "    for idx in [4,5,6,8,10]:\n",
    "        # Create id, source, destination fields in SFrame\n",
    "        frame_temp = sframes[idx][['job_uuid','person_uuid','org_uuid','job_type','title']].rename({'job_uuid':'__id','person_uuid':'src','org_uuid':'dst'})\n",
    "        frame_temp['__edge_type'] = 'job'\n",
    "        # Current jobs\n",
    "        if idx == 4:\n",
    "            frame_temp['status'] = 'primary'\n",
    "            if add_weights:\n",
    "                frame_temp['weight'] = w['status']['primary'] * w['__edge_type']['job']\n",
    "            g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "            continue\n",
    "        \n",
    "        # Secondary relationships, skip if not specified at input\n",
    "        if 2 in include_edges:\n",
    "            # Former jobs | Former new jobs | Current old jobs \n",
    "            if idx in [5,6,10]:\n",
    "                frame_temp['status'] = 'secondary'\n",
    "                if add_weights:\n",
    "                    frame_temp['weight'] = w['status']['secondary'] * w['__edge_type']['job']\n",
    "                g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "                continue\n",
    "        \n",
    "        # Tertiary relationships, skip if not specified at input\n",
    "        if 3 in include_edges:\n",
    "            # Other partners at firm\n",
    "            if idx == 8:\n",
    "                frame_temp['status'] = 'tertiary'\n",
    "                if add_weights:\n",
    "                    frame_temp['weight'] = w['status']['tertiary'] * w['__edge_type']['job']\n",
    "                g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "                continue\n",
    "    \n",
    "    # Return SGraph\n",
    "    return g\n",
    "\n",
    "def update_cb_weights(src, edge, dst):\n",
    "    if src['__id'] != dst['__id']: # ignore self-links\n",
    "        edge['weight'] = 0\n",
    "        edge['weight_status'] = 0\n",
    "        edge['weight_type'] = 0\n",
    "        if edge['status'] == 'primary':\n",
    "            edge['weight_status'] = 3\n",
    "        if edge['status'] == 'secondary':\n",
    "            edge['weight_status'] = 2\n",
    "        if edge['status'] == 'tertiary':\n",
    "            edge['weight_status'] = 1\n",
    "        if edge['__edge_type'] == 'job':\n",
    "            edge['weight_type'] = 1\n",
    "        if edge['__edge_type'] == 'investment':\n",
    "            edge['weight_type'] = 2\n",
    "        edge['weight'] = edge['weight_status'] * edge['weight_type']\n",
    "    return (src, edge, dst)\n",
    "#cb = cb.triple_apply(update_cb_weights, ['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_of_frames = []\n",
    "for val in ['cb','p1']:\n",
    "    lst = []\n",
    "    for idx in range(12):\n",
    "        path = 'files/output/graph_temp/{}/{}_df.csv'.format(val, idx)\n",
    "        lst.append(SFrame(data=path))\n",
    "    lst_of_frames.append(lst)\n",
    "cb_sframes,p1_sframes = lst_of_frames\n",
    "\n",
    "# List of Pledge 1% uuids\n",
    "global p1_companies_uuid\n",
    "p1_companies_uuid = []\n",
    "p1_companies_uuid.extend(list(p1_sframes[0]['uuid'].unique()))\n",
    "p1_companies_uuid.extend(list(p1_sframes[1]['uuid'].unique()))\n",
    "p1_companies_uuid = list(set(p1_companies_uuid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(cb_sframes, weights=False, reverse_edges=False, remove_parallel_edges=False):\n",
    "    \n",
    "    print('\\nBuIlDiNg GrApH...')\n",
    "    # Load in crunchbase with relationships\n",
    "    \n",
    "    # If adding weights...\n",
    "    if weights:\n",
    "        print('- ADDING WEIGHTS IN THE FORWARD DIRECTION')\n",
    "        cb = load_edges(cb_sframes, load_vertices(cb_sframes, SGraph()), p1_affiliations=[], include_edges=[2,3], reverse=False, add_weights=True)\n",
    "    elif not weights:\n",
    "        cb = load_edges(cb_sframes, load_vertices(cb_sframes, SGraph()), p1_affiliations=[], include_edges=[2,3], reverse=False, add_weights=False)\n",
    "    \n",
    "    # If adding reversed edges...\n",
    "    if reverse_edges:\n",
    "        print('- ADDING EDGES IN THE REVERSE DIRECTION')\n",
    "        # If adding weights...\n",
    "        if weights:\n",
    "            print('  - ADDING WEIGHTS IN THE REVERSE DIRECTION')\n",
    "            cb = load_edges(cb_sframes, cb, p1_affiliations=[], include_edges=[2,3], reverse=True, add_weights=True)\n",
    "        elif not weights:\n",
    "            cb = load_edges(cb_sframes, cb, p1_affiliations=[], include_edges=[2,3], reverse=True, add_weights=False)\n",
    "\n",
    "#     # Before comparison\n",
    "#     before = cb.summary()\n",
    "#     before_pri = cb.get_edges(fields={'status':'primary'}).shape[0]\n",
    "#     before_sec = cb.get_edges(fields={'status':'secondary'}).shape[0]\n",
    "#     before_ter = cb.get_edges(fields={'status':'tertiary'}).shape[0]\n",
    "\n",
    "    # Get list of edge fields\n",
    "    graph_edge_fields = cb.get_edge_fields()\n",
    "    \n",
    "    # If removing parallel edges...\n",
    "#     if remove_parallel_edges:\n",
    "#         print('- REMOVING PARALLEL EDGES')\n",
    "#         # Create temporary edge attribute that you'll use in aggregate function\n",
    "#         cb.edges['relationship'] = cb.edges['status']\n",
    "#         if weights:\n",
    "#             cb = SGraph(cb.vertices, cb.edges.groupby(['__src_id','__dst_id','__edge_type','weight'], {'status': aggregate.SELECT_ONE('relationship')}))\n",
    "#         elif not weights:\n",
    "#             cb = SGraph(cb.vertices, cb.edges.groupby(['__src_id','__dst_id','__edge_type'], {'status': aggregate.SELECT_ONE('relationship')}))\n",
    "#     elif not remove_parallel_edges:\n",
    "#         # Create temporary edge attribute that you'll use in aggregate function\n",
    "#         cb.edges['combined'] = cb.edges['__id']+','+cb.edges['status']+','+cb.edges['__src_id']+','+cb.edges['__dst_id']\n",
    "#         cb = SGraph(cb.vertices, cb.edges.groupby(graph_edge_fields, {'combined': aggregate.SELECT_ONE('combined')}))\n",
    "#         del cb.edges['combined']\n",
    "\n",
    "#     # After comparison\n",
    "#     after = cb.summary()\n",
    "#     after_pri = cb.get_edges(fields={'status':'primary'}).shape[0]\n",
    "#     after_sec = cb.get_edges(fields={'status':'secondary'}).shape[0]\n",
    "#     after_ter = cb.get_edges(fields={'status':'tertiary'}).shape[0]\n",
    "\n",
    "# #     # Output\n",
    "# #     print('\\nRemove duplicates from Crunchbase graph')\n",
    "# #     print('\\nNode change: {:,} --> {:,}'.format(before['num_vertices'], after['num_vertices']))\n",
    "# #     print('Edge change: {:,} --> {:,}'.format(before['num_edges'], after['num_edges']))\n",
    "# #     print('\\nPRIMARY Edge change: {:,} --> {:,}'.format(before_pri,after_pri))\n",
    "# #     print('SECONDARY Edge change: {:,} --> {:,}'.format(before_sec,after_sec))\n",
    "# #     print('TERTIARY Edge change: {:,} --> {:,}'.format(before_ter,after_ter))\n",
    "    \n",
    "    # Save and load graphs\n",
    "    # UPDATE PATH \n",
    "    if not reverse_edges and  not remove_parallel_edges: #(~A,~B)\n",
    "        name = 'Cruncbase_1Way_MultiEdge'\n",
    "    elif reverse_edges and not remove_parallel_edges: #(A,~B)\n",
    "        name = 'Crunchbase_2Ways_MultiEdge'\n",
    "    elif not reverse_edges and remove_parallel_edges: #(~A,B)\n",
    "        name = 'Cruncbase_1Way_SingleEdge'\n",
    "    elif reverse_edges and remove_parallel_edges: #(A,B)\n",
    "        name = 'Crunchbase_2Ways_SingleEdge'\n",
    "    if weights:\n",
    "        name += '_Weighted'\n",
    "    print('\\nSAVING {}'.format(name))\n",
    "    print('*'*50)\n",
    "    path = 'CrunchbaseGraphs/{}'.format(name)\n",
    "    cb.save(path)\n",
    "    cb = load_sgraph(path)\n",
    "    return cb\n",
    "\n",
    "# Construct all 8\n",
    "for weights_bool in [True]:\n",
    "    for reverse_bool in [True]:\n",
    "        for parallel_bool in [False]:\n",
    "            cb = make_graph(cb_sframes, weights=weights_bool, reverse_edges=reverse_bool, remove_parallel_edges=parallel_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = load_sgraph('files/CrunchbaseGraphs/Crunchbase_2Ways_MultiEdge_Weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_tag = cb.vertices[cb.vertices['p1_tag']==1]\n",
    "p1_tag_df = pd.DataFrame(p1_tag['__id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turicreate import load_sgraph\n",
    "from turicreate import shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-349eefd6e67a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp1_tag_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1157\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2312\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshortest_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_vid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sanjayelangovan/opt/anaconda3/envs/py27/lib/python2.7/site-packages/turicreate/toolkits/graph_analytics/shortest_path.pyc\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(graph, source_vid, weight_field, max_distance, verbose)\u001b[0m\n\u001b[1;32m    276\u001b[0m     }\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mQuietProgress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msssp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mShortestPathModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sanjayelangovan/opt/anaconda3/envs/py27/lib/python2.7/site-packages/turicreate/extensions.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_make_injected_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_run_toolkit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sanjayelangovan/opt/anaconda3/envs/py27/lib/python2.7/site-packages/turicreate/extensions.pyc\u001b[0m in \u001b[0;36m_run_toolkit_function\u001b[0;34m(fnname, arguments, args, kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;31m# unwrap it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcython_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_unity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_toolkit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margument_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;31m# handle errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sanjayelangovan/opt/anaconda3/envs/py27/lib/python2.7/site-packages/turicreate/_cython/context.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_cython_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;31m# To hide cython trace, we re-raise from here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m# To show the full trace, we do nothing and let exception propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_check = 0\n",
    "\n",
    "for i in p1_tag_df[0]:\n",
    "    sp = shortest_path.create(cb, source_vid=i, verbose = False, weight_value = 'weight')\n",
    "    a = sp['distance']\n",
    "    \n",
    "    if initial_check == 0:\n",
    "        distances2 = a\n",
    "        initial_check = 1\n",
    "        \n",
    "    else:\n",
    "        distances2['distance'] = np.where(a['distance'] < distances2['distance'], a['distance'], distances2['distance'])  #create new column in df1 to check if prices match\n",
    "        \n",
    "    if (p1_tag_df[p1_tag_df[0]==i].index.values % 100 == 0):\n",
    "        print (str(int(df1[df1[0]==i].index.values)) + \" P1 companies have been checked.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1 = pd.read_csv('files/output/spath1.csv')\n",
    "sp2 = pd.read_csv('files/output/spath2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1['__id']=sp1['__id'].astype(str)\n",
    "sp2['__id']=sp2['__id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dfs = sp1.merge(sp2, on = '__id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dfs['minimum_distance'] = merged_dfs.min(axis=1) \n",
    "#merged_dfs.drop(columns = ['distance_x','distance_y'], inplace = True)\n",
    "merged_dfs.rename(columns={\"minimum_distance\": \"spath\", \"__id\": \"uuid\"},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED TO CSV files/output/graph_model/model/spath.csv\n"
     ]
    }
   ],
   "source": [
    "path = 'files/output/graph_model/model/spath.csv'\n",
    "print ('SAVED TO CSV', path)\n",
    "merged_dfs.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.000000e+30    811522\n",
       "3.000000e+00    178358\n",
       "2.000000e+00    131468\n",
       "4.000000e+00     83440\n",
       "5.000000e+00     36024\n",
       "1.000000e+00     28278\n",
       "6.000000e+00      9625\n",
       "0.000000e+00      6937\n",
       "7.000000e+00      3401\n",
       "8.000000e+00       870\n",
       "9.000000e+00       285\n",
       "1.000000e+01       109\n",
       "1.100000e+01        23\n",
       "1.200000e+01         6\n",
       "Name: spath, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dfs['spath'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>spath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26f6621b-7df7-4f47-b971-f101a0a31de7</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d28f82a4-cf95-8e53-dcd4-fa2ea5d919e2</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6345c8d9-71ef-0f1e-589e-0ff32449eeae</td>\n",
       "      <td>1.000000e+30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f8d3a62c-abee-c2f3-4f67-fc04746bf166</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>038a451b-7d17-2b4d-927e-7c59f1e450e8</td>\n",
       "      <td>1.000000e+30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290341</th>\n",
       "      <td>aeaaf56c-aaaa-488e-9035-9208c551a900</td>\n",
       "      <td>1.000000e+30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290342</th>\n",
       "      <td>5fd0c08c-4271-49d3-bb2a-a591a8431490</td>\n",
       "      <td>1.000000e+30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290343</th>\n",
       "      <td>790b66a1-3f41-4785-a8b2-0f8b4ad190cb</td>\n",
       "      <td>1.000000e+30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290344</th>\n",
       "      <td>bfdfeb77-c317-4c7b-8aef-cff28ee4fdef</td>\n",
       "      <td>1.000000e+30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290345</th>\n",
       "      <td>6029c50e-2c2d-8ac4-4eca-e313163ea9d1</td>\n",
       "      <td>1.000000e+30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1290346 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         uuid         spath\n",
       "0        26f6621b-7df7-4f47-b971-f101a0a31de7  2.000000e+00\n",
       "1        d28f82a4-cf95-8e53-dcd4-fa2ea5d919e2  2.000000e+00\n",
       "2        6345c8d9-71ef-0f1e-589e-0ff32449eeae  1.000000e+30\n",
       "3        f8d3a62c-abee-c2f3-4f67-fc04746bf166  1.000000e+00\n",
       "4        038a451b-7d17-2b4d-927e-7c59f1e450e8  1.000000e+30\n",
       "...                                       ...           ...\n",
       "1290341  aeaaf56c-aaaa-488e-9035-9208c551a900  1.000000e+30\n",
       "1290342  5fd0c08c-4271-49d3-bb2a-a591a8431490  1.000000e+30\n",
       "1290343  790b66a1-3f41-4785-a8b2-0f8b4ad190cb  1.000000e+30\n",
       "1290344  bfdfeb77-c317-4c7b-8aef-cff28ee4fdef  1.000000e+30\n",
       "1290345  6029c50e-2c2d-8ac4-4eca-e313163ea9d1  1.000000e+30\n",
       "\n",
       "[1290346 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
