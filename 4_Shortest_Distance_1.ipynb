{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_SS_MIDS_1B_Trees_Graphs_Notes.ipynb 4_SS_Graph_nx.ipynb\r\n",
      "1_AM_EDA.ipynb                        4_SS_Graph_turicreate.ipynb\r\n",
      "1_AM_Top_Investing_Companies.ipynb    4_SS_Graph_turicreate_UPDATED.ipynb\r\n",
      "1_AM_Top_Investing_People.ipynb       5_AM_Model_Pipeline.ipynb\r\n",
      "1_OS_EDA.ipynb                        Pandas_Profiling.zip\r\n",
      "1_SE_EDA.ipynb                        Primary.html\r\n",
      "1_SP_EDA.ipynb                        README.md\r\n",
      "1_SS_EDA.ipynb                        Secondary.html\r\n",
      "2_Baseline_Model.ipynb                Shortest_Distance_1.ipynb\r\n",
      "3_Baseline_Pipeline.ipynb             Shortest_Distance_2.ipynb\r\n",
      "3_Baseline_Pipeline_2.ipynb           Shortest_Distance_3.ipynb\r\n",
      "4_SE_Graph.ipynb                      Teritiary.html\r\n",
      "4_SE_Graph_COLAB.ipynb                \u001b[34mfiles\u001b[m\u001b[m\r\n",
      "4_SE_Graph_Features.ipynb             \u001b[34moct_28_submission\u001b[m\u001b[m\r\n",
      "4_SE_spath_top.ipynb                  \u001b[34mold_notebooks\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing basic data analysis packages'''\n",
    "'''test'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "#datetime.today().strftime('%Y%m%d')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "'''Graph'''\n",
    "#import networkx as nx\n",
    "#from pyvis.network import Network\n",
    "from turicreate import SFrame, SGraph, pagerank, load_sgraph, degree_counting, aggregate, visualization, shortest_path, connected_components, kcore\n",
    "\n",
    "'''Plotting packages'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#%matplotlib inline\n",
    "sns.set(style='white', font_scale=1.3)\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):   \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def network_by_date(date, df_input, jobs_input, invest_input, invest_prtnr_input, model_uuids=[], skip_not_p1=True):\n",
    "    '''\n",
    "    This function filters down Crunchbase dataframes by date \n",
    "    to ensure that the companies/people/investments being used in modeling exist at a given time.\n",
    "\n",
    "    INPUT:\n",
    "        - `date`: string w/ format 'YEAR-MO-DY' (e.g. '2020-09-08')\n",
    "        - `df`: pandas dataframe of Crunchbase organizationss with necessary column fields:\n",
    "            * `p1_date`, `founded_on`, `closed_on`\n",
    "        - `jobs`: pandas dataframe of Crunchbase jobss with necessary column fields:\n",
    "            * `p1_date`, `started_on`, `ended_on`\n",
    "        - `invest`: pandas dataframe of Crunchbase investmentss with necessary column fields:\n",
    "            * `p1_date`, `announced_on`\n",
    "        - `invest_prtnr`: pandas dataframe of Crunchbase investments with necessary column fields:\n",
    "            * `p1_date`, `announced_on`\n",
    "        - `model_uuids`: list that contains the uuids of organizations that are used to construct the model graph\n",
    "        - `skip_no_p1`: Boolean that defaults to excluding the opposite of the Pledge 1% neighborhood. Likely will delete option altogether later.\n",
    "    \n",
    "    OUTPUT:\n",
    "        - List of dataframe lists, 2 lists of length 12: \n",
    "            * [Crunchbase neighborhood dataframes], [Pledge 1% neighborhood dataframes]\n",
    "                                        OR\n",
    "              [Crunchbase neighborhood dataframes], [Model neighborhood dataframes]\n",
    "        - Each dataframe list contains 12 dataframes that will be saved & loaded as SFrames in the next processing step.\n",
    "            0. Companies\n",
    "            1. Investors\n",
    "            2. Investments\n",
    "            3. Partner investments\n",
    "            4. Current Jobs\n",
    "            5. Former jobs\n",
    "            6. Former affiliated's new jobs\n",
    "            7. Partner investor's affiliation (if not in jobs dataframes)\n",
    "            8. Partner investor's coworkers at the investing firm\n",
    "            9. Partner investor's coworkers' partner investments\n",
    "            10. Current affiliated's old jobs\n",
    "            11. Organization nodes from edges in 2,3,6,7,9,10 if not already in 0 or 1\n",
    "    '''\n",
    "    # Soft copy of dataframes\n",
    "    df = df_input.copy()\n",
    "    jobs = jobs_input.copy()\n",
    "    invest = invest_input.copy()\n",
    "    invest_prtnr = invest_prtnr_input.copy()\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # DATE PROCESSING\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    df['p1_date'] = pd.to_datetime(df['p1_date'], errors='coerce')\n",
    "    df['founded_on'] = pd.to_datetime(df['founded_on'], errors='coerce')\n",
    "    df['closed_on'] = pd.to_datetime(df['closed_on'], errors='coerce')\n",
    "    jobs['p1_date'] = pd.to_datetime(jobs['p1_date'], errors='coerce')\n",
    "    jobs['started_on'] = pd.to_datetime(jobs['started_on'], errors='coerce')\n",
    "    jobs['ended_on'] = pd.to_datetime(jobs['ended_on'], errors='coerce')\n",
    "    invest['p1_date'] = pd.to_datetime(invest['p1_date'], errors='coerce')\n",
    "    invest['announced_on'] = pd.to_datetime(invest['announced_on'], errors='coerce')\n",
    "    invest_prtnr['p1_date'] = pd.to_datetime(invest_prtnr['p1_date'], errors='coerce')\n",
    "    invest_prtnr['announced_on'] = pd.to_datetime(invest_prtnr['announced_on'], errors='coerce')\n",
    "    \n",
    "    # Convert input date to datetime object\n",
    "    date = pd.Timestamp(date)\n",
    "    print('\\nAS OF {}:\\n'.format(date.strftime('%B %d, %Y').upper()))\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # Create new row for tagging model companies\n",
    "    df['add_to_model'] = 0\n",
    "    df['add_to_model'][df['uuid'].isin(model_uuids)] = 1\n",
    "    jobs['add_to_model'] = 0\n",
    "    jobs['add_to_model'][jobs['org_uuid'].isin(model_uuids)] = 1\n",
    "    invest['add_to_model'] = 0\n",
    "    invest['add_to_model'][invest['org_uuid'].isin(model_uuids)] = 1\n",
    "    invest_prtnr['add_to_model'] = 0\n",
    "    invest_prtnr['add_to_model'][invest_prtnr['org_uuid'].isin(model_uuids)] = 1\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # COMPANY FILTER\n",
    "    # Crunchbase company must be founded after DATE and closed before DATE (or DATE == NaT)\n",
    "    CB_companies = df[(df['founded_on']<=date) & \n",
    "                      ((df['closed_on']>date) | (pd.isnull(df['closed_on']))) & \n",
    "                      (df['primary_role']=='company')].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # INVESTOR FILTER:\n",
    "    # Crunchbase investor must be founded AFTER date and closed BEFORE date (or date == NaT)\n",
    "    CB_investors = df[(df['founded_on']<=date) & \n",
    "                      ((df['closed_on']>date) | (pd.isnull(df['closed_on']))) & \n",
    "                      (df['primary_role']=='investor')].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # INVESTMENT FILTER\n",
    "    # Crunchbase investment must have taken place BEFORE date\n",
    "    CB_investments = invest[(invest['announced_on']<=date) & \n",
    "                            (invest['investor_type']=='organization')].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # PARTNER INVESTMENT FILTER\n",
    "    # Crunchbase partner investment must have taken place BEFORE date\n",
    "    CB_investment_partners = invest_prtnr[invest_prtnr['announced_on']<=date].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # CURRENT JOB FILTER\n",
    "    # Crunchbase job must have started BEFORE date and ended AFTER date (or date == NaT)\n",
    "    CB_jobs = jobs[(jobs['job_type'].isin(['executive','board_member','advisor','board_observer'])) & \n",
    "                      (jobs['started_on']<=date) & \n",
    "                      ((jobs['ended_on']>date) | (pd.isnull(jobs['ended_on'])))].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # FORMER JOB FILTER\n",
    "    # Crunchbase job must have ended BEFORE date or started AFTER date\n",
    "    CB_jobs_former = jobs[(jobs['job_type'].isin(['executive','board_member','advisor','board_observer'])) & \n",
    "                          ((jobs['ended_on']<=date) | (jobs['started_on']>date))].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # COMBINE THESE 6 (or 7) INTO LIST OF FRAMES\n",
    "    lst_of_frames = []\n",
    "    \n",
    "    # Crunchbase frames\n",
    "    CB_frames = [CB_companies,CB_investors,CB_investments,CB_investment_partners,CB_jobs,CB_jobs_former]\n",
    "    # Add to list of frames\n",
    "    lst_of_frames.append(CB_frames)\n",
    "    \n",
    "    # If model_uuids are not supplied, calculate Pledge 1% neighborhood\n",
    "    if model_uuids == []:\n",
    "        P1_frames = []\n",
    "        for frame in CB_frames:\n",
    "            # Pledge 1% frames must have Crunchbase assumptions in addition to an earlier pledge date\n",
    "            new_frame = frame[frame['p1_date']<=date].reset_index(drop=True).drop('add_to_model',axis=1)\n",
    "            P1_frames.append(new_frame)\n",
    "        # Add to list of frames\n",
    "        lst_of_frames.append(P1_frames)\n",
    "    \n",
    "    # If model_uuids are supplied, calculate model neighborhood\n",
    "    if model_uuids != []:\n",
    "        model_frames = []\n",
    "        for frame in CB_frames:\n",
    "            # Include model dataframe if condition satisfied: either are a Pledge 1% company or tagged by model_uuids\n",
    "            new_frame=frame[(frame['p1_date']<=date) | (frame['add_to_model']==1)].reset_index(drop=True).drop('add_to_model',axis=1)\n",
    "            model_frames.append(new_frame)\n",
    "        # Add to list of frames\n",
    "        lst_of_frames.append(model_frames)\n",
    "    \n",
    "    # If this boolean value is False, calculate ~Pledge 1% neighborhood\n",
    "    if skip_not_p1 is False:\n",
    "        not_P1_frames = []\n",
    "        for frame in CB_frames:\n",
    "            # Non-Pledge 1% frames must have Crunchbase assumptions in addition to NaT pledge date or later pledge date\n",
    "            new_frame = frame[(pd.isnull(frame['p1_date']) | (frame['p1_date']>date))].reset_index(drop=True).drop('add_to_model',axis=1)\n",
    "            not_P1_frames.append(new_frame)\n",
    "        # Add to list of frames\n",
    "        lst_of_frames.append(not_P1_frames)\n",
    "        \n",
    "    # Remove extra column 'add_to_model'\n",
    "    for idx,frame in enumerate(CB_frames):\n",
    "        CB_frames[idx] = frame.drop('add_to_model',axis=1)\n",
    "\n",
    "    #*******************************************************************************************************\n",
    "    # FORMER NEW JOB FILTER\n",
    "    print('CaLcUlAtInG... FORMER NEW JOB FILTER')\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        \n",
    "        # Where do the former affiliated work now?\n",
    "        \n",
    "        # Pull their uuids\n",
    "        former_people = frame[5].person_uuid.unique()\n",
    "        # Pull their current jobs from Crunchbase\n",
    "        jobs_former_new = CB_frames[4][CB_frames[4].person_uuid.isin(former_people)] \n",
    "\n",
    "        # Check they're not already in the current jobs dataframe\n",
    "        # Combine into one temp data frame\n",
    "        combined_jobs = pd.concat([frame[4], jobs_former_new]).reset_index(drop=True) \n",
    "        df_gpby = combined_jobs.groupby(list(combined_jobs.columns))\n",
    "        \n",
    "        # Only count non-duplicated columns\n",
    "        idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "        \n",
    "        # Reindex dataframe\n",
    "        jobs_former_new = combined_jobs.reindex(idx)\n",
    "        \n",
    "        # Add to list of frames\n",
    "        frame.append(jobs_former_new)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # PARTNER INVESTMENT JOB FILTER\n",
    "    print('CaLcUlAtInG... PARTNER INVESTMENT JOB FILTER')\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        \n",
    "        # Are the partner investment jobs already in one of the jobs dataframes? If not, we should add them.\n",
    "        \n",
    "        # Create temporary dataframe and column to make checking the intersection between dataframes easier \n",
    "        # frame[4]: current jobs | frame[5]: former jobs | frame[6]: former new jobs\n",
    "        jobs_combined = pd.concat([frame[4],frame[5],frame[6]])\n",
    "        jobs_combined['person,company'] = jobs_combined['person_uuid'] + ',' + jobs_combined['org_uuid']\n",
    "        \n",
    "        # frame[3]: partner investments\n",
    "        frame[3]['person,company'] = frame[3]['partner_uuid']+ ',' + frame[3]['investor_uuid']\n",
    "\n",
    "        # Number of unique partner investments\n",
    "        unique_PI = frame[3]['person,company'].unique()\n",
    "\n",
    "        # Overlap between PI and combined J frames, create temporary jobs view\n",
    "        # These PI are already found in J frames, so we do not need to include them\n",
    "        jobs_already_in_J = jobs_combined[jobs_combined['person,company'].isin(unique_PI)] \n",
    "\n",
    "        # This will return non intersecting value of PI with temp J\n",
    "        # These PI are not found in J, so we would like to include them\n",
    "        PI_not_in_J = np.setdiff1d(unique_PI,jobs_already_in_J['person,company'].unique())\n",
    "\n",
    "        # Need to create separate jobs dataframe for non intersecting PI/J person/company pairs\n",
    "        grouped = frame[3][frame[3]['person,company'].isin(PI_not_in_J)].groupby(['partner_uuid','partner_name','investor_uuid','investor_name']).count()\n",
    "        grouped_df = grouped.reset_index()[['partner_uuid','partner_name','investor_uuid','investor_name']]\n",
    "        grouped_df['job_type'] = 'executive'\n",
    "        \n",
    "        # Add to list of frames\n",
    "        frame.append(grouped_df)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # OTHER FIRM PARNTERS\n",
    "    print('CaLcUlAtInG... OTHER FIRM PARTNER JOBS & INVESTMENTS FILTER')\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        \n",
    "        # OTHER FIRM PARNTERS - JOBS\n",
    "        # Who are the other partners that work at the investment firms present in the neighborhood?\n",
    "        \n",
    "        # Get the unique investor uuids associated with the dataframes\n",
    "        # frame[2]: from investments dataframe\n",
    "        unique_investor_firm_A = list(frame[2]['investor_uuid'].unique())\n",
    "        \n",
    "        # frame[3]: from partner investments dataframe\n",
    "        unique_investor_firm_B = list(frame[3]['investor_uuid'].unique())\n",
    "        partners = list(frame[3]['partner_uuid'].unique())\n",
    "        \n",
    "        # Combine to get list of unique uuids of VC firms\n",
    "        unique_firms = list(set(unique_investor_firm_A+unique_investor_firm_B))\n",
    "        \n",
    "        # Grab current jobs from Crunchbase for these investing firms\n",
    "        # Exclude duplicate partner job (already represented by partners list calculated above)\n",
    "        partner_jobs = CB_frames[4][(CB_frames[4]['org_uuid'].isin(unique_firms)) &  \n",
    "                                    ~(CB_frames[4]['person_uuid'].isin(partners))].reset_index(drop=True)\n",
    "        \n",
    "        # Check they're not already in the current/former jobs dataframe\n",
    "        # Combine into one temp data frame\n",
    "        combined_jobs = pd.concat([frame[4], partner_jobs]).reset_index(drop=True) \n",
    "        df_gpby = combined_jobs.groupby(list(combined_jobs.columns))\n",
    "        \n",
    "        # Only count non-duplicated rows\n",
    "        idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "        \n",
    "        # Reindex dataframe\n",
    "        partner_jobs = combined_jobs.reindex(idx)\n",
    "        \n",
    "        # Add to list of frames\n",
    "        frame.append(partner_jobs)\n",
    "        \n",
    "        # OTHER FIRM PARNTERS - PARTNER INVESTMENTS\n",
    "        # For these new partners, what companies are they invested in?\n",
    "        # Get the unique parnter uuids associated with the dataframes\n",
    "        other_partners = partner_jobs['person_uuid'].unique()\n",
    "        other_partner_investments = CB_frames[3][CB_frames[3]['partner_uuid'].isin(other_partners)]\n",
    "        \n",
    "        # Check they're not already in the partner investments dataframe\n",
    "        # Combine into one temp data frame\n",
    "        combined_jobs = pd.concat([frame[3], other_partner_investments]).reset_index(drop=True) \n",
    "        df_gpby = combined_jobs.groupby(list(combined_jobs.columns))\n",
    "        \n",
    "        # Only count non-duplicated rows\n",
    "        idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "        \n",
    "        # Reindex dataframe\n",
    "        other_partner_investments = combined_jobs.reindex(idx)\n",
    "        \n",
    "        # Add to list of frames\n",
    "        frame.append(other_partner_investments)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # CURRENT OLD JOB FILTER\n",
    "    print('CaLcUlAtInG... CURRENT OLD JOB FILTER')\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        \n",
    "        # Where did the current affiliated work previously?\n",
    "        current_people = frame[4].person_uuid.unique() # Pull their IDs\n",
    "        jobs_current_old = CB_frames[5][CB_frames[5].person_uuid.isin(current_people)] # Pull their current jobs from Crunchbase\n",
    "\n",
    "        # Check they're not already in the current jobs dataframe\n",
    "        # Combine into one temp data frame\n",
    "        combined_jobs = pd.concat([frame[5], jobs_current_old]).reset_index(drop=True) \n",
    "        df_gpby = combined_jobs.groupby(list(combined_jobs.columns))\n",
    "        \n",
    "        # Only count non-duplicated columns\n",
    "        idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "        \n",
    "        # Reindex dataframe\n",
    "        jobs_current_old = combined_jobs.reindex(idx)\n",
    "        \n",
    "        # Add to list of frames\n",
    "        frame.append(jobs_current_old)\n",
    "        \n",
    "    #*******************************************************************************************************\n",
    "    # GET EXTRA ORG UUID ATTRIBUTES FROM INVESTMENTS & JOBS\n",
    "    print('CaLcUlAtInG... EXTRA ORGANIZATION NODES')\n",
    "    \n",
    "    CB_orgs = pd.concat([CB_companies, CB_investors])\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        \n",
    "        unique_orgs = []\n",
    "        # Investments\n",
    "        unique_orgs.extend(list(frame[2]['investor_uuid'].unique()))\n",
    "        # Partner investments\n",
    "        unique_orgs.extend(list(frame[3]['investor_uuid'].unique()))\n",
    "        # Former new jobs organizations\n",
    "        unique_orgs.extend(list(frame[6]['org_uuid'].unique()))\n",
    "        # Parter jobs organizations\n",
    "        unique_orgs.extend(list(frame[7]['investor_uuid'].unique()))\n",
    "        # Other parter investments organizations\n",
    "        unique_orgs.extend(list(frame[9]['org_uuid'].unique()))\n",
    "        # Current old jobs organizations\n",
    "        unique_orgs.extend(list(frame[10]['org_uuid'].unique()))\n",
    "\n",
    "        # Pull their organization information from Crunchbase\n",
    "        new_org_nodes = CB_orgs[CB_orgs['uuid'].isin(list(set(unique_orgs)))]\n",
    "        # Add to list of frames\n",
    "        frame.append(new_org_nodes)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    del df['add_to_model'], invest['add_to_model'], invest_prtnr['add_to_model'], jobs['add_to_model']\n",
    "    \n",
    "    # Output print statements\n",
    "    print('\\nCrunchbase Neighborhood')\n",
    "    print('NODES | OUTPUT FRAME 0/CB_companies {}'.format(CB_frames[0].shape))\n",
    "    print('NODES | OUTPUT FRAME 1/CB_investors {}'.format(CB_frames[1].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 2/CB_investments {}'.format(CB_frames[2].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 3/CB_investment_partners {}'.format(CB_frames[3].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 4/CB_jobs {}'.format(CB_frames[4].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 5/CB_jobs_former {}'.format(CB_frames[5].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 6/CB_jobs_former_new {}'.format(CB_frames[6].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 7/CB_jobs_partner {}'.format(CB_frames[7].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 8/CB_jobs_other_partners {}'.format(CB_frames[8].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 9/CB_invest_other_partners {}'.format(CB_frames[9].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 10/CB_jobs_current_old {}'.format(CB_frames[10].shape))\n",
    "    print('NODES | OUTPUT FRAME 11/CB_extra_org_nodes {}'.format(CB_frames[11].shape))\n",
    "    \n",
    "    if model_uuids != []:\n",
    "\n",
    "        print('\\nModel Neighborhood')\n",
    "        print('NODES | OUTPUT FRAME 0/model_companies {}'.format(model_frames[0].shape))\n",
    "        print('NODES | OUTPUT FRAME 1/model_investors {}'.format(model_frames[1].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 2/model_investments {}'.format(model_frames[2].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 3/model_investment_partners {}'.format(model_frames[3].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 4/model_jobs {}'.format(model_frames[4].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 5/model_jobs_former {}'.format(model_frames[5].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 6/model_jobs_former_new {}'.format(model_frames[6].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 7/model_jobs_partner {}'.format(model_frames[7].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 8/model_jobs_other_partners {}'.format(model_frames[8].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 9/model_invest_other_partners {}'.format(model_frames[9].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 10/model_jobs_current_old {}'.format(model_frames[10].shape))\n",
    "        print('NODES | OUTPUT FRAME 11/model_extra_org_nodes {}'.format(model_frames[11].shape))\n",
    "        \n",
    "        return lst_of_frames\n",
    "\n",
    "    print('\\nPledge 1% Neighborhood')\n",
    "    print('NODES | OUTPUT FRAME 0/P1_companies {}'.format(P1_frames[0].shape))\n",
    "    print('NODES | OUTPUT FRAME 1/P1_investors {}'.format(P1_frames[1].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 2/P1_investments {}'.format(P1_frames[2].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 3/P1_investment_partners {}'.format(P1_frames[3].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 4/P1_jobs {}'.format(P1_frames[4].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 5/P1_jobs_former {}'.format(P1_frames[5].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 6/P1_jobs_former_new {}'.format(P1_frames[6].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 7/P1_jobs_partner {}'.format(P1_frames[7].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 8/P1_jobs_other_partners {}'.format(P1_frames[8].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 9/P1_invest_other_partners {}'.format(P1_frames[9].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 10/P1_jobs_current_old {}'.format(P1_frames[10].shape))\n",
    "    print('NODES | OUTPUT FRAME 11/P1_extra_org_nodes {}'.format(P1_frames[11].shape))\n",
    "    \n",
    "    # Skip Not P1 Calculations\n",
    "    if skip_not_p1 is False:\n",
    "        \n",
    "        print('\\n~Pledge 1% Neighborhood')\n",
    "        print('NODES | OUTPUT FRAME 0/not_P1_companies {}'.format(not_P1_frames[0].shape))\n",
    "        print('NODES | OUTPUT FRAME 1/not_P1_investors {}'.format(not_P1_frames[1].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 2/not_P1_investments {}'.format(not_P1_frames[2].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 3/not_P1_investment_partners {}'.format(not_P1_frames[3].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 4/not_P1_jobs {}'.format(not_P1_frames[4].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 5/not_P1_jobs_former {}'.format(not_P1_frames[5].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 6/not_P1_jobs_former_new {}'.format(not_P1_frames[6].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 7/not_P1_jobs_partner {}'.format(not_P1_frames[7].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 8/not_P1_jobs_other_partners {}'.format(not_P1_frames[8].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 9/not_P1_invest_other_partners {}'.format(not_P1_frames[9].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 10/not_P1_jobs_current_old {}'.format(not_P1_frames[10].shape))\n",
    "        print('NODES | OUTPUT FRAME 11/not_P1_extra_org_nodes {}'.format(not_P1_frames[11].shape))\n",
    "    \n",
    "    return lst_of_frames\n",
    "\n",
    "def load_vertices(sframes, g):\n",
    "    \n",
    "    # For jobs dataframes\n",
    "    for idx in [4,5,6,8,10]:\n",
    "        # Keep relevant node attributes\n",
    "        frame_temp = sframes[idx][['person_uuid', 'person_name']].rename({'person_uuid':'__id', 'person_name':'name'})\n",
    "        frame_temp['__node_type'] = 'person'\n",
    "        # Add p1_tag to the vertex\n",
    "        frame_temp['p1_tag'] = 0\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    \n",
    "    # For jobs and partner investments dataframes\n",
    "    for idx in [2,3,4,5,6,8,9,10]:\n",
    "        # Keep relevant node attributes\n",
    "        frame_temp = sframes[idx][['org_uuid', 'org_name', 'p1_tag']].rename({'org_uuid':'__id', 'org_name':'name'})\n",
    "        frame_temp['__node_type'] = 'company'\n",
    "        # Add p1_tag to the vertex\n",
    "        frame_temp['p1_tag'] = frame_temp['p1_tag'].apply(lambda x: 0 if (x==\"\" or x==0) else 1)\n",
    "        frame_temp['p1_tag'] = frame_temp['p1_tag'].astype(int)\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    \n",
    "    # For investments dataframes\n",
    "    for idx in [2,3,7,9]:\n",
    "        # Keep relevant node attributes\n",
    "        frame_temp = sframes[idx][['investor_uuid', 'investor_name']].rename({'investor_uuid':'__id', 'investor_name':'name'})\n",
    "        frame_temp['__node_type'] = 'investor'\n",
    "        # Add p1_tag to the vertex\n",
    "        frame_temp['p1_tag'] = 0\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    \n",
    "    # For partner investments dataframes\n",
    "    for idx in [3,7,9]:\n",
    "        # Keep relevant node attributes\n",
    "        frame_temp = sframes[idx][['partner_uuid', 'partner_name']].rename({'partner_uuid':'__id', 'partner_name':'name'})\n",
    "        frame_temp['__node_type'] = 'person'\n",
    "        # Add p1_tag to the vertex\n",
    "        frame_temp['p1_tag'] = 0\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    \n",
    "    # Organizations\n",
    "    for idx in [0,1,11]:\n",
    "        # Keep relevant node attributes\n",
    "        frame_temp = sframes[idx][['uuid', 'name', 'primary_role', 'p1_tag']].rename({'uuid':'__id', 'primary_role':'__node_type'})\n",
    "        # Add p1_tag to the vertex\n",
    "        frame_temp['p1_tag'] = frame_temp['p1_tag'].apply(lambda x: 0 if (x==\"\" or x==0) else 1)\n",
    "        frame_temp['p1_tag'] = frame_temp['p1_tag'].astype(int)\n",
    "        # Load into graph\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    \n",
    "    # Return SGraph\n",
    "    return g\n",
    "\n",
    "def find_p1_affiliations(p1_sframes):\n",
    "    frames = p1_sframes.copy()\n",
    "    \n",
    "    # Combine company and investor Pledge 1% dataframes, keeping only the uuid column\n",
    "    p1_affiliations = frames[0][['uuid']].append(frames[1][['uuid']])\n",
    "    \n",
    "    # Add edge connecting to Pledge 1% uuid\n",
    "    p1_affiliations['p1_uuid'] = 'fd9e2d10-a882-c6f4-737e-fd388d4ffd7c'\n",
    "    \n",
    "    # Create id, source, destination fields in SFrame\n",
    "    p1_affiliations = p1_affiliations.rename({'uuid':'src','p1_uuid':'dst'})\n",
    "    p1_affiliations['p1_tag'] = 1\n",
    "    \n",
    "    # Return SFrame\n",
    "    return p1_affiliations\n",
    "\n",
    "def load_edges(sframes, g, p1_affiliations=[], include_edges=[2,3], reverse=False, add_weights=False):\n",
    "    w = {'status':{'primary':3,'secondary':2,'tertiary':1}, '__edge_type':{'job':1, 'investment':2}}\n",
    "    \n",
    "    # Since it is a directed graph, need to include option for reverse direction\n",
    "    # Forward\n",
    "    source = 'src'\n",
    "    destination = 'dst'\n",
    "    # Reverse\n",
    "    if reverse:\n",
    "        source = 'dst'\n",
    "        destination = 'src'\n",
    "\n",
    "    if type(p1_affiliations) == SFrame:\n",
    "        # P1 Companies: Company/Investor --> Pledge 1%\n",
    "        g = g.add_edges(edges=p1_affiliations, src_field=source, dst_field=destination)\n",
    "        if add_weights:\n",
    "            frame_temp['weight'] = 6\n",
    "    \n",
    "    # Investments: Investor --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[2][['investment_uuid','investor_uuid','org_uuid','investment_type','raised_amount_usd','investor_count','is_lead_investor','lead_investor_count']].rename({'investment_uuid':'__id','investor_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 'primary'\n",
    "    if add_weights:\n",
    "        frame_temp['weight'] = w['__edge_type']['investment'] * w['status']['primary']\n",
    "    g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "    \n",
    "    # Partner Investments, Investments: Person --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[3][['investment_uuid','partner_uuid','org_uuid','investment_type','raised_amount_usd','investor_count']].rename({'investment_uuid':'__id','partner_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 'primary'\n",
    "    if add_weights:\n",
    "        frame_temp['weight'] = w['__edge_type']['investment'] * w['status']['primary']\n",
    "    g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "    \n",
    "    # Partner Investments, Investments: Investor --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[3][['investor_uuid','org_uuid','investment_type','investor_count']].rename({'investor_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 'secondary'\n",
    "    if add_weights:\n",
    "        frame_temp['weight'] = w['__edge_type']['investment'] * w['status']['secondary']\n",
    "    # Secondary relationships, skip if not specified at input\n",
    "    if 2 in include_edges:\n",
    "        g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "    \n",
    "    # Partner Investments, Jobs: Person --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[7][['partner_uuid','investor_uuid']].rename({'partner_uuid':'src','investor_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'job'\n",
    "    frame_temp['status'] = 'secondary'\n",
    "    if add_weights:\n",
    "        frame_temp['weight'] = w['__edge_type']['job'] * w['status']['secondary']\n",
    "    # Secondary relationships, skip if not specified at input\n",
    "    if 2 in include_edges:\n",
    "        g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)    \n",
    "    \n",
    "    # Other Partner Investments, Investments: Person --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[9][['investment_uuid','partner_uuid','org_uuid','investment_type','raised_amount_usd','investor_count']].rename({'investment_uuid':'__id','partner_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 'tertiary'\n",
    "    if add_weights:\n",
    "        frame_temp['weight'] = w['status']['tertiary'] * w['__edge_type']['investment']\n",
    "    # Tertiary relationships, skip if not specified at input\n",
    "    if 3 in include_edges:\n",
    "        g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "    \n",
    "    # Jobs: Person --> Company\n",
    "    for idx in [4,5,6,8,10]:\n",
    "        # Create id, source, destination fields in SFrame\n",
    "        frame_temp = sframes[idx][['job_uuid','person_uuid','org_uuid','job_type','title']].rename({'job_uuid':'__id','person_uuid':'src','org_uuid':'dst'})\n",
    "        frame_temp['__edge_type'] = 'job'\n",
    "        # Current jobs\n",
    "        if idx == 4:\n",
    "            frame_temp['status'] = 'primary'\n",
    "            if add_weights:\n",
    "                frame_temp['weight'] = w['status']['primary'] * w['__edge_type']['job']\n",
    "            g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "            continue\n",
    "        \n",
    "        # Secondary relationships, skip if not specified at input\n",
    "        if 2 in include_edges:\n",
    "            # Former jobs | Former new jobs | Current old jobs \n",
    "            if idx in [5,6,10]:\n",
    "                frame_temp['status'] = 'secondary'\n",
    "                if add_weights:\n",
    "                    frame_temp['weight'] = w['status']['secondary'] * w['__edge_type']['job']\n",
    "                g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "                continue\n",
    "        \n",
    "        # Tertiary relationships, skip if not specified at input\n",
    "        if 3 in include_edges:\n",
    "            # Other partners at firm\n",
    "            if idx == 8:\n",
    "                frame_temp['status'] = 'tertiary'\n",
    "                if add_weights:\n",
    "                    frame_temp['weight'] = w['status']['tertiary'] * w['__edge_type']['job']\n",
    "                g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "                continue\n",
    "    \n",
    "    # Return SGraph\n",
    "    return g\n",
    "\n",
    "def update_cb_weights(src, edge, dst):\n",
    "    if src['__id'] != dst['__id']: # ignore self-links\n",
    "        edge['weight'] = 0\n",
    "        edge['weight_status'] = 0\n",
    "        edge['weight_type'] = 0\n",
    "        if edge['status'] == 'primary':\n",
    "            edge['weight_status'] = 3\n",
    "        if edge['status'] == 'secondary':\n",
    "            edge['weight_status'] = 2\n",
    "        if edge['status'] == 'tertiary':\n",
    "            edge['weight_status'] = 1\n",
    "        if edge['__edge_type'] == 'job':\n",
    "            edge['weight_type'] = 1\n",
    "        if edge['__edge_type'] == 'investment':\n",
    "            edge['weight_type'] = 2\n",
    "        edge['weight'] = edge['weight_status'] * edge['weight_type']\n",
    "    return (src, edge, dst)\n",
    "#cb = cb.triple_apply(update_cb_weights, ['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/0_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/0_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 2.15709 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 2.15709 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,float,str,str,str,str,str,float,str,str,str,str,int,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 290126 lines. Lines per second: 162704</pre>"
      ],
      "text/plain": [
       "Read 290126 lines. Lines per second: 162704"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/0_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/0_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 825393 lines in 3.67096 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 825393 lines in 3.67096 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/1_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/1_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.156187 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.156187 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,float,str,str,str,str,str,float,str,str,str,str,int,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/1_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/1_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 31499 lines in 0.127475 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 31499 lines in 0.127475 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/2_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/2_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 1.17407 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 1.17407 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,float,float,float,str,float,str,str,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 201405 lines. Lines per second: 140951</pre>"
      ],
      "text/plain": [
       "Read 201405 lines. Lines per second: 140951"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/2_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/2_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 453058 lines in 2.44735 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 453058 lines in 2.44735 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/3_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/3_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.68244 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.68244 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,float,float,float,str,float,str,str,float,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/3_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/3_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 89926 lines in 0.755397 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 89926 lines in 0.755397 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/4_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/4_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 1.31167 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 1.31167 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 279331 lines. Lines per second: 199191</pre>"
      ],
      "text/plain": [
       "Read 279331 lines. Lines per second: 199191"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/4_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/4_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 395270 lines in 1.76282 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 395270 lines in 1.76282 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/5_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/5_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.928988 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.928988 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/5_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/5_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 182483 lines in 0.941509 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 182483 lines in 0.941509 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/6_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/6_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 1.33069 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 1.33069 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 280191 lines. Lines per second: 191209</pre>"
      ],
      "text/plain": [
       "Read 280191 lines. Lines per second: 191209"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/6_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/6_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 483629 lines in 2.06879 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 483629 lines in 2.06879 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/7_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/7_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.040904 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.040904 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/7_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/7_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 11771 lines in 0.043138 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 11771 lines in 0.043138 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/8_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/8_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 1.32664 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 1.32664 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/8_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/8_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 434370 lines in 1.82176 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 434370 lines in 1.82176 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/9_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/9_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 1.0505 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 1.0505 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,float,float,float,str,float,str,str,float,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 138777 lines. Lines per second: 115525</pre>"
      ],
      "text/plain": [
       "Read 138777 lines. Lines per second: 115525"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/9_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/9_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 161382 lines in 1.37691 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 161382 lines in 1.37691 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/10_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/10_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 1.68709 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 1.68709 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/10_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/10_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 289847 lines in 1.71245 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 289847 lines in 1.71245 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/11_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/11_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 1.15816 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 1.15816 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,float,str,str,str,str,str,float,str,str,str,str,int,str,int]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/11_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/cb/11_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 225184 lines in 1.08963 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 225184 lines in 1.08963 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/0_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/0_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.043551 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.043551 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,float,str,str,str,str,str,float,str,str,str,str,int,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/0_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/0_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 6615 lines in 0.034245 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 6615 lines in 0.034245 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/1_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/1_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.00897 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.00897 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,float,str,str,str,str,str,float,str,str,str,str,int,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/1_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/1_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 141 lines in 0.01218 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 141 lines in 0.01218 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/2_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/2_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.096398 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.096398 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,float,float,float,str,float,str,str,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/2_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/2_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 12005 lines in 0.065815 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 12005 lines in 0.065815 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/3_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/3_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.036806 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.036806 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,float,float,float,str,float,str,str,float,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/3_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/3_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 3628 lines in 0.036361 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 3628 lines in 0.036361 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/4_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/4_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.069118 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.069118 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/4_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/4_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 11758 lines in 0.063147 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 11758 lines in 0.063147 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/5_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/5_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.04386 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.04386 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/5_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/5_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 6653 lines in 0.047804 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 6653 lines in 0.047804 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/6_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/6_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.098345 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.098345 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/6_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/6_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 17224 lines in 0.081453 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 17224 lines in 0.081453 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/7_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/7_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.012517 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.012517 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 1460 lines. Lines per second: 93559.8</pre>"
      ],
      "text/plain": [
       "Read 1460 lines. Lines per second: 93559.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/7_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/7_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 1460 lines in 0.015874 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 1460 lines in 0.015874 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/8_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/8_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.151914 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.151914 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/8_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/8_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 25036 lines in 0.118933 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 25036 lines in 0.118933 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/9_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/9_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.257642 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.257642 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,float,float,float,str,float,str,str,float,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/9_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/9_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 33517 lines in 0.278732 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 33517 lines in 0.278732 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/10_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/10_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.082627 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.082627 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/10_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/10_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 13729 lines in 0.07035 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 13729 lines in 0.07035 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/11_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/11_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.159083 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.159083 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,float,str,str,str,str,str,float,str,str,str,str,int,str,int]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/11_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/sanjayelangovan/Desktop/School/Grad School/Term 3/coursework/Projects/final_project/crunchbase-p1-machine-learning/files/output/graph_temp/p1/11_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 24993 lines in 0.112855 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 24993 lines in 0.112855 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lst_of_frames = []\n",
    "for val in ['cb','p1']:\n",
    "    lst = []\n",
    "    for idx in range(12):\n",
    "        path = 'files/output/graph_temp/{}/{}_df.csv'.format(val, idx)\n",
    "        lst.append(SFrame(data=path))\n",
    "    lst_of_frames.append(lst)\n",
    "cb_sframes,p1_sframes = lst_of_frames\n",
    "\n",
    "# List of Pledge 1% uuids\n",
    "global p1_companies_uuid\n",
    "p1_companies_uuid = []\n",
    "p1_companies_uuid.extend(list(p1_sframes[0]['uuid'].unique()))\n",
    "p1_companies_uuid.extend(list(p1_sframes[1]['uuid'].unique()))\n",
    "p1_companies_uuid = list(set(p1_companies_uuid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BuIlDiNg GrApH...\n",
      "- REMOVING PARALLEL EDGES\n",
      "\n",
      "SAVING Cruncbase_1Way_SingleEdge\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "def make_graph(cb_sframes, weights=False, reverse_edges=False, remove_parallel_edges=False):\n",
    "    \n",
    "    print('\\nBuIlDiNg GrApH...')\n",
    "    # Load in crunchbase with relationships\n",
    "    \n",
    "    # If adding weights...\n",
    "    if weights:\n",
    "        print('- ADDING WEIGHTS IN THE FORWARD DIRECTION')\n",
    "        cb = load_edges(cb_sframes, load_vertices(cb_sframes, SGraph()), p1_affiliations=[], include_edges=[2,3], reverse=False, add_weights=True)\n",
    "    elif not weights:\n",
    "        cb = load_edges(cb_sframes, load_vertices(cb_sframes, SGraph()), p1_affiliations=[], include_edges=[2,3], reverse=False, add_weights=False)\n",
    "    \n",
    "    # If adding reversed edges...\n",
    "    if reverse_edges:\n",
    "        print('- ADDING EDGES IN THE REVERSE DIRECTION')\n",
    "        # If adding weights...\n",
    "        if weights:\n",
    "            print('  - ADDING WEIGHTS IN THE REVERSE DIRECTION')\n",
    "            cb = load_edges(cb_sframes, cb, p1_affiliations=[], include_edges=[2,3], reverse=True, add_weights=True)\n",
    "        elif not weights:\n",
    "            cb = load_edges(cb_sframes, cb, p1_affiliations=[], include_edges=[2,3], reverse=True, add_weights=False)\n",
    "\n",
    "#     # Before comparison\n",
    "    before = cb.summary()\n",
    "    before_pri = cb.get_edges(fields={'status':'primary'}).shape[0]\n",
    "    before_sec = cb.get_edges(fields={'status':'secondary'}).shape[0]\n",
    "    before_ter = cb.get_edges(fields={'status':'tertiary'}).shape[0]\n",
    "\n",
    "    # Get list of edge fields\n",
    "    graph_edge_fields = cb.get_edge_fields()\n",
    "    \n",
    "    # If removing parallel edges...\n",
    "    if remove_parallel_edges:\n",
    "        print('- REMOVING PARALLEL EDGES')\n",
    "        # Create temporary edge attribute that you'll use in aggregate function\n",
    "        cb.edges['relationship'] = cb.edges['status']\n",
    "        if weights:\n",
    "            cb = SGraph(cb.vertices, cb.edges.groupby(['__src_id','__dst_id','__edge_type','weight'], {'status': aggregate.SELECT_ONE('relationship')}))\n",
    "        elif not weights:\n",
    "            cb = SGraph(cb.vertices, cb.edges.groupby(['__src_id','__dst_id','__edge_type'], {'status': aggregate.SELECT_ONE('relationship')}))\n",
    "    elif not remove_parallel_edges:\n",
    "         # Create temporary edge attribute that you'll use in aggregate function\n",
    "        cb.edges['combined'] = cb.edges['__id']+','+cb.edges['status']+','+cb.edges['__src_id']+','+cb.edges['__dst_id']\n",
    "        cb = SGraph(cb.vertices, cb.edges.groupby(graph_edge_fields, {'combined': aggregate.SELECT_ONE('combined')}))\n",
    "        del cb.edges['combined']\n",
    "\n",
    "#     # After comparison\n",
    "    after = cb.summary()\n",
    "    after_pri = cb.get_edges(fields={'status':'primary'}).shape[0]\n",
    "    after_sec = cb.get_edges(fields={'status':'secondary'}).shape[0]\n",
    "    after_ter = cb.get_edges(fields={'status':'tertiary'}).shape[0]\n",
    "\n",
    "# #     # Output\n",
    "# #     print('\\nRemove duplicates from Crunchbase graph')\n",
    "# #     print('\\nNode change: {:,} --> {:,}'.format(before['num_vertices'], after['num_vertices']))\n",
    "# #     print('Edge change: {:,} --> {:,}'.format(before['num_edges'], after['num_edges']))\n",
    "# #     print('\\nPRIMARY Edge change: {:,} --> {:,}'.format(before_pri,after_pri))\n",
    "# #     print('SECONDARY Edge change: {:,} --> {:,}'.format(before_sec,after_sec))\n",
    "# #     print('TERTIARY Edge change: {:,} --> {:,}'.format(before_ter,after_ter))\n",
    "    \n",
    "    # Save and load graphs\n",
    "    # UPDATE PATH \n",
    "    if not reverse_edges and  not remove_parallel_edges: #(~A,~B)\n",
    "        name = 'Cruncbase_1Way_MultiEdge'\n",
    "    elif reverse_edges and not remove_parallel_edges: #(A,~B)\n",
    "        name = 'Crunchbase_2Ways_MultiEdge'\n",
    "    elif not reverse_edges and remove_parallel_edges: #(~A,B)\n",
    "        name = 'Cruncbase_1Way_SingleEdge'\n",
    "    elif reverse_edges and remove_parallel_edges: #(A,B)\n",
    "        name = 'Crunchbase_2Ways_SingleEdge'\n",
    "    if weights:\n",
    "        name += '_Weighted'\n",
    "    print('\\nSAVING {}'.format(name))\n",
    "    print('*'*50)\n",
    "    path = 'CrunchbaseGraphs/{}'.format(name)\n",
    "    cb.save(path)\n",
    "    cb = load_sgraph(path)\n",
    "    return cb\n",
    "\n",
    "# Construct all 8\n",
    "for weights_bool in [False]:\n",
    "    for reverse_bool in [False]:\n",
    "        for parallel_bool in [True]:\n",
    "            cb = make_graph(cb_sframes, weights=weights_bool, reverse_edges=reverse_bool, remove_parallel_edges=parallel_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = load_sgraph('files/CrunchbaseGraphs/Crunchbase_2Ways_MultiEdge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_tag = cb.vertices[cb.vertices['p1_tag']==1]\n",
    "p1_tag_df = pd.DataFrame(p1_tag['__id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turicreate import load_sgraph\n",
    "from turicreate import shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "0 P1 companies have been checked.\n",
      "100 P1 companies have been checked.\n",
      "200 P1 companies have been checked.\n",
      "300 P1 companies have been checked.\n",
      "400 P1 companies have been checked.\n",
      "500 P1 companies have been checked.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-65e4903a598c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdistances2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdistances2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#create new column in df1 to check if prices match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp1_tag_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp1_tag_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/turicreate/data_structures/sarray.py\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0melems_at_a_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_check = 0\n",
    "\n",
    "print(\"Starting\")\n",
    "for i in p1_tag_df[0:3469][0]:\n",
    "    sp = shortest_path.create(cb, source_vid=i, verbose = False)\n",
    "    a = sp['distance']\n",
    "    \n",
    "    if initial_check == 0:\n",
    "        distances2 = a\n",
    "        initial_check = 1\n",
    "        \n",
    "    else:\n",
    "        distances2['distance'] = np.where(a['distance'] < distances2['distance'], a['distance'], distances2['distance'])  #create new column in df1 to check if prices match\n",
    "        \n",
    "    if (p1_tag_df[p1_tag_df[0]==i].index.values % 100 == 0):\n",
    "        print (str(int(p1_tag_df[p1_tag_df[0]==i].index.values)) + \" P1 companies have been checked.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances2 = pd.DataFrame(distances2)\n",
    "path = 'files/output/graph_model/model/spath.csv'\n",
    "print ('SAVED TO CSV', path)\n",
    "distances2.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1_tag_df[p1_tag_df[0]==i].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
