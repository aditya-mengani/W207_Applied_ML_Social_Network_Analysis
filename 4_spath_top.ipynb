{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turicreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing basic data analysis packages'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "#datetime.today().strftime('%Y%m%d')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "'''Graph'''\n",
    "import networkx as nx\n",
    "#from pyvis.network import Network\n",
    "from turicreate import SFrame, SGraph, pagerank, degree_counting, aggregate, visualization\n",
    "\n",
    "'''Plotting packages'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#%matplotlib inline\n",
    "sns.set(style='white', font_scale=1.3)\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):   \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def network_by_date(date, df_input, jobs_input, invest_input, invest_prtnr_input, model_uuids=[], skip_not_p1=True):\n",
    "    '''\n",
    "    This function filters down Crunchbase dataframes by date \n",
    "    to ensure that the companies/people/investments being used in modeling exist at a given time.\n",
    "\n",
    "    INPUT:\n",
    "        - `date`: string w/ format 'YEAR-MO-DY' (e.g. '2020-09-08')\n",
    "        - `df`: pandas dataframe of Crunchbase organizationss with necessary column fields:\n",
    "            * `p1_date`, `founded_on`, `closed_on`\n",
    "        - `jobs`: pandas dataframe of Crunchbase jobss with necessary column fields:\n",
    "            * `p1_date`, `started_on`, `ended_on`\n",
    "        - `invest`: pandas dataframe of Crunchbase investmentss with necessary column fields:\n",
    "            * `p1_date`, `announced_on`\n",
    "        - `invest_prtnr`: pandas dataframe of Crunchbase investments with necessary column fields:\n",
    "            * `p1_date`, `announced_on`\n",
    "        - `model_uuids`: list that contains the uuids of organizations that are used to construct the model graph\n",
    "    \n",
    "    OUTPUT:\n",
    "        - List of dataframe lists, 3 (or 4) lists of length 10: \n",
    "            * [Crunchbase neighborhood dataframes], [Pledge 1% neighborhood dataframes], \n",
    "              [~Pledge 1% neighborhood dataframes], { [Model neighborhood dataframes] }\n",
    "        - Each dataframe list contains dataframes that will be used in the next processing step:\n",
    "            0. Companies\n",
    "            1. Investors\n",
    "            2. Investments\n",
    "            3. Partner investments\n",
    "            4. Current Jobs\n",
    "            5. Former jobs\n",
    "            6. Former affiliated's new jobs\n",
    "            7. Partner investor's affiliation (if not in jobs dataframes)\n",
    "            8. Partner investor's coworkers at the investing firm\n",
    "            9. Partner investor's coworkers' partner investments\n",
    "            10. Current affiliated's old jobs\n",
    "            11. Organization nodes from edges in 2,3,6,7,9,10 if not already in 0 or 1\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Soft copy of dataframes\n",
    "    df = df_input.copy()\n",
    "    jobs = jobs_input.copy()\n",
    "    invest = invest_input.copy()\n",
    "    invest_prtnr = invest_prtnr_input.copy()\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # DATE PROCESSING\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    df['p1_date'] = pd.to_datetime(df['p1_date'], errors='coerce')\n",
    "    df['founded_on'] = pd.to_datetime(df['founded_on'], errors='coerce')\n",
    "    df['closed_on'] = pd.to_datetime(df['closed_on'], errors='coerce')\n",
    "    jobs['p1_date'] = pd.to_datetime(jobs['p1_date'], errors='coerce')\n",
    "    jobs['started_on'] = pd.to_datetime(jobs['started_on'], errors='coerce')\n",
    "    jobs['ended_on'] = pd.to_datetime(jobs['ended_on'], errors='coerce')\n",
    "    invest['p1_date'] = pd.to_datetime(invest['p1_date'], errors='coerce')\n",
    "    invest['announced_on'] = pd.to_datetime(invest['announced_on'], errors='coerce')\n",
    "    invest_prtnr['p1_date'] = pd.to_datetime(invest_prtnr['p1_date'], errors='coerce')\n",
    "    invest_prtnr['announced_on'] = pd.to_datetime(invest_prtnr['announced_on'], errors='coerce')\n",
    "    \n",
    "    # Convert input date to datetime object\n",
    "    date = pd.Timestamp(date)\n",
    "    print('\\nAS OF {}:\\n'.format(date.strftime('%B %d, %Y').upper()))\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # Create new row for tagging model companies\n",
    "    df['add_to_model'] = 0\n",
    "    df['add_to_model'][df['uuid'].isin(model_uuids)] = 1\n",
    "    jobs['add_to_model'] = 0\n",
    "    jobs['add_to_model'][jobs['org_uuid'].isin(model_uuids)] = 1\n",
    "    invest['add_to_model'] = 0\n",
    "    invest['add_to_model'][invest['org_uuid'].isin(model_uuids)] = 1\n",
    "    invest_prtnr['add_to_model'] = 0\n",
    "    invest_prtnr['add_to_model'][invest_prtnr['org_uuid'].isin(model_uuids)] = 1\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # COMPANY FILTER\n",
    "    # Crunchbase company must be founded after DATE and closed before DATE (or DATE == NaT)\n",
    "    CB_companies = df[(df['founded_on']<=date) & \n",
    "                      ((df['closed_on']>date) | (pd.isnull(df['closed_on']))) & \n",
    "                      (df['primary_role']=='company')].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # INVESTOR FILTER:\n",
    "    # Crunchbase investor must be founded AFTER date and closed BEFORE date (or date == NaT)\n",
    "    CB_investors = df[(df['founded_on']<=date) & \n",
    "                      ((df['closed_on']>date) | (pd.isnull(df['closed_on']))) & \n",
    "                      (df['primary_role']=='investor')].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # INVESTMENT FILTER\n",
    "    # Crunchbase investment must have taken place BEFORE date\n",
    "    CB_investments = invest[(invest['announced_on']<=date) & \n",
    "                            (invest['investor_type']=='organization')].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # PARTNER INVESTMENT FILTER\n",
    "    # Crunchbase partner investment must have taken place BEFORE date\n",
    "    CB_investment_partners = invest_prtnr[invest_prtnr['announced_on']<=date].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # CURRENT JOB FILTER\n",
    "    # Crunchbase job must have started BEFORE date and ended AFTER date (or date == NaT)\n",
    "    CB_jobs = jobs[(jobs['job_type'].isin(['executive','board_member','advisor','board_observer'])) & \n",
    "                      (jobs['started_on']<=date) & \n",
    "                      ((jobs['ended_on']>date) | (pd.isnull(jobs['ended_on'])))].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # FORMER JOB FILTER\n",
    "    # Crunchbase job must have ended BEFORE date or started AFTER date\n",
    "    CB_jobs_former = jobs[(jobs['job_type'].isin(['executive','board_member','advisor','board_observer'])) & \n",
    "                          ((jobs['ended_on']<=date) | (jobs['started_on']>date))].reset_index(drop=True)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # COMBINE THESE 6 (or 7) INTO LIST OF FRAMES\n",
    "    lst_of_frames = []\n",
    "    \n",
    "    # Crunchbase frames\n",
    "    CB_frames = [CB_companies,CB_investors,CB_investments,CB_investment_partners,CB_jobs,CB_jobs_former]\n",
    "    \n",
    "    # Add to list of frames\n",
    "    lst_of_frames.append(CB_frames)\n",
    "    \n",
    "    # If model_uuids are not supplied, calculate Pledge 1% neighborhood\n",
    "    if model_uuids == []:\n",
    "        P1_frames = []\n",
    "        for frame in CB_frames:\n",
    "            \n",
    "            # Pledge 1% frames must have Crunchbase assumptions in addition to an earlier pledge date\n",
    "            new_frame = frame[frame['p1_date']<=date].reset_index(drop=True).drop('add_to_model',axis=1)\n",
    "            P1_frames.append(new_frame)\n",
    "        # Add to list of frames\n",
    "        lst_of_frames.append(P1_frames)\n",
    "    \n",
    "    # If model_uuids are supplied, calculate model neighborhood\n",
    "    if model_uuids != []:\n",
    "        model_frames = []\n",
    "        for frame in CB_frames:\n",
    "            \n",
    "            # Include model dataframe if condition satisfied: either are a Pledge 1% company or tagged by model_uuids\n",
    "            new_frame=frame[(frame['p1_date']<=date) | (frame['add_to_model']==1)].reset_index(drop=True).drop('add_to_model',axis=1)\n",
    "            model_frames.append(new_frame)\n",
    "        \n",
    "        # Add to list of frames\n",
    "        lst_of_frames.append(model_frames)\n",
    "    \n",
    "    # If this boolean value is False, calculate ~Pledge 1% neighborhood\n",
    "    if skip_not_p1 is False:\n",
    "        not_P1_frames = []\n",
    "        for frame in CB_frames:\n",
    "            \n",
    "            # Non-Pledge 1% frames must have Crunchbase assumptions in addition to NaT pledge date or later pledge date\n",
    "            new_frame = frame[(pd.isnull(frame['p1_date']) | (frame['p1_date']>date))].reset_index(drop=True).drop('add_to_model',axis=1)\n",
    "            not_P1_frames.append(new_frame)\n",
    "        \n",
    "        # Add to list of frames\n",
    "        lst_of_frames.append(not_P1_frames)\n",
    "        \n",
    "    # Remove extra column 'add_to_model'\n",
    "    for idx,frame in enumerate(CB_frames):\n",
    "        CB_frames[idx] = frame.drop('add_to_model',axis=1)\n",
    "\n",
    "    #*******************************************************************************************************\n",
    "    # FORMER NEW JOB FILTER\n",
    "    print('CaLcUlAtInG... FORMER NEW JOB FILTER')\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        \n",
    "        # Where do the former affiliated work now?\n",
    "        former_people = frame[5].person_uuid.unique() # Pull their IDs\n",
    "        jobs_former_new = CB_frames[4][CB_frames[4].person_uuid.isin(former_people)] # Pull their current jobs from Crunchbase\n",
    "\n",
    "        # Check they're not already in the current jobs dataframe\n",
    "        # Combine into one temp data frame\n",
    "        combined_jobs = pd.concat([frame[4], jobs_former_new]).reset_index(drop=True) \n",
    "        df_gpby = combined_jobs.groupby(list(combined_jobs.columns))\n",
    "        \n",
    "        # Only count non-duplicated columns\n",
    "        idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "        \n",
    "        # Reindex dataframe\n",
    "        jobs_former_new = combined_jobs.reindex(idx)\n",
    "        \n",
    "        # Add to list of frames\n",
    "        frame.append(jobs_former_new)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # PARTNER INVESTMENT JOB FILTER\n",
    "    print('CaLcUlAtInG... PARTNER INVESTMENT JOB FILTER')\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        \n",
    "        # Are the partner investment jobs already in one of the jobs dataframes? If not, we should add them.\n",
    "        \n",
    "        # Create temporary dataframe and column to make checking the intersection between dataframes easier \n",
    "        # frame[4]: current jobs | frame[5]: former jobs | frame[6]: former new jobs\n",
    "        jobs_combined = pd.concat([frame[4],frame[5],frame[6]])\n",
    "        jobs_combined['person,company'] = jobs_combined['person_uuid'] + ',' + jobs_combined['org_uuid']\n",
    "        \n",
    "        # frame[3]: partner investments\n",
    "        frame[3]['person,company'] = frame[3]['partner_uuid']+ ',' + frame[3]['investor_uuid']\n",
    "\n",
    "        # Number of unique partner investments\n",
    "        unique_PI = frame[3]['person,company'].unique()\n",
    "\n",
    "        # Overlap between PI and combined J frames, create temporary jobs view\n",
    "        # These PI are already found in J frames, so we do not need to include them\n",
    "        jobs_already_in_J = jobs_combined[jobs_combined['person,company'].isin(unique_PI)] \n",
    "\n",
    "        # This will return non intersecting value of PI with temp J\n",
    "        # These PI are not found in J, so we would like to include them\n",
    "        PI_not_in_J = np.setdiff1d(unique_PI,jobs_already_in_J['person,company'].unique())\n",
    "\n",
    "        # Need to create separate jobs dataframe for non intersecting PI/J person/company pairs\n",
    "        grouped = frame[3][frame[3]['person,company'].isin(PI_not_in_J)].groupby(['partner_uuid','partner_name','investor_uuid','investor_name']).count()\n",
    "        grouped_df = grouped.reset_index()[['partner_uuid','partner_name','investor_uuid','investor_name']]\n",
    "        grouped_df['job_type'] = 'executive'\n",
    "        \n",
    "        # Add to list of frames\n",
    "        frame.append(grouped_df)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # OTHER FIRM PARNTERS\n",
    "    print('CaLcUlAtInG... OTHER FIRM PARTNER JOBS & INVESTMENTS FILTER')\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        \n",
    "        # OTHER FIRM PARNTERS - JOBS\n",
    "        # Who are the other partners that work at the investment firms present in the neighborhood?\n",
    "        \n",
    "        # Get the unique investor uuids associated with the dataframes\n",
    "        # frame[2]: from investments dataframe\n",
    "        unique_investor_firm_A = list(frame[2]['investor_uuid'].unique())\n",
    "        \n",
    "        # frame[3]: from partner investments dataframe\n",
    "        unique_investor_firm_B = list(frame[3]['investor_uuid'].unique())\n",
    "        partners = list(frame[3]['partner_uuid'].unique())\n",
    "        \n",
    "        # Combine to get list of unique uuids of VC firms\n",
    "        unique_firms = list(set(unique_investor_firm_A+unique_investor_firm_B))\n",
    "        \n",
    "        # Grab current jobs from Crunchbase for these investing firms\n",
    "        # Exclude duplicate partner job (already represented by partners list calculated above)\n",
    "        partner_jobs = CB_frames[4][(CB_frames[4]['org_uuid'].isin(unique_firms)) &  \n",
    "                                    ~(CB_frames[4]['person_uuid'].isin(partners))].reset_index(drop=True)\n",
    "        \n",
    "        # Check they're not already in the current/former jobs dataframe\n",
    "        # Combine into one temp data frame\n",
    "        combined_jobs = pd.concat([frame[4], partner_jobs]).reset_index(drop=True) \n",
    "        df_gpby = combined_jobs.groupby(list(combined_jobs.columns))\n",
    "        \n",
    "        # Only count non-duplicated rows\n",
    "        idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "        \n",
    "        # Reindex dataframe\n",
    "        partner_jobs = combined_jobs.reindex(idx)\n",
    "        \n",
    "        # Add to list of frames\n",
    "        frame.append(partner_jobs)\n",
    "        \n",
    "        # OTHER FIRM PARNTERS - PARTNER INVESTMENTS\n",
    "        # For these new partners, what companies are they invested in?\n",
    "        # Get the unique parnter uuids associated with the dataframes\n",
    "        other_partners = partner_jobs['person_uuid'].unique()\n",
    "        other_partner_investments = CB_frames[3][CB_frames[3]['partner_uuid'].isin(other_partners)]\n",
    "        \n",
    "        # Check they're not already in the partner investments dataframe\n",
    "        # Combine into one temp data frame\n",
    "        combined_jobs = pd.concat([frame[3], other_partner_investments]).reset_index(drop=True) \n",
    "        df_gpby = combined_jobs.groupby(list(combined_jobs.columns))\n",
    "        \n",
    "        # Only count non-duplicated rows\n",
    "        idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "        \n",
    "        # Reindex dataframe\n",
    "        other_partner_investments = combined_jobs.reindex(idx)\n",
    "        \n",
    "        # Add to list of frames\n",
    "        frame.append(other_partner_investments)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    # CURRENT OLD JOB FILTER\n",
    "    print('CaLcUlAtInG... CURRENT OLD JOB FILTER')\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        \n",
    "        # Where did the current affiliated work previously?\n",
    "        current_people = frame[4].person_uuid.unique() # Pull their IDs\n",
    "        jobs_current_old = CB_frames[5][CB_frames[5].person_uuid.isin(current_people)] # Pull their current jobs from Crunchbase\n",
    "\n",
    "        # Check they're not already in the current jobs dataframe\n",
    "        # Combine into one temp data frame\n",
    "        combined_jobs = pd.concat([frame[5], jobs_current_old]).reset_index(drop=True) \n",
    "        df_gpby = combined_jobs.groupby(list(combined_jobs.columns))\n",
    "        \n",
    "        # Only count non-duplicated columns\n",
    "        idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "        \n",
    "        # Reindex dataframe\n",
    "        jobs_current_old = combined_jobs.reindex(idx)\n",
    "        \n",
    "        # Add to list of frames\n",
    "        frame.append(jobs_current_old)\n",
    "        \n",
    "    #*******************************************************************************************************\n",
    "    # GET EXTRA ORG UUID ATTRIBUTES FROM INVESTMENTS & JOBS\n",
    "    print('CaLcUlAtInG... EXTRA ORGANIZATION NODES')\n",
    "    \n",
    "    CB_orgs = pd.concat([CB_companies, CB_investors])\n",
    "    \n",
    "    for frame in lst_of_frames:\n",
    "        \n",
    "        unique_orgs = []\n",
    "        \n",
    "        # Investments\n",
    "        unique_orgs.extend(list(frame[2]['investor_uuid'].unique()))\n",
    "        \n",
    "        # Partner investments\n",
    "        unique_orgs.extend(list(frame[3]['investor_uuid'].unique()))\n",
    "        \n",
    "        # Former new jobs organizations\n",
    "        unique_orgs.extend(list(frame[6]['org_uuid'].unique()))\n",
    "        \n",
    "        # Parter jobs organizations\n",
    "        unique_orgs.extend(list(frame[7]['investor_uuid'].unique()))\n",
    "        \n",
    "        # Other parter investments organizations\n",
    "        unique_orgs.extend(list(frame[9]['org_uuid'].unique()))\n",
    "        \n",
    "        # Current old jobs organizations\n",
    "        unique_orgs.extend(list(frame[10]['org_uuid'].unique()))\n",
    "        \n",
    "        # Pull their organization information from Crunchbase\n",
    "        new_org_nodes = CB_orgs[CB_orgs['uuid'].isin(list(set(unique_orgs)))]\n",
    "        \n",
    "        # Add to list of frames\n",
    "        frame.append(new_org_nodes)\n",
    "    \n",
    "    #*******************************************************************************************************\n",
    "    del df['add_to_model'], invest['add_to_model'], invest_prtnr['add_to_model'], jobs['add_to_model']\n",
    "    \n",
    "    # Output print statements\n",
    "    print('\\nCrunchbase Neighborhood')\n",
    "    print('NODES | OUTPUT FRAME 0/CB_companies {}'.format(CB_frames[0].shape))\n",
    "    print('NODES | OUTPUT FRAME 1/CB_investors {}'.format(CB_frames[1].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 2/CB_investments {}'.format(CB_frames[2].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 3/CB_investment_partners {}'.format(CB_frames[3].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 4/CB_jobs {}'.format(CB_frames[4].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 5/CB_jobs_former {}'.format(CB_frames[5].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 6/CB_jobs_former_new {}'.format(CB_frames[6].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 7/CB_jobs_partner {}'.format(CB_frames[7].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 8/CB_jobs_other_partners {}'.format(CB_frames[8].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 9/CB_invest_other_partners {}'.format(CB_frames[9].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 10/CB_jobs_current_old {}'.format(CB_frames[10].shape))\n",
    "    print('NODES | OUTPUT FRAME 11/CB_extra_org_nodes {}'.format(CB_frames[11].shape))\n",
    "    \n",
    "    if model_uuids != []:\n",
    "\n",
    "        print('\\nModel Neighborhood')\n",
    "        print('NODES | OUTPUT FRAME 0/model_companies {}'.format(model_frames[0].shape))\n",
    "        print('NODES | OUTPUT FRAME 1/model_investors {}'.format(model_frames[1].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 2/model_investments {}'.format(model_frames[2].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 3/model_investment_partners {}'.format(model_frames[3].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 4/model_jobs {}'.format(model_frames[4].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 5/model_jobs_former {}'.format(model_frames[5].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 6/model_jobs_former_new {}'.format(model_frames[6].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 7/model_jobs_partner {}'.format(model_frames[7].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 8/model_jobs_other_partners {}'.format(model_frames[8].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 9/model_invest_other_partners {}'.format(model_frames[9].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 10/model_jobs_current_old {}'.format(model_frames[10].shape))\n",
    "        print('NODES | OUTPUT FRAME 11/model_extra_org_nodes {}'.format(model_frames[11].shape))\n",
    "        \n",
    "        return CB_frames, model_frames\n",
    "    \n",
    "    print('\\nPledge 1% Neighborhood')\n",
    "    print('NODES | OUTPUT FRAME 0/P1_companies {}'.format(P1_frames[0].shape))\n",
    "    print('NODES | OUTPUT FRAME 1/P1_investors {}'.format(P1_frames[1].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 2/P1_investments {}'.format(P1_frames[2].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 3/P1_investment_partners {}'.format(P1_frames[3].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 4/P1_jobs {}'.format(P1_frames[4].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 5/P1_jobs_former {}'.format(P1_frames[5].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 6/P1_jobs_former_new {}'.format(P1_frames[6].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 7/P1_jobs_partner {}'.format(P1_frames[7].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 8/P1_jobs_other_partners {}'.format(P1_frames[8].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 9/P1_invest_other_partners {}'.format(P1_frames[9].shape))\n",
    "    print('NODES&EDGES | OUTPUT FRAME 10/P1_jobs_current_old {}'.format(P1_frames[10].shape))\n",
    "    print('NODES | OUTPUT FRAME 11/P1_extra_org_nodes {}'.format(P1_frames[11].shape))\n",
    "    \n",
    "    # Skip Not P1 Calculations\n",
    "    if skip_not_p1 is False:\n",
    "        \n",
    "        print('\\n~Pledge 1% Neighborhood')\n",
    "        print('NODES | OUTPUT FRAME 0/not_P1_companies {}'.format(not_P1_frames[0].shape))\n",
    "        print('NODES | OUTPUT FRAME 1/not_P1_investors {}'.format(not_P1_frames[1].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 2/not_P1_investments {}'.format(not_P1_frames[2].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 3/not_P1_investment_partners {}'.format(not_P1_frames[3].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 4/not_P1_jobs {}'.format(not_P1_frames[4].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 5/not_P1_jobs_former {}'.format(not_P1_frames[5].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 6/not_P1_jobs_former_new {}'.format(not_P1_frames[6].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 7/not_P1_jobs_partner {}'.format(not_P1_frames[7].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 8/not_P1_jobs_other_partners {}'.format(not_P1_frames[8].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 9/not_P1_invest_other_partners {}'.format(not_P1_frames[9].shape))\n",
    "        print('NODES&EDGES | OUTPUT FRAME 10/not_P1_jobs_current_old {}'.format(not_P1_frames[10].shape))\n",
    "        print('NODES | OUTPUT FRAME 11/not_P1_extra_org_nodes {}'.format(not_P1_frames[11].shape))\n",
    "    \n",
    "    return CB_frames, P1_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Successfully parsed 8 tokens: \n",
       "\t0: 91f654cf-f ... 0fc1883288\n",
       "\t1: Solomon Group\n",
       "\t2: organization\n",
       "\t3: 206310\n",
       "\t4: company\n",
       "\t5: USA\n",
       "\t6: Louisiana\n",
       "\t7: operati</pre>"
      ],
      "text/plain": [
       "Successfully parsed 8 tokens: \n",
       "\t0: 91f654cf-f ... 0fc1883288\n",
       "\t1: Solomon Group\n",
       "\t2: organization\n",
       "\t3: 206310\n",
       "\t4: company\n",
       "\t5: USA\n",
       "\t6: Louisiana\n",
       "\t7: operati"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>1 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "1 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/0_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/0_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 2.73227 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 2.73227 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,float,str,str,str,str,str,float,str,str,str,str,int,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Successfully parsed 8 tokens: \n",
       "\t0: 91f654cf-f ... 0fc1883288\n",
       "\t1: Solomon Group\n",
       "\t2: organization\n",
       "\t3: 206310\n",
       "\t4: company\n",
       "\t5: USA\n",
       "\t6: Louisiana\n",
       "\t7: operati</pre>"
      ],
      "text/plain": [
       "Successfully parsed 8 tokens: \n",
       "\t0: 91f654cf-f ... 0fc1883288\n",
       "\t1: Solomon Group\n",
       "\t2: organization\n",
       "\t3: 206310\n",
       "\t4: company\n",
       "\t5: USA\n",
       "\t6: Louisiana\n",
       "\t7: operati"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>1 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "1 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/0_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/0_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 207480 lines in 1.52614 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 207480 lines in 1.52614 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/1_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/1_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.408093 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.408093 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,float,str,str,str,str,str,float,str,str,str,str,int,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/1_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/1_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 31499 lines in 0.181389 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 31499 lines in 0.181389 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Successfully parsed 14 tokens: \n",
       "\t0: a84bb3e7-c ... 768df0a828\n",
       "\t1: 51274b49-d ... 96d7d97325\n",
       "\t2: 376db5b5-3 ... 0c3f749632\n",
       "\t3: Safeguard Scientifics\n",
       "\t4: organization\n",
       "\t5: True\n",
       "\t6: series_a\n",
       "\t7: 2016-01-04\n",
       "\t8: 1.7e+07\n",
       "\t9: \n",
       "\t10: 7\n",
       "\t11: 376db5b5-3 ... 0c3f749632\n",
       "\t12: 1\n",
       "\t13: f1c5c1</pre>"
      ],
      "text/plain": [
       "Successfully parsed 14 tokens: \n",
       "\t0: a84bb3e7-c ... 768df0a828\n",
       "\t1: 51274b49-d ... 96d7d97325\n",
       "\t2: 376db5b5-3 ... 0c3f749632\n",
       "\t3: Safeguard Scientifics\n",
       "\t4: organization\n",
       "\t5: True\n",
       "\t6: series_a\n",
       "\t7: 2016-01-04\n",
       "\t8: 1.7e+07\n",
       "\t9: \n",
       "\t10: 7\n",
       "\t11: 376db5b5-3 ... 0c3f749632\n",
       "\t12: 1\n",
       "\t13: f1c5c1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>1 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "1 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/2_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/2_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 2.1194 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 2.1194 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,float,float,float,str,float,str,str,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Successfully parsed 14 tokens: \n",
       "\t0: a84bb3e7-c ... 768df0a828\n",
       "\t1: 51274b49-d ... 96d7d97325\n",
       "\t2: 376db5b5-3 ... 0c3f749632\n",
       "\t3: Safeguard Scientifics\n",
       "\t4: organization\n",
       "\t5: True\n",
       "\t6: series_a\n",
       "\t7: 2016-01-04\n",
       "\t8: 1.7e+07\n",
       "\t9: \n",
       "\t10: 7\n",
       "\t11: 376db5b5-3 ... 0c3f749632\n",
       "\t12: 1\n",
       "\t13: f1c5c1</pre>"
      ],
      "text/plain": [
       "Successfully parsed 14 tokens: \n",
       "\t0: a84bb3e7-c ... 768df0a828\n",
       "\t1: 51274b49-d ... 96d7d97325\n",
       "\t2: 376db5b5-3 ... 0c3f749632\n",
       "\t3: Safeguard Scientifics\n",
       "\t4: organization\n",
       "\t5: True\n",
       "\t6: series_a\n",
       "\t7: 2016-01-04\n",
       "\t8: 1.7e+07\n",
       "\t9: \n",
       "\t10: 7\n",
       "\t11: 376db5b5-3 ... 0c3f749632\n",
       "\t12: 1\n",
       "\t13: f1c5c1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>1 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "1 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/2_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/2_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 145350 lines in 1.06074 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 145350 lines in 1.06074 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/3_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/3_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 1.66975 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 1.66975 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,float,float,float,str,float,str,str,float,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/3_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/3_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 89926 lines in 0.813525 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 89926 lines in 0.813525 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Successfully parsed 3 tokens: \n",
       "\t0: ddc937dc-4 ... 3634165b56\n",
       "\t1: 08a503c4-8 ... 2f4a2da2f8\n",
       "\t2: K</pre>"
      ],
      "text/plain": [
       "Successfully parsed 3 tokens: \n",
       "\t0: ddc937dc-4 ... 3634165b56\n",
       "\t1: 08a503c4-8 ... 2f4a2da2f8\n",
       "\t2: K"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>1 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "1 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/4_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/4_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 2.30251 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 2.30251 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Successfully parsed 3 tokens: \n",
       "\t0: ddc937dc-4 ... 3634165b56\n",
       "\t1: 08a503c4-8 ... 2f4a2da2f8\n",
       "\t2: K</pre>"
      ],
      "text/plain": [
       "Successfully parsed 3 tokens: \n",
       "\t0: ddc937dc-4 ... 3634165b56\n",
       "\t1: 08a503c4-8 ... 2f4a2da2f8\n",
       "\t2: K"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>1 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "1 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/4_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/4_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 206468 lines in 1.19237 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 206468 lines in 1.19237 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/5_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/5_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 2.21861 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 2.21861 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/5_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/5_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 182483 lines in 1.09412 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 182483 lines in 1.09412 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Successfully parsed 6 tokens: \n",
       "\t0: a4097e96-3 ... 5ab06ab0d3\n",
       "\t1: 6fb68603-d ... be4bf39a2e\n",
       "\t2: Rochus Moenter\n",
       "\t3: -ba61-e313 ... 6f3f1742fc\n",
       "\t4: Airbus (Co ...  Aircraft)\n",
       "\t5: 2009-0</pre>"
      ],
      "text/plain": [
       "Successfully parsed 6 tokens: \n",
       "\t0: a4097e96-3 ... 5ab06ab0d3\n",
       "\t1: 6fb68603-d ... be4bf39a2e\n",
       "\t2: Rochus Moenter\n",
       "\t3: -ba61-e313 ... 6f3f1742fc\n",
       "\t4: Airbus (Co ...  Aircraft)\n",
       "\t5: 2009-0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>1 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "1 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/6_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/6_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 2.18731 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 2.18731 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Successfully parsed 6 tokens: \n",
       "\t0: a4097e96-3 ... 5ab06ab0d3\n",
       "\t1: 6fb68603-d ... be4bf39a2e\n",
       "\t2: Rochus Moenter\n",
       "\t3: 491e2411-b ... 6f3f1742fc\n",
       "\t4: Airbus (Co ...  Aircraft)\n",
       "\t5: 2009-0</pre>"
      ],
      "text/plain": [
       "Successfully parsed 6 tokens: \n",
       "\t0: a4097e96-3 ... 5ab06ab0d3\n",
       "\t1: 6fb68603-d ... be4bf39a2e\n",
       "\t2: Rochus Moenter\n",
       "\t3: 491e2411-b ... 6f3f1742fc\n",
       "\t4: Airbus (Co ...  Aircraft)\n",
       "\t5: 2009-0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>1 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "1 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/6_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/6_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 207349 lines in 1.18744 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 207349 lines in 1.18744 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/7_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/7_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.131726 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.131726 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/7_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/7_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 11771 lines in 0.063941 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 11771 lines in 0.063941 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Successfully parsed 10 tokens: \n",
       "\t0: 30b64822-2 ... 768f0d5045\n",
       "\t1: d4936f69-7 ... 55ef63df92\n",
       "\t2: Brian Peters\n",
       "\t3: 332043cb-b ... c3e6a403fb\n",
       "\t4: Janrain\n",
       "\t5: 2015-12-01\n",
       "\t6: \n",
       "\t7: True\n",
       "\t8: Board Member\n",
       "\t9: board_member</pre>"
      ],
      "text/plain": [
       "Successfully parsed 10 tokens: \n",
       "\t0: 30b64822-2 ... 768f0d5045\n",
       "\t1: d4936f69-7 ... 55ef63df92\n",
       "\t2: Brian Peters\n",
       "\t3: 332043cb-b ... c3e6a403fb\n",
       "\t4: Janrain\n",
       "\t5: 2015-12-01\n",
       "\t6: \n",
       "\t7: True\n",
       "\t8: Board Member\n",
       "\t9: board_member"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>1 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "1 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/8_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/8_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 2.27827 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 2.27827 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Successfully parsed 10 tokens: \n",
       "\t0: 30b64822-2 ... 768f0d5045\n",
       "\t1: d4936f69-7 ... 55ef63df92\n",
       "\t2: Brian Peters\n",
       "\t3: 332043cb-b ... c3e6a403fb\n",
       "\t4: Janrain\n",
       "\t5: 2015-12-01\n",
       "\t6: \n",
       "\t7: True\n",
       "\t8: Board Member\n",
       "\t9: board_member</pre>"
      ],
      "text/plain": [
       "Successfully parsed 10 tokens: \n",
       "\t0: 30b64822-2 ... 768f0d5045\n",
       "\t1: d4936f69-7 ... 55ef63df92\n",
       "\t2: Brian Peters\n",
       "\t3: 332043cb-b ... c3e6a403fb\n",
       "\t4: Janrain\n",
       "\t5: 2015-12-01\n",
       "\t6: \n",
       "\t7: True\n",
       "\t8: Board Member\n",
       "\t9: board_member"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>1 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "1 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/8_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/8_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 212234 lines in 1.18723 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 212234 lines in 1.18723 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Successfully parsed 6 tokens: \n",
       "\t0: d77fe91e-2 ... 69b3aefaba\n",
       "\t1: 53baf618-0 ... 579e490b3f\n",
       "\t2: ba756f7b-e ... 01ba5ab41a\n",
       "\t3: Proxy Ventures\n",
       "\t4: 9697dd75-e ... 8c54d7f92f\n",
       "\t5:</pre>"
      ],
      "text/plain": [
       "Successfully parsed 6 tokens: \n",
       "\t0: d77fe91e-2 ... 69b3aefaba\n",
       "\t1: 53baf618-0 ... 579e490b3f\n",
       "\t2: ba756f7b-e ... 01ba5ab41a\n",
       "\t3: Proxy Ventures\n",
       "\t4: 9697dd75-e ... 8c54d7f92f\n",
       "\t5:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>1 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "1 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/9_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/9_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 1.9279 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 1.9279 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,float,float,float,str,float,str,str,float,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Successfully parsed 6 tokens: \n",
       "\t0: d77fe91e-2 ... 69b3aefaba\n",
       "\t1: 53baf618-0 ... 579e490b3f\n",
       "\t2: ba756f7b-e ... 01ba5ab41a\n",
       "\t3: Proxy Ventures\n",
       "\t4: 9697dd75-e ... 8c54d7f92f\n",
       "\t5:</pre>"
      ],
      "text/plain": [
       "Successfully parsed 6 tokens: \n",
       "\t0: d77fe91e-2 ... 69b3aefaba\n",
       "\t1: 53baf618-0 ... 579e490b3f\n",
       "\t2: ba756f7b-e ... 01ba5ab41a\n",
       "\t3: Proxy Ventures\n",
       "\t4: 9697dd75-e ... 8c54d7f92f\n",
       "\t5:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>1 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "1 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/9_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/9_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 105478 lines in 0.990086 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 105478 lines in 0.990086 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Successfully parsed 2 tokens: \n",
       "\t0: 633b95a6-e ... 371c5ff4e8\n",
       "\t1: b</pre>"
      ],
      "text/plain": [
       "Successfully parsed 2 tokens: \n",
       "\t0: 633b95a6-e ... 371c5ff4e8\n",
       "\t1: b"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>1 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "1 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/10_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/10_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 2.2439 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 2.2439 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Successfully parsed 2 tokens: \n",
       "\t0: 633b95a6-e ... 371c5ff4e8\n",
       "\t1: b</pre>"
      ],
      "text/plain": [
       "Successfully parsed 2 tokens: \n",
       "\t0: 633b95a6-e ... 371c5ff4e8\n",
       "\t1: b"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>1 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "1 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/10_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/10_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 198448 lines in 1.19893 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 198448 lines in 1.19893 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Successfully parsed 1 tokens: \n",
       "\t0: f58060f8-8 ... 773-efc5-7</pre>"
      ],
      "text/plain": [
       "Successfully parsed 1 tokens: \n",
       "\t0: f58060f8-8 ... 773-efc5-7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>1 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "1 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/11_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/11_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 2.37764 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 2.37764 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,float,str,str,str,str,str,float,str,str,str,str,int,str,int]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Successfully parsed 1 tokens: \n",
       "\t0: f58060f8-8 ... 773-efc5-7</pre>"
      ],
      "text/plain": [
       "Successfully parsed 1 tokens: \n",
       "\t0: f58060f8-8 ... 773-efc5-7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>1 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "1 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/cb/11_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/cb/11_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 211785 lines in 1.45112 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 211785 lines in 1.45112 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/0_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/0_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.14107 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.14107 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,float,str,str,str,str,str,float,str,str,str,str,int,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/0_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/0_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 6615 lines in 0.064888 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 6615 lines in 0.064888 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 100 lines. Lines per second: 2932.38</pre>"
      ],
      "text/plain": [
       "Read 100 lines. Lines per second: 2932.38"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/1_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/1_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.041017 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.041017 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,float,str,str,str,str,str,float,str,str,str,str,int,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/1_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/1_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 141 lines in 0.063278 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 141 lines in 0.063278 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/2_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/2_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.204139 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.204139 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,float,float,float,str,float,str,str,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/2_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/2_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 12005 lines in 0.100369 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 12005 lines in 0.100369 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/3_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/3_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.140625 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.140625 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,float,float,float,str,float,str,str,float,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/3_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/3_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 3628 lines in 0.059616 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 3628 lines in 0.059616 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/4_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/4_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.210274 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.210274 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/4_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/4_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 11758 lines in 0.091795 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 11758 lines in 0.091795 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/5_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/5_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.128933 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.128933 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/5_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/5_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 6653 lines in 0.066086 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 6653 lines in 0.066086 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/6_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/6_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.235388 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.235388 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/6_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/6_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 17224 lines in 0.114688 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 17224 lines in 0.114688 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/7_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/7_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.038222 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.038222 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/7_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/7_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 1460 lines in 0.054716 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 1460 lines in 0.054716 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/8_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/8_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.371022 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.371022 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/8_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/8_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 25036 lines in 0.15429 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 25036 lines in 0.15429 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/9_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/9_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.636895 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.636895 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,float,float,float,str,float,str,str,float,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/9_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/9_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 33517 lines in 0.340085 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 33517 lines in 0.340085 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/10_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/10_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.25672 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.25672 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,str,str,str,str,str,str,str,float,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/10_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/10_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 13729 lines in 0.098658 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 13729 lines in 0.098658 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/11_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/11_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.376027 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.376027 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,str,float,str,str,str,str,str,float,str,str,str,str,int,str,int]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/jupyter/p1/11_df.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/jupyter/p1/11_df.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 24993 lines in 0.162503 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 24993 lines in 0.162503 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lst_of_frames = []\n",
    "for val in ['cb','p1']:\n",
    "    lst = []\n",
    "    for idx in range(12):\n",
    "        path = '{}/{}_df.csv'.format(val, idx)\n",
    "        lst.append(SFrame(data=path))\n",
    "    lst_of_frames.append(lst)\n",
    "cb_sframes,p1_sframes = lst_of_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def load_vertices(sframes, g):\n",
    "    \n",
    "    # For jobs dataframes\n",
    "    for idx in [4,5,6,8,10]:\n",
    "        frame_temp = sframes[idx][['person_uuid', 'person_name']].rename({'person_uuid':'__id', 'person_name':'name'})\n",
    "        frame_temp['__node_type'] = 'person'\n",
    "        frame_temp['p1_tag'] = 0\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    \n",
    "    # For jobs and partner investments dataframes\n",
    "    for idx in [2,3,4,5,6,8,9,10]:\n",
    "        frame_temp = sframes[idx][['org_uuid', 'org_name', 'p1_tag']].rename({'org_uuid':'__id', 'org_name':'name'})\n",
    "        frame_temp['p1_tag'] = frame_temp['p1_tag'].apply(lambda x: 0 if (x==\"\" or x==0) else 1)\n",
    "        frame_temp['p1_tag'] = frame_temp['p1_tag'].astype(int)\n",
    "        frame_temp['__node_type'] = 'company'\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    \n",
    "    # For investments dataframes\n",
    "    for idx in [2,3,7,9]:\n",
    "        frame_temp = sframes[idx][['investor_uuid', 'investor_name']].rename({'investor_uuid':'__id', 'investor_name':'name'})\n",
    "        frame_temp['__node_type'] = 'investor'\n",
    "        frame_temp['p1_tag'] = 0\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    \n",
    "    # For partner investments dataframes\n",
    "    for idx in [3,7,9]:\n",
    "        frame_temp = sframes[idx][['partner_uuid', 'partner_name']].rename({'partner_uuid':'__id', 'partner_name':'name'})\n",
    "        frame_temp['__node_type'] = 'person'\n",
    "        frame_temp['p1_tag'] = 0\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    \n",
    "    # Organizations\n",
    "    for idx in [0,1,11]:\n",
    "        # Create id field in SFrame\n",
    "        frame_temp = sframes[idx][['uuid', 'name', 'primary_role', 'p1_tag']].rename({'uuid':'__id', 'primary_role':'__node_type'})\n",
    "        frame_temp['p1_tag'] = frame_temp['p1_tag'].apply(lambda x: 0 if (x==\"\" or x==0) else 1)\n",
    "        frame_temp['p1_tag'] = frame_temp['p1_tag'].astype(int)\n",
    "        g = g.add_vertices(vertices=frame_temp, vid_field='__id')\n",
    "    \n",
    "    return g\n",
    "\n",
    "def find_p1_affiliations(p1_sframes):\n",
    "    frames = copy.deepcopy(p1_sframes)\n",
    "    \n",
    "    # Combine company and investor Pledge 1% dataframes\n",
    "    p1_affiliations = frames[0][['uuid']].append(frames[1][['uuid']])\n",
    "    \n",
    "    # Add edge connecting to Pledge 1% uuid\n",
    "    p1_affiliations['p1_uuid'] = 'fd9e2d10-a882-c6f4-737e-fd388d4ffd7c'\n",
    "    \n",
    "    # Create id, source, destination fields in SFrame\n",
    "    p1_affiliations = p1_affiliations.rename({'uuid':'src','p1_uuid':'dst'})\n",
    "    p1_affiliations['p1_tag'] = 1\n",
    "    \n",
    "    return p1_affiliations\n",
    "\n",
    "p1_aff = find_p1_affiliations(p1_sframes)\n",
    "\n",
    "def load_edges(sframes, g, p1_affiliations=[], include_edges=[2,3]):\n",
    "    \n",
    "    if type(p1_affiliations) == SFrame:\n",
    "        # P1 Companies: Company/Investor --> Pledge 1%\n",
    "        g = g.add_edges(edges=p1_affiliations, src_field='src', dst_field='dst')\n",
    "    \n",
    "    # Investments: Investor --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[2][['investment_uuid','investor_uuid','org_uuid','investment_type','raised_amount_usd','investor_count','is_lead_investor','lead_investor_count']].rename({'investment_uuid':'__id','investor_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 'primary'\n",
    "    g = g.add_edges(edges=frame_temp, src_field='src', dst_field='dst')\n",
    "    \n",
    "    # Partner Investments, Investments: Person --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[3][['investment_uuid','partner_uuid','org_uuid','investment_type','raised_amount_usd','investor_count']].rename({'investment_uuid':'__id','partner_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 'primary'\n",
    "    g = g.add_edges(edges=frame_temp, src_field='src', dst_field='dst')\n",
    "    \n",
    "    # Partner Investments, Investments: Investor --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[3][['investor_uuid','org_uuid','investment_type','investor_count']].rename({'investor_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 'secondary'\n",
    "    # Secondary relationships\n",
    "    if 2 in include_edges:\n",
    "        g = g.add_edges(edges=frame_temp, src_field='src', dst_field='dst')\n",
    "    \n",
    "    # Partner Investments, Jobs: Person --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[7][['partner_uuid','investor_uuid']].rename({'partner_uuid':'src','investor_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'job'\n",
    "    frame_temp['status'] = 'secondary'\n",
    "    # Secondary relationships\n",
    "    if 2 in include_edges:\n",
    "        g = g.add_edges(edges=frame_temp, src_field='src', dst_field='dst')    \n",
    "    \n",
    "    # Other Partner Investments, Investments: Person --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[9][['investment_uuid','partner_uuid','org_uuid','investment_type','raised_amount_usd','investor_count']].rename({'investment_uuid':'__id','partner_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 'tertiary'\n",
    "    # Tertiary relationships\n",
    "    if 3 in include_edges:\n",
    "        g = g.add_edges(edges=frame_temp, src_field='src', dst_field='dst')\n",
    "    \n",
    "    # Jobs: Person --> Company\n",
    "    for idx in [4,5,6,8,10]:\n",
    "        # Create id, source, destination fields in SFrame\n",
    "        frame_temp = sframes[idx][['job_uuid','person_uuid','org_uuid','job_type','title']].rename({'job_uuid':'__id','person_uuid':'src','org_uuid':'dst'})\n",
    "        frame_temp['__edge_type'] = 'job'\n",
    "        \n",
    "        # Current jobs\n",
    "        if idx == 4:\n",
    "            frame_temp['status'] = 'primary'\n",
    "            g = g.add_edges(edges=frame_temp, src_field='src', dst_field='dst')\n",
    "            continue\n",
    "        \n",
    "        # Secondary relationships\n",
    "        if 2 in include_edges:\n",
    "            \n",
    "            # Former jobs | Former new jobs | Current old jobs \n",
    "            if idx in [5,6,10]:\n",
    "                frame_temp['status'] = 'secondary'\n",
    "                g = g.add_edges(edges=frame_temp, src_field='src', dst_field='dst')\n",
    "                continue\n",
    "        \n",
    "        # Tertiary relationships\n",
    "        if 3 in include_edges:\n",
    "            \n",
    "            # Other partners at firm\n",
    "            if idx == 8:\n",
    "                frame_temp['status'] = 'tertiary'\n",
    "                g = g.add_edges(edges=frame_temp, src_field='src', dst_field='dst')\n",
    "                continue\n",
    "\n",
    "    return g\n",
    "\n",
    "#cb = load_edges(cb_sframes, load_vertices(cb_sframes,SGraph()), p1_aff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_edges2(sframes, g, p1_affiliations=[], include_edges=[2,3], reverse=False):\n",
    "    # Since it is a directed graph, need to include option for reverse direction\n",
    "    # Forward\n",
    "    source = 'src'\n",
    "    destination = 'dst'\n",
    "    # Reverse\n",
    "    if reverse:\n",
    "        source = 'dst'\n",
    "        destination = 'src'\n",
    "    if type(p1_affiliations) == SFrame:\n",
    "        # P1 Companies: Company/Investor --> Pledge 1%\n",
    "        g = g.add_edges(edges=p1_affiliations, src_field=source, dst_field=destination)\n",
    "    # Investments: Investor --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[2][['investment_uuid','investor_uuid','org_uuid','investment_type','raised_amount_usd','investor_count','is_lead_investor','lead_investor_count']].rename({'investment_uuid':'__id','investor_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 1\n",
    "    g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "    # Partner Investments, Investments: Person --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[3][['investment_uuid','partner_uuid','org_uuid','investment_type','raised_amount_usd','investor_count']].rename({'investment_uuid':'__id','partner_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 1\n",
    "    g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "    # Partner Investments, Investments: Investor --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[3][['investor_uuid','org_uuid','investment_type','investor_count']].rename({'investor_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 2\n",
    "    # Secondary relationships, skip if not specified at input\n",
    "    if 2 in include_edges:\n",
    "        g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "    # Partner Investments, Jobs: Person --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[7][['partner_uuid','investor_uuid']].rename({'partner_uuid':'src','investor_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'job'\n",
    "    frame_temp['status'] = 2\n",
    "    # Secondary relationships, skip if not specified at input\n",
    "    if 2 in include_edges:\n",
    "        g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)    \n",
    "    # Other Partner Investments, Investments: Person --> Company\n",
    "    # Create id, source, destination fields in SFrame\n",
    "    frame_temp = sframes[9][['investment_uuid','partner_uuid','org_uuid','investment_type','raised_amount_usd','investor_count']].rename({'investment_uuid':'__id','partner_uuid':'src','org_uuid':'dst'})\n",
    "    frame_temp['__edge_type'] = 'investment'\n",
    "    frame_temp['status'] = 3\n",
    "    # Tertiary relationships, skip if not specified at input\n",
    "    if 3 in include_edges:\n",
    "        g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "    # Jobs: Person --> Company\n",
    "    for idx in [4,5,6,8,10]:\n",
    "        # Create id, source, destination fields in SFrame\n",
    "        frame_temp = sframes[idx][['job_uuid','person_uuid','org_uuid','job_type','title']].rename({'job_uuid':'__id','person_uuid':'src','org_uuid':'dst'})\n",
    "        frame_temp['__edge_type'] = 'job'\n",
    "        # Current jobs\n",
    "        if idx == 4:\n",
    "            frame_temp['status'] = 1\n",
    "            g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "            continue\n",
    "        # Secondary relationships, skip if not specified at input\n",
    "        if 2 in include_edges:\n",
    "            # Former jobs | Former new jobs | Current old jobs \n",
    "            if idx in [5,6,10]:\n",
    "                frame_temp['status'] = 2\n",
    "                g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "                continue\n",
    "        # Tertiary relationships, skip if not specified at input\n",
    "        if 3 in include_edges:\n",
    "            # Other partners at firm\n",
    "            if idx == 8:\n",
    "                frame_temp['status'] = 3\n",
    "                g = g.add_edges(edges=frame_temp, src_field=source, dst_field=destination)\n",
    "                continue\n",
    "    return g\n",
    "# Load in crunchbase with relationships defined above (primary, secondary, tertiary)\n",
    "cb = load_edges2(cb_sframes, load_vertices(cb_sframes,SGraph()), p1_affiliations=[], include_edges=[2,3], reverse=False)\n",
    "cb = load_edges2(cb_sframes, cb, p1_affiliations=[], include_edges=[2,3], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGraph({'num_edges': 2898866, 'num_vertices': 703507})\n",
       "Vertex Fields:['__id', 'name', '__node_type', 'p1_tag']\n",
       "Edge Fields:['__src_id', '__dst_id', '__id', 'investment_type', 'raised_amount_usd', 'investor_count', 'is_lead_investor', 'lead_investor_count', '__edge_type', 'status', 'job_type', 'title']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turicreate import load_sgraph\n",
    "from turicreate import shortest_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Shortest Distance to top P1 Companies (Weighted) </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pr_F_S\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "graph input must be a SGraph object.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a1141c025454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpagerank_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshortest_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_vid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/turicreate/toolkits/graph_analytics/shortest_path.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(graph, source_vid, weight_field, max_distance, verbose)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SGraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"graph input must be a SGraph object.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     opts = {\n",
      "\u001b[0;31mTypeError\u001b[0m: graph input must be a SGraph object."
     ]
    }
   ],
   "source": [
    "##Before putting it in the loop, you'll have to edit this code so that cb (in the second for loop) is the correct 2-sided dataframe\n",
    "##I didn't create all of the frames, but if you can store them in frame_list, the code should work fine.\n",
    "\n",
    "from functools import reduce\n",
    "pagerank_output = pd.read_csv('pagerank_df.csv')\n",
    "\n",
    "frame_names = ['pr_FR_S', 'pr_FR']\n",
    "frame_list = [pr_FR_S, pr_FR]\n",
    "for j,k in zip(list(pagerank_output.columns)[1:],frame_list):\n",
    "    pagerank_sorted = pagerank_output.sort_values(by=[j], ascending = False)\n",
    "\n",
    "    distances_list = []\n",
    "    for i in list(pagerank_sorted[:5]['__id']):\n",
    "        sp = shortest_path.create(k, source_vid=i, verbose = False, weight_field = 'weight')\n",
    "        df = pd.DataFrame(sp['distance'])\n",
    "        df.rename(columns={'distance': i}, inplace = True)\n",
    "        distances_list.append(df)\n",
    "\n",
    "    distances = reduce(lambda  left, right: pd.merge(left, right,on=['__id']), distances_list)\n",
    "    distances['minimum_distance'] = distances.min(axis=1) \n",
    "\n",
    "    path = 'files/output/graph_model/model/wspath_top_{}.csv'.format(j)\n",
    "    distances.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Shortest Distance to top P1 Companies (Unweighted) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "graph input must be a SGraph object.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9017233b61be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdistances_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpagerank_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshortest_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_vid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/turicreate/toolkits/graph_analytics/shortest_path.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(graph, source_vid, weight_field, max_distance, verbose)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SGraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"graph input must be a SGraph object.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     opts = {\n",
      "\u001b[0;31mTypeError\u001b[0m: graph input must be a SGraph object."
     ]
    }
   ],
   "source": [
    "##Before putting it in the loop, you'll have to edit this code so that cb (in the second for loop) is the correct 2-sided dataframe\n",
    "##I didn't create all of the frames, but if you can store them in frame_list, the code should work fine.\n",
    "\n",
    "frame_names = ['pr_FR_S', 'pr_FR']\n",
    "frame_list = [pr_FR_S, pr_FR]\n",
    "for j,k in zip(frame_names, frame_list):\n",
    "    pagerank_sorted = pagerank_output.sort_values(by=[j], ascending = False)\n",
    "\n",
    "    distances_list = []\n",
    "    for i in list(pagerank_sorted[:5]['__id']):\n",
    "        sp = shortest_path.create(k, source_vid=i, verbose = False)\n",
    "        df = pd.DataFrame(sp['distance'])\n",
    "        df.rename(columns={'distance': i}, inplace = True)\n",
    "        distances_list.append(df)\n",
    "\n",
    "    distances = reduce(lambda  left, right: pd.merge(left, right,on=['__id']), distances_list)\n",
    "    distances['minimum_distance'] = distances.min(axis=1) \n",
    "\n",
    "    path = 'files/output/graph_model/model/spath_top_{}.csv'.format(j)\n",
    "    distances.to_csv(path, index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
