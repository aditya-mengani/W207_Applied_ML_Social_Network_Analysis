{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Pipeline\n",
    "\n",
    "By: Aditya Mengani, Ognjen Sosa, Sanjay Elangovan, Song Park, Sophia Skowronski\n",
    "\n",
    "**Can we improve on the baseline scores using different encoding, imputing, and scaling schemes?**\n",
    "- Averaged Logistic Regression accuracy Score: 0.5\n",
    "- Averaged Linear Regression accuracy score: 0.2045\n",
    "- Averaged K-Nearest Neighbour accuracy score: 0.6198\n",
    "- Averaged Naive Bayes accuracy score: 0.649\n",
    "\n",
    "**`p1_tag` ~  `rank` + `total_funding_usd` + `employee_count` (ordinal) + `country` (nominal) + `category_groups` (nominal)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data analysis'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "import statistics\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "'''Plotting'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "'''Stat'''\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "'''ML'''\n",
    "import prince\n",
    "import category_encoders as ce\n",
    "from sklearn import metrics, svm, preprocessing, utils\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100*(start_mem-end_mem)/start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Dataframe Columns:\n",
      "\n",
      "['employee_count', 'category_groups_list', 'country_code', 'uuid', 'p1_tag', 'rank', 'employee_count_ord', 'total_funding_usd', 'ind_1', 'ind_2', 'ind_3', 'ind_4', 'ind_5', 'ind_6', 'ind_7', 'ind_8', 'ind_9', 'ind_10', 'ind_11', 'ind_12', 'ind_13', 'ind_14', 'ind_15', 'ind_16', 'ind_17', 'ind_18', 'ind_19', 'ind_20', 'ind_21', 'ind_22', 'ind_23', 'ind_24', 'ind_25', 'ind_26', 'ind_27', 'ind_28', 'ind_29', 'ind_30', 'ind_31', 'ind_32', 'ind_33', 'ind_34', 'ind_35', 'ind_36', 'ind_37', 'ind_38', 'ind_39', 'ind_40', 'ind_41', 'ind_42', 'ind_43', 'ind_44', 'ind_45', 'ind_46']\n",
      "\n",
      "OUTPUT TO CSV `files/output/baseline_fixed.csv`\n",
      "\n",
      "Mem. usage decreased to 149.97 Mb (65.9% reduction)\n",
      "\n",
      "Ending Dataframe Columns:\n",
      "\n",
      "['country_code', 'p1_tag', 'rank', 'employee_count_ord', 'total_funding_usd', 'ind_1', 'ind_2', 'ind_3', 'ind_4', 'ind_5', 'ind_6', 'ind_7', 'ind_8', 'ind_9', 'ind_10', 'ind_11', 'ind_12', 'ind_13', 'ind_14', 'ind_15', 'ind_16', 'ind_17', 'ind_18', 'ind_19', 'ind_20', 'ind_21', 'ind_22', 'ind_23', 'ind_24', 'ind_25', 'ind_26', 'ind_27', 'ind_28', 'ind_29', 'ind_30', 'ind_31', 'ind_32', 'ind_33', 'ind_34', 'ind_35', 'ind_36', 'ind_37', 'ind_38', 'ind_39', 'ind_40', 'ind_41', 'ind_42', 'ind_43', 'ind_44', 'ind_45', 'ind_46']\n",
      "\n",
      "Dataframe shape: (1131326, 51)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('files/output/baseline.csv',sep=';')\n",
    "df = df[df.columns.to_list()[61:]]\n",
    "column_map = {'employee_count':'employee_count_ord', 'employee_size':'employee_count', 'category_groups':'category_groups_list', 'country':'country_code'}\n",
    "df = df.rename(column_map, axis=1)\n",
    "print('Starting Dataframe Columns:\\n\\n{}\\n'.format(df.columns.to_list()))\n",
    "\n",
    "# Have industry mapper for 'ind_1'...'ind_46' columns\n",
    "industries = ['Software', 'Information Technology', 'Internet Services', 'Data and Analytics',\n",
    "              'Sales and Marketing', 'Media and Entertainment', 'Commerce and Shopping', \n",
    "              'Financial Services', 'Apps', 'Mobile', 'Science and Engineering', 'Hardware',\n",
    "              'Health Care', 'Education', 'Artificial Intelligence', 'Professional Services', \n",
    "              'Design', 'Community and Lifestyle', 'Real Estate', 'Advertising',\n",
    "              'Transportation', 'Consumer Electronics', 'Lending and Investments',\n",
    "              'Sports', 'Travel and Tourism', 'Food and Beverage',\n",
    "              'Content and Publishing', 'Consumer Goods', 'Privacy and Security',\n",
    "              'Video', 'Payments', 'Sustainability', 'Events', 'Manufacturing',\n",
    "              'Clothing and Apparel', 'Administrative Services', 'Music and Audio',\n",
    "              'Messaging and Telecommunications', 'Energy', 'Platforms', 'Gaming',\n",
    "              'Government and Military', 'Biotechnology', 'Navigation and Mapping',\n",
    "              'Agriculture and Farming', 'Natural Resources']\n",
    "industry_map = {industry:'ind_'+str(idx+1) for idx,industry in enumerate(industries)}\n",
    "\n",
    "# Rename columns to match column names from Crunchbase datasets\n",
    "print('OUTPUT TO CSV `files/output/baseline_fixed.csv`\\n')\n",
    "df.to_csv('files/output/baseline_fixed.csv', index=False, sep=';')\n",
    "\n",
    "# Create \n",
    "df_simple = df.drop(['employee_count','category_groups_list','uuid'], axis=1)\n",
    "df_simple = reduce_mem_usage(df_simple)\n",
    "\n",
    "print('\\nEnding Dataframe Columns:\\n\\n{}'.format(df_simple.columns.to_list()))\n",
    "\n",
    "print('\\nDataframe shape:', df_simple.shape)\n",
    "\n",
    "del industries, industry_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  1.66 Mb (20.1% reduction)\n",
      "Numeric features: ['rank', 'employee_count_ord', 'total_funding_usd', 'ind_1', 'ind_2', 'ind_3', 'ind_4', 'ind_5', 'ind_6', 'ind_7', 'ind_8', 'ind_9', 'ind_10', 'ind_11', 'ind_12', 'ind_13', 'ind_14', 'ind_15', 'ind_16', 'ind_17', 'ind_18', 'ind_19', 'ind_20', 'ind_21', 'ind_22', 'ind_23', 'ind_24', 'ind_25', 'ind_26', 'ind_27', 'ind_28', 'ind_29', 'ind_30', 'ind_31', 'ind_32', 'ind_33', 'ind_34', 'ind_35', 'ind_36', 'ind_37', 'ind_38', 'ind_39', 'ind_40', 'ind_41', 'ind_42', 'ind_43', 'ind_44', 'ind_45', 'ind_46']\n",
      "Categorical features: ['country_code']\n",
      "Training data shape: (12531, 50)\n",
      "Train label shape: (12531,)\n",
      "Test data shape: (3133, 50)\n",
      "Test label shape: (3133,)\n"
     ]
    }
   ],
   "source": [
    "# Select equal sample of non-Pledge 1% organizations\n",
    "df_p1 = df_simple[df_simple['p1_tag']==1]\n",
    "df_notp1 = df_simple[df_simple['p1_tag']==0].sample(n=df_p1.shape[0], replace=False)\n",
    "df_model = pd.concat([df_p1, df_notp1]).reset_index(drop=True)\n",
    "df_model = reduce_mem_usage(df_model)\n",
    "\n",
    "# Create variable for each feature type: categorical and numerical\n",
    "numeric_features = df_model.select_dtypes(include=['int8', 'int16', 'int32', 'int64', 'float16', 'float32','float64']).drop(['p1_tag'], axis=1).columns\n",
    "categorical_features = df_model.select_dtypes(include=['object']).columns\n",
    "print('Numeric features:', numeric_features.to_list())\n",
    "print('Categorical features:', categorical_features.to_list())\n",
    "\n",
    "X = df_model.drop('p1_tag', axis=1)\n",
    "y = df_model['p1_tag']\n",
    "y = preprocessing.LabelEncoder().fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print('Training data shape:', X_train.shape)\n",
    "print('Train label shape:', y_train.shape)\n",
    "print('Test data shape:',  X_test.shape)\n",
    "print('Test label shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run through pipeline to determine best categorical feature encoder\n",
    "\n",
    "From: <a href='https://towardsdatascience.com/an-easier-way-to-encode-categorical-features-d840ff6b3900'>An Easier Way to Encode Categorical Features</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRR with BackwardDifferenceEncoder\n",
      "Best parameter (CV score=0.716): {'classifier__C': 100.0}\n",
      "Best score: 0.7121\n",
      "\n",
      "LRR with BaseNEncoder\n",
      "Best parameter (CV score=0.691): {'classifier__C': 10.0}\n",
      "Best score: 0.6828\n",
      "\n",
      "LRR with BinaryEncoder\n",
      "Best parameter (CV score=0.691): {'classifier__C': 10.0}\n",
      "Best score: 0.6828\n",
      "\n",
      "LRR with CatBoostEncoder\n",
      "Best parameter (CV score=0.716): {'classifier__C': 10.0}\n",
      "Best score: 0.7064\n",
      "\n",
      "LRR with HashingEncoder\n",
      "Best parameter (CV score=nan): {'classifier__C': 0.0001}\n",
      "Best score: 0.6653\n",
      "\n",
      "LRR with HelmertEncoder\n",
      "Best parameter (CV score=0.717): {'classifier__C': 0.01}\n",
      "Best score: 0.7113\n",
      "\n",
      "LRR with JamesSteinEncoder\n",
      "Best parameter (CV score=0.715): {'classifier__C': 1000}\n",
      "Best score: 0.7087\n",
      "\n",
      "LRR with OneHotEncoder\n",
      "Best parameter (CV score=0.717): {'classifier__C': 1.0}\n",
      "Best score: 0.7108\n",
      "\n",
      "LRR with LeaveOneOutEncoder\n",
      "Best parameter (CV score=0.715): {'classifier__C': 10.0}\n",
      "Best score: 0.7059\n",
      "\n",
      "LRR with MEstimateEncoder\n",
      "Best parameter (CV score=0.714): {'classifier__C': 10.0}\n",
      "Best score: 0.7029\n",
      "\n",
      "LRR with OrdinalEncoder\n",
      "Best parameter (CV score=0.672): {'classifier__C': 0.001}\n",
      "Best score: 0.6699\n",
      "\n",
      "LRR with PolynomialEncoder\n",
      "Best parameter (CV score=0.718): {'classifier__C': 1.0}\n",
      "Best score: 0.7108\n",
      "\n",
      "LRR with SumEncoder\n",
      "Best parameter (CV score=0.717): {'classifier__C': 1.0}\n",
      "Best score: 0.7108\n",
      "\n",
      "LRR with TargetEncoder\n",
      "Best parameter (CV score=0.714): {'classifier__C': 1.0}\n",
      "Best score: 0.7046\n",
      "\n",
      "LRR with WOEEncoder\n",
      "Best parameter (CV score=0.715): {'classifier__C': 0.01}\n",
      "Best score: 0.7095\n",
      "\n",
      "KNN with BackwardDifferenceEncoder\n",
      "Score: 0.6669\n",
      "\n",
      "KNN with BaseNEncoder\n",
      "Score: 0.6889\n",
      "\n",
      "KNN with BinaryEncoder\n",
      "Score: 0.6889\n",
      "\n",
      "KNN with CatBoostEncoder\n",
      "Score: 0.6799\n",
      "\n",
      "KNN with HashingEncoder\n",
      "Score: 0.6885\n",
      "\n",
      "KNN with HelmertEncoder\n",
      "Score: 0.6754\n",
      "\n",
      "KNN with JamesSteinEncoder\n",
      "Score: 0.6708\n",
      "\n",
      "KNN with OneHotEncoder\n",
      "Score: 0.6923\n",
      "\n",
      "KNN with LeaveOneOutEncoder\n",
      "Score: 0.6797\n",
      "\n",
      "KNN with MEstimateEncoder\n",
      "Score: 0.6799\n",
      "\n",
      "KNN with OrdinalEncoder\n",
      "Score: 0.6680\n",
      "\n",
      "KNN with PolynomialEncoder\n",
      "Score: 0.6792\n",
      "\n",
      "KNN with SumEncoder\n",
      "Score: 0.6923\n",
      "\n",
      "KNN with TargetEncoder\n",
      "Score: 0.6801\n",
      "\n",
      "KNN with WOEEncoder\n",
      "Score: 0.6926\n",
      "\n",
      "BNB with BackwardDifferenceEncoder\n",
      "Best parameter (CV score=0.597): {'classifier__alpha': 10.0}\n",
      "Best score: 0.6586\n",
      "\n",
      "BNB with BaseNEncoder\n",
      "Best parameter (CV score=0.661): {'classifier__alpha': 10.0}\n",
      "Best score: 0.6517\n",
      "\n",
      "BNB with BinaryEncoder\n",
      "Best parameter (CV score=0.661): {'classifier__alpha': 10.0}\n",
      "Best score: 0.6517\n",
      "\n",
      "BNB with CatBoostEncoder\n",
      "Best parameter (CV score=0.645): {'classifier__alpha': 1.0}\n",
      "Best score: 0.6246\n",
      "\n",
      "BNB with HashingEncoder\n",
      "Best parameter (CV score=nan): {'classifier__alpha': 0.0001}\n",
      "Best score: 0.6583\n",
      "\n",
      "BNB with HelmertEncoder\n",
      "Best parameter (CV score=0.691): {'classifier__alpha': 0.0001}\n",
      "Best score: 0.6682\n",
      "\n",
      "BNB with JamesSteinEncoder\n",
      "Best parameter (CV score=0.645): {'classifier__alpha': 0.0001}\n",
      "Best score: 0.6227\n",
      "\n",
      "BNB with OneHotEncoder\n",
      "Best parameter (CV score=0.695): {'classifier__alpha': 1.0}\n",
      "Best score: 0.6835\n",
      "\n",
      "BNB with LeaveOneOutEncoder\n",
      "Best parameter (CV score=0.645): {'classifier__alpha': 0.0001}\n",
      "Best score: 0.6242\n",
      "\n",
      "BNB with MEstimateEncoder\n",
      "Best parameter (CV score=0.645): {'classifier__alpha': 1.0}\n",
      "Best score: 0.6246\n",
      "\n",
      "BNB with OrdinalEncoder\n",
      "Best parameter (CV score=0.645): {'classifier__alpha': 1.0}\n",
      "Best score: 0.6246\n",
      "\n",
      "BNB with PolynomialEncoder\n",
      "Best parameter (CV score=0.609): {'classifier__alpha': 0.0001}\n",
      "Best score: 0.5812\n",
      "\n",
      "BNB with SumEncoder\n",
      "Best parameter (CV score=0.695): {'classifier__alpha': 1.0}\n",
      "Best score: 0.6835\n",
      "\n",
      "BNB with TargetEncoder\n",
      "Best parameter (CV score=0.645): {'classifier__alpha': 1.0}\n",
      "Best score: 0.6246\n",
      "\n",
      "BNB with WOEEncoder\n",
      "Best parameter (CV score=0.674): {'classifier__alpha': 10.0}\n",
      "Best score: 0.6656\n",
      "\n",
      "GNB with BackwardDifferenceEncoder\n",
      "Best parameter (CV score=0.648): {'classifier__var_smoothing': 1.0}\n",
      "Best score: 0.6689\n",
      "\n",
      "GNB with BaseNEncoder\n",
      "Best parameter (CV score=0.638): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.6418\n",
      "\n",
      "GNB with BinaryEncoder\n",
      "Best parameter (CV score=0.638): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.6418\n",
      "\n",
      "GNB with CatBoostEncoder\n",
      "Best parameter (CV score=0.636): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.6279\n",
      "\n",
      "GNB with HashingEncoder\n",
      "Best parameter (CV score=nan): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.6640\n",
      "\n",
      "GNB with HelmertEncoder\n",
      "Best parameter (CV score=0.656): {'classifier__var_smoothing': 0.01}\n",
      "Best score: 0.6858\n",
      "\n",
      "GNB with JamesSteinEncoder\n",
      "Best parameter (CV score=0.636): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.6310\n",
      "\n",
      "GNB with OneHotEncoder\n",
      "Best parameter (CV score=0.667): {'classifier__var_smoothing': 0.01}\n",
      "Best score: 0.6961\n",
      "\n",
      "GNB with LeaveOneOutEncoder\n",
      "Best parameter (CV score=0.636): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.6283\n",
      "\n",
      "GNB with MEstimateEncoder\n",
      "Best parameter (CV score=0.638): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.6305\n",
      "\n",
      "GNB with OrdinalEncoder\n",
      "Best parameter (CV score=0.640): {'classifier__var_smoothing': 0.1}\n",
      "Best score: 0.6650\n",
      "\n",
      "GNB with PolynomialEncoder\n",
      "Best parameter (CV score=0.657): {'classifier__var_smoothing': 0.01}\n",
      "Best score: 0.6818\n",
      "\n",
      "GNB with SumEncoder\n",
      "Best parameter (CV score=0.656): {'classifier__var_smoothing': 0.01}\n",
      "Best score: 0.6985\n",
      "\n",
      "GNB with TargetEncoder\n",
      "Best parameter (CV score=0.638): {'classifier__var_smoothing': 0.0001}\n",
      "Best score: 0.6312\n",
      "\n",
      "GNB with WOEEncoder\n",
      "Best parameter (CV score=0.637): {'classifier__var_smoothing': 0.001}\n",
      "Best score: 0.6308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "classifier_list = []\n",
    "LRR = LogisticRegression(max_iter=10000, tol=0.1)\n",
    "KNN = KNeighborsClassifier(n_neighbors=30, p=1, leaf_size=25)\n",
    "BNB = BernoulliNB()\n",
    "GNB = GaussianNB()\n",
    "classifier_list.append(('LRR', LRR, {'classifier__C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000]}))\n",
    "classifier_list.append(('KNN', KNN, {}))\n",
    "classifier_list.append(('BNB', BNB, {'classifier__alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]}))\n",
    "classifier_list.append(('GNB', GNB, {'classifier__var_smoothing': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]}))\n",
    "\n",
    "#classifier_list.append(('SVM', svm.SVC()))\n",
    "#classifier_list.append(('CART', DecisionTreeClassifier()))\n",
    "#classifier_list.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "\n",
    "encoder_list = [ce.backward_difference.BackwardDifferenceEncoder, \n",
    "                ce.basen.BaseNEncoder,\n",
    "                ce.binary.BinaryEncoder,\n",
    "                ce.cat_boost.CatBoostEncoder,\n",
    "                ce.hashing.HashingEncoder,\n",
    "                ce.helmert.HelmertEncoder,\n",
    "                ce.james_stein.JamesSteinEncoder,\n",
    "                ce.one_hot.OneHotEncoder,\n",
    "                ce.leave_one_out.LeaveOneOutEncoder,\n",
    "                ce.m_estimate.MEstimateEncoder,\n",
    "                ce.ordinal.OrdinalEncoder,\n",
    "                ce.polynomial.PolynomialEncoder,\n",
    "                ce.sum_coding.SumEncoder,\n",
    "                ce.target_encoder.TargetEncoder,\n",
    "                ce.woe.WOEEncoder]\n",
    "\n",
    "for label, classifier, params in classifier_list:\n",
    "    results[label] = {}\n",
    "    for encoder in encoder_list:\n",
    "        results[label][encoder.__name__] = {}\n",
    "        print('{} with {}'.format(label, encoder.__name__))\n",
    "        \n",
    "        #numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),('scaler', MinMaxScaler())])\n",
    "        numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),('scaler', StandardScaler())])\n",
    "\n",
    "        categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                                                  ('woe', encoder())])\n",
    "\n",
    "        preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),\n",
    "                                                       ('cat', categorical_transformer, categorical_features)])\n",
    "        pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', classifier)])\n",
    "        if params != {}:\n",
    "            try:\n",
    "                search = GridSearchCV(pipe, params, n_jobs=-1)\n",
    "                search.fit(X_train, y_train)\n",
    "                print('Best parameter (CV score={:.3f}): {}'.format(search.best_score_, search.best_params_))\n",
    "                model = search.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                score = f1_score(y_test, y_pred)\n",
    "                print('Best score: {:.4f}\\n'.format(score))\n",
    "                results[label][encoder.__name__]['score'] = score\n",
    "                results[label][encoder.__name__]['best_params'] = search.best_params_\n",
    "            except:\n",
    "                print('Something went wrong w/ GridSearch or pipeline fitting.')\n",
    "        else:\n",
    "            try:\n",
    "                model = pipe.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                score = f1_score(y_test, y_pred)\n",
    "                print('Score: {:.4f}\\n'.format(score))\n",
    "                results[label][encoder.__name__]['score'] = score\n",
    "            except:\n",
    "                print('Something went wrong with pipeline fitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BNB': {'BackwardDifferenceEncoder': {'best_params': {'classifier__alpha': 10.0}, 'score': 0.6585559375826502}, 'BaseNEncoder': {'best_params': {'classifier__alpha': 10.0}, 'score': 0.6517379679144384}, 'BinaryEncoder': {'best_params': {'classifier__alpha': 10.0}, 'score': 0.6517379679144384}, 'CatBoostEncoder': {'best_params': {'classifier__alpha': 1.0}, 'score': 0.6245638520586182}, 'HashingEncoder': {'best_params': {'classifier__alpha': 0.0001}, 'score': 0.6583030703202377}, 'HelmertEncoder': {'best_params': {'classifier__alpha': 0.0001}, 'score': 0.6682432432432432}, 'JamesSteinEncoder': {'best_params': {'classifier__alpha': 0.0001}, 'score': 0.6227336122733613}, 'LeaveOneOutEncoder': {'best_params': {'classifier__alpha': 0.0001}, 'score': 0.6241727621037966}, 'MEstimateEncoder': {'best_params': {'classifier__alpha': 1.0}, 'score': 0.6245638520586182}, 'OneHotEncoder': {'best_params': {'classifier__alpha': 1.0}, 'score': 0.6834862385321102}, 'OrdinalEncoder': {'best_params': {'classifier__alpha': 1.0}, 'score': 0.6245638520586182}, 'PolynomialEncoder': {'best_params': {'classifier__alpha': 0.0001}, 'score': 0.5812250332889481}, 'SumEncoder': {'best_params': {'classifier__alpha': 1.0}, 'score': 0.6834862385321102}, 'TargetEncoder': {'best_params': {'classifier__alpha': 1.0}, 'score': 0.6245638520586182}, 'WOEEncoder': {'best_params': {'classifier__alpha': 10.0}, 'score': 0.6655540720961282}}, 'GNB': {'BackwardDifferenceEncoder': {'best_params': {'classifier__var_smoothing': 1.0}, 'score': 0.6688841850619898}, 'BaseNEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.6417525773195876}, 'BinaryEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.6417525773195876}, 'CatBoostEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.6278763971071663}, 'HashingEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.6640098099325568}, 'HelmertEncoder': {'best_params': {'classifier__var_smoothing': 0.01}, 'score': 0.6857954545454544}, 'JamesSteinEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.6309914642153643}, 'LeaveOneOutEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.6283273085770621}, 'MEstimateEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.6305062458908613}, 'OneHotEncoder': {'best_params': {'classifier__var_smoothing': 0.01}, 'score': 0.6961111111111111}, 'OrdinalEncoder': {'best_params': {'classifier__var_smoothing': 0.1}, 'score': 0.6649874055415618}, 'PolynomialEncoder': {'best_params': {'classifier__var_smoothing': 0.01}, 'score': 0.6818312697955659}, 'SumEncoder': {'best_params': {'classifier__var_smoothing': 0.01}, 'score': 0.6985413290113452}, 'TargetEncoder': {'best_params': {'classifier__var_smoothing': 0.0001}, 'score': 0.6311986863711001}, 'WOEEncoder': {'best_params': {'classifier__var_smoothing': 0.001}, 'score': 0.6308196721311475}}, 'KNN': {'BackwardDifferenceEncoder': {'score': 0.6668876367252238}, 'BaseNEncoder': {'score': 0.6889168765743072}, 'BinaryEncoder': {'score': 0.6889168765743072}, 'CatBoostEncoder': {'score': 0.6798830029249269}, 'HashingEncoder': {'score': 0.6885350318471338}, 'HelmertEncoder': {'score': 0.6753944276602888}, 'JamesSteinEncoder': {'score': 0.6707872478854913}, 'LeaveOneOutEncoder': {'score': 0.6796621182586094}, 'MEstimateEncoder': {'score': 0.6798959011060507}, 'OneHotEncoder': {'score': 0.6922588832487309}, 'OrdinalEncoder': {'score': 0.6679907315458458}, 'PolynomialEncoder': {'score': 0.679245283018868}, 'SumEncoder': {'score': 0.6922588832487309}, 'TargetEncoder': {'score': 0.6801040312093628}, 'WOEEncoder': {'score': 0.6925841228356746}}, 'LRR': {'BackwardDifferenceEncoder': {'best_params': {'classifier__C': 100.0}, 'score': 0.7120585427935094}, 'BaseNEncoder': {'best_params': {'classifier__C': 10.0}, 'score': 0.6828489300542957}, 'BinaryEncoder': {'best_params': {'classifier__C': 10.0}, 'score': 0.6828489300542957}, 'CatBoostEncoder': {'best_params': {'classifier__C': 10.0}, 'score': 0.706442399238337}, 'HashingEncoder': {'best_params': {'classifier__C': 0.0001}, 'score': 0.6653266331658292}, 'HelmertEncoder': {'best_params': {'classifier__C': 0.01}, 'score': 0.7113237639553429}, 'JamesSteinEncoder': {'best_params': {'classifier__C': 1000}, 'score': 0.7086914995224451}, 'LeaveOneOutEncoder': {'best_params': {'classifier__C': 10.0}, 'score': 0.7058823529411765}, 'MEstimateEncoder': {'best_params': {'classifier__C': 10.0}, 'score': 0.7029262086513994}, 'OneHotEncoder': {'best_params': {'classifier__C': 1.0}, 'score': 0.7107858733693923}, 'OrdinalEncoder': {'best_params': {'classifier__C': 0.001}, 'score': 0.6698903932946487}, 'PolynomialEncoder': {'best_params': {'classifier__C': 1.0}, 'score': 0.7107858733693923}, 'SumEncoder': {'best_params': {'classifier__C': 1.0}, 'score': 0.7107858733693923}, 'TargetEncoder': {'best_params': {'classifier__C': 1.0}, 'score': 0.704574332909784}, 'WOEEncoder': {'best_params': {'classifier__C': 0.01}, 'score': 0.7095132039452752}}}\n"
     ]
    }
   ],
   "source": [
    "with open('files/output/results_baseline.json', 'w') as fp:\n",
    "    json.dump(results, fp, sort_keys=True, indent=4)\n",
    "with open('files/output/results_baseline.json', 'r') as fp:\n",
    "    results = json.load(fp)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with manual encoding from previous notebook + `total_funding_usd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  3.48 Mb (56.0% reduction)\n",
      "\n",
      "MISSING VALUES BY PERCENTAGE\n",
      "rank                 0.00%\n",
      "employee_count_ord   0.00%\n",
      "total_funding_usd    0.00%\n",
      "\n",
      "Training data shape: (12531, 179)\n",
      "Train label shape: (12531,)\n",
      "Test data shape: (3133, 179)\n",
      "Test label shape: (3133,)\n",
      "\n",
      "KNN Accuracy score: 0.6409\n",
      "LRR Accuracy score: 0.5806\n",
      "MNB Accuracy score: 0.5812\n"
     ]
    }
   ],
   "source": [
    "# Comparison\n",
    "df_b4 = df.drop(['category_groups_list','employee_count', 'uuid'], axis=1)\n",
    "\n",
    "# Sample\n",
    "df_p1 = df_b4[df_b4['p1_tag']==1]\n",
    "df_notp1 = df_b4[df_b4['p1_tag']==0].sample(n=df_p1.shape[0], replace=False)\n",
    "df_b4 = pd.concat([df_p1, df_notp1]).reset_index(drop=True)\n",
    "df_b4 = pd.get_dummies(df_b4) # OneHot encoder for country_code columns\n",
    "df_b4 = reduce_mem_usage(df_b4)\n",
    "\n",
    "# Impute missing data in employee_count and rank columns\n",
    "imputer = SimpleImputer(missing_values=-1, strategy='median')\n",
    "df_b4['employee_count_ord'] = imputer.fit_transform(df_b4['employee_count_ord'].values.reshape(-1, 1))\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_b4['rank'] = imputer.fit_transform(df_b4['rank'].values.reshape(-1, 1))\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_b4['total_funding_usd'] = imputer.fit_transform(df_b4['total_funding_usd'].values.reshape(-1, 1))\n",
    "df_num_missing = df_b4[['rank', 'employee_count_ord', 'total_funding_usd']].isna().sum()/len(df_b4)\n",
    "output_string = df_num_missing.to_string(float_format=lambda x: '{:.2f}%'.format(x*100))\n",
    "print('\\nMISSING VALUES BY PERCENTAGE')\n",
    "print(output_string)\n",
    "\n",
    "# Scale numeric values -- TBD\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "X = df_b4.drop('p1_tag', axis=1)\n",
    "y = df_b4['p1_tag']\n",
    "y = preprocessing.LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Replace any NaNs with zeros\n",
    "col_mask = X.isnull().any(axis=0) \n",
    "row_mask = X.isnull().any(axis=1)\n",
    "X.loc[row_mask,col_mask] = 0\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print('\\nTraining data shape:', X_train.shape)\n",
    "print('Train label shape:', y_train.shape)\n",
    "print('Test data shape:',  X_test.shape)\n",
    "print('Test label shape:', y_test.shape)\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=30, p=1, leaf_size=25)\n",
    "KNN.fit(X_train, y_train)\n",
    "y_pred = KNN.predict(X_test)\n",
    "print('\\nKNN Accuracy score: {:.4f}'.format(KNN.score(X_test, y_test)))\n",
    "\n",
    "LR = LogisticRegression(C=10)\n",
    "LR.fit(X_train, y_train)\n",
    "print('LRR Accuracy score: {:.4f}'.format(LR.score(X_test, y_test)))\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, y_train)\n",
    "y_pred = MNB.predict(X_test)\n",
    "print('MNB Accuracy score: {:.4f}'.format(MNB.score(X_test, y_test)))\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
